{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import numpy as np\n",
    "from jaxlie import SE3, SO3\n",
    "from jax.random import normal\n",
    "from jax import grad, hessian, vmap, pmap\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.lax import scan\n",
    "from functools import partial\n",
    "import jax.random as jax_random\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "from plyfile import PlyData\n",
    "import open3d as o3d \n",
    "import trimesh as tm\n",
    "import trimesh\n",
    "import time\n",
    "import jaxopt\n",
    "import polyscope as ps\n",
    "\n",
    "\n",
    "import meshcat\n",
    "import meshcat.geometry as mc_geom\n",
    "\n",
    "# from ergodic_mmd.aug_lagrange_jaxopt import AugmentedLagrangeSolver\n",
    "from ergodic_mmd.aug_lagrange_solver import AugmentedLagrangeSolver\n",
    "\n",
    "import adam\n",
    "from adam.jax import KinDynComputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7000/static/\n"
     ]
    }
   ],
   "source": [
    "from robomeshcat import Object, Robot, Scene\n",
    "from example_robot_data.robots_loader import PandaLoader\n",
    "from pathlib import Path\n",
    "\n",
    "\"Create a scene that stores all objects and robots and has rendering capability\"\n",
    "scene = Scene()\n",
    "\n",
    "robot = Robot(urdf_path=PandaLoader().df_path, mesh_folder_path=Path(PandaLoader().model_path).parent.parent)\n",
    "scene.add_robot(robot)\n",
    "\"Render the initial scene\"\n",
    "scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = o3d.data.BunnyMesh()\n",
    "plydata = PlyData.read(dataset.path)\n",
    "# plydata = PlyData.read('../assets/sphere.ply')\n",
    "verts = np.vstack((\n",
    "    plydata['vertex']['x'], # offset mesh location by half a meter\n",
    "    plydata['vertex']['y'],\n",
    "    plydata['vertex']['z']\n",
    ")).T\n",
    "faces = np.array(np.vstack(plydata['face']['vertex_indices']))\n",
    "\n",
    "mesh = tm.Trimesh(vertices=verts, faces=faces)\n",
    "mesh.apply_scale(1.2)\n",
    "mesh.apply_transform(\n",
    "    SE3.from_rotation(SO3.from_x_radians(np.pi/2)).as_matrix()\n",
    "    )\n",
    "mesh.apply_translation(np.array([0.5,0.,0.1]))\n",
    "\n",
    "\n",
    "num_points = 1000  # Change this number based on your requirement\n",
    "points, face_indices = tm.sample.sample_surface(mesh.convex_hull, num_points)\n",
    "\n",
    "info_distr = lambda x: jnp.exp(-20*(x[0]-0.5)**2 - 20*(x[1]+0.1)**2 - 20*(x[2]-0.4)**2)\n",
    "P_XI = vmap(info_distr, in_axes=(0,))(points)\n",
    "P_XI = P_XI/jnp.sum(P_XI)\n",
    "h = 0.01\n",
    "args = {'h' : h, \n",
    "        'points' : points+0.05*mesh.convex_hull.face_normals[face_indices], \n",
    "        'normals' : mesh.convex_hull.face_normals[face_indices], \n",
    "        'P_XI' : P_XI}\n",
    "\n",
    "\n",
    "# exp_obj = trimesh.exchange.obj.export_obj(mesh)\n",
    "# mc_obj = mc_geom.ObjMeshGeometry.from_stream(trimesh.util.wrap_as_stream(exp_obj))\n",
    "# mc_obj = Object(mc_obj, name='red_sphere', opacity=0.5, color=[1., 0., 0.])\n",
    "# scene.add_object(mc_obj)\n",
    "\n",
    "mesh_vertex_color_info = np.zeros_like(mesh.vertices)\n",
    "_color_map = vmap(info_distr)(mesh.vertices)\n",
    "_color_map = _color_map - _color_map.min()\n",
    "_color_map = _color_map/_color_map. max()\n",
    "mesh_vertex_color_info[:,0] = _color_map\n",
    "mc_obj = mc_geom.TriangularMeshGeometry(mesh.vertices, mesh.faces, color=mesh_vertex_color_info)\n",
    "\n",
    "scene.vis['obj'].set_object(\n",
    "    mc_obj, \n",
    "    mc_geom.MeshLambertMaterial(reflectivity=0.9, vertexColors=True)\n",
    ")\n",
    "\n",
    "# mc_obj = Object(mc_obj, name='red_sphere', opacity=1)\n",
    "# scene.add_object(mc_obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link0']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link1']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link2']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link4']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link5']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link6']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link7']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_hand']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_leftfinger']/collision[1]\n",
      "Unknown tag \"contact\" in /robot[@name='panda']/link[@name='panda_leftfinger']\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_rightfinger']/collision[1]\n",
      "Unknown tag \"contact\" in /robot[@name='panda']/link[@name='panda_rightfinger']\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../assets/panda.urdf\"\n",
    "# The joint list\n",
    "joints_name_list = [\n",
    "    'panda_joint1', 'panda_joint2', 'panda_joint3', 'panda_joint4', \n",
    "    'panda_joint5', 'panda_joint6', 'panda_joint7'\n",
    "]\n",
    "\n",
    "kinDyn = KinDynComputations(model_path, joints_name_list)\n",
    "# kinDyn.set_frame_velocity_representation(adam.Representations.BODY_FIXED_REPRESENTATION)\n",
    "w_H_ee = kinDyn.forward_kinematics_fun(\"panda_grasptarget\")\n",
    "w_H_b = np.eye(4) # base frame \n",
    "\n",
    "q0 = jnp.array([0,-np.pi/3,0,-3, 0, 2.5, -0.7853])\n",
    "args.update({'q0':q0})\n",
    "# w_H_ee(w_H_b, q0)\n",
    "for i, jt in enumerate(q0):\n",
    "    robot[i] = jt\n",
    "scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fk(q):\n",
    "    return SE3.from_matrix(w_H_ee(w_H_b, q))\n",
    "\n",
    "def RBF_kernel(x, xp, h=0.01):\n",
    "    return jnp.exp(\n",
    "        -jnp.sum((x-xp)**2)/h\n",
    "    )\n",
    "\n",
    "# def camera_view_penalty(g, v, h=0.01):\n",
    "#     p = g.translation()\n",
    "#     rot = g.rotation()\n",
    "#     w = rot.apply(jnp.array([0.,0.,1.]))\n",
    "#     root, norm  = jnp.split(v, 2)\n",
    "#     return jnp.exp(-jnp.sum((root-p)**2)/h - jnp.sum((-norm-w)**2)/h)\n",
    "#     # return jnp.exp(-jnp.sum((root-p)**2)/h)*jnp.sum((-norm-w)**2)\n",
    "\n",
    "def camera_view_penalty(g, v, h=0.01):\n",
    "    p = g.translation()\n",
    "    rot = g.rotation()\n",
    "    w = rot.apply(jnp.array([0.,0.,1.]))\n",
    "    root, norm  = jnp.split(v, 2)\n",
    "    # return jnp.exp(-jnp.sum((root-p)**2)/h - 0.01*jnp.sum((-norm-w)**2)/h)\n",
    "    return jnp.exp(\n",
    "        -jnp.sum((root-p)**2)/h #- 1e-1*jnp.sum((norm-w)**2)\n",
    "    )*jnp.dot(-norm,w)\n",
    "\n",
    "\n",
    "\n",
    "# def RBF_kernel(g, v, h=0.01):\n",
    "#     p = g.translation()\n",
    "#     rot = g.rotation()\n",
    "#     w = rot.apply(jnp.array([0.,0.,1.]))\n",
    "#     root, norm  = jnp.split(v, 2)\n",
    "#     return jnp.exp(\n",
    "#         -jnp.sum((root-p)**2)/h #- 1e-1*jnp.sum((norm-w)**2)\n",
    "#     ) * jnp.exp(\n",
    "#         -10*jnp.sum((-norm-w)**2)\n",
    "#     )\n",
    "\n",
    "def create_kernel_matrix(kernel):\n",
    "    return vmap(vmap(kernel, in_axes=(0, None, None)), in_axes=(None, 0, None))\n",
    "\n",
    "camera_view_matrix = create_kernel_matrix(camera_view_penalty)\n",
    "KernelMatrix = create_kernel_matrix(RBF_kernel)\n",
    "def emmd_loss(params, args):\n",
    "    X = params['X']\n",
    "    T = X.shape[0]\n",
    "    h = args['h']\n",
    "    q0 = args['q0']\n",
    "    points    = args['points']\n",
    "    norms     = args['normals']\n",
    "    P_XI      = args['P_XI']\n",
    "\n",
    "    g = vmap(get_fk)(X)\n",
    "    p = g.translation()\n",
    "    \n",
    "    view_matrix = camera_view_matrix(g, jnp.hstack([points, norms]), 0.1)\n",
    "\n",
    "    return np.sum(KernelMatrix(p, p, h))/(T**2) \\\n",
    "            - 2 * np.sum(P_XI @ KernelMatrix(p, points, h))/T \\\n",
    "            - np.sum(P_XI@view_matrix)/T \\\n",
    "                    + 1e-5*jnp.mean(jnp.square(X-q0)) \\\n",
    "                        + jnp.mean(jnp.square(X[1:] - X[:-1]))\n",
    "\n",
    "def eq_constr(params, args):\n",
    "    X = params['X']\n",
    "    q0 = args['q0']\n",
    "    return jnp.hstack([X[0]-q0, X[-1]-q0])\n",
    "\n",
    "def ineq_constr(params, args):\n",
    "    X = params['X']\n",
    "    return jnp.square(X[1:] - X[:-1]) - 0.1**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "X = jnp.linspace(q0, q0+0.1, num=T)\n",
    "\n",
    "params = {'X' : X}\n",
    "\n",
    "# solver = AugmentedLagrangeSolver(params, emmd_loss, eq_constr, ineq_constr, args=args)\n",
    "solver = AugmentedLagrangeSolver(\n",
    "    params, emmd_loss, eq_constr, ineq_constr, \n",
    "    step_size=1e-3,\n",
    "    args=args)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  loss  0.5871675  grad l2 norm  0.46018854\n",
      "iter  1  loss  0.62646693  grad l2 norm  0.64370817\n",
      "iter  2  loss  0.6644217  grad l2 norm  0.8517197\n",
      "iter  3  loss  0.6996696  grad l2 norm  1.066953\n",
      "iter  4  loss  0.7321252  grad l2 norm  1.2830049\n",
      "iter  5  loss  0.7619387  grad l2 norm  1.4972678\n",
      "iter  6  loss  0.7892666  grad l2 norm  1.7085488\n",
      "iter  7  loss  0.8142911  grad l2 norm  1.9162997\n",
      "iter  8  loss  0.8371874  grad l2 norm  2.1202693\n",
      "iter  9  loss  0.85811675  grad l2 norm  2.3203523\n",
      "iter  10  loss  0.87722415  grad l2 norm  2.5165217\n",
      "iter  11  loss  0.8946333  grad l2 norm  2.7087927\n",
      "iter  12  loss  0.9104439  grad l2 norm  2.8972018\n",
      "iter  13  loss  0.9247347  grad l2 norm  3.0817952\n",
      "iter  14  loss  0.9375694  grad l2 norm  3.2626207\n",
      "iter  15  loss  0.94900393  grad l2 norm  3.439729\n",
      "iter  16  loss  0.95908844  grad l2 norm  3.6131694\n",
      "iter  17  loss  0.9678657  grad l2 norm  3.782988\n",
      "iter  18  loss  0.97537136  grad l2 norm  3.9492266\n",
      "iter  19  loss  0.98163617  grad l2 norm  4.111923\n",
      "iter  20  loss  0.9866904  grad l2 norm  4.2711115\n",
      "iter  21  loss  0.99056613  grad l2 norm  4.4268255\n",
      "iter  22  loss  0.99329567  grad l2 norm  4.5790963\n",
      "iter  23  loss  0.99491155  grad l2 norm  4.7279544\n",
      "iter  24  loss  0.99544436  grad l2 norm  4.8734274\n",
      "iter  25  loss  0.9949232  grad l2 norm  5.015544\n",
      "iter  26  loss  0.99337137  grad l2 norm  5.154329\n",
      "iter  27  loss  0.9908099  grad l2 norm  5.289806\n",
      "iter  28  loss  0.9872569  grad l2 norm  5.4219966\n",
      "iter  29  loss  0.98273027  grad l2 norm  5.550919\n",
      "iter  30  loss  0.97725165  grad l2 norm  5.6765933\n",
      "iter  31  loss  0.9708458  grad l2 norm  5.7990384\n",
      "iter  32  loss  0.9635428  grad l2 norm  5.9182734\n",
      "iter  33  loss  0.9553708  grad l2 norm  6.034318\n",
      "iter  34  loss  0.94636065  grad l2 norm  6.1471944\n",
      "iter  35  loss  0.93654305  grad l2 norm  6.2569237\n",
      "iter  36  loss  0.9259448  grad l2 norm  6.36353\n",
      "iter  37  loss  0.9145827  grad l2 norm  6.4670315\n",
      "iter  38  loss  0.9024708  grad l2 norm  6.5673447\n",
      "iter  39  loss  0.8896259  grad l2 norm  6.664403\n",
      "iter  40  loss  0.87605727  grad l2 norm  6.75828\n",
      "iter  41  loss  0.86177677  grad l2 norm  6.848929\n",
      "iter  42  loss  0.84679794  grad l2 norm  6.9363885\n",
      "iter  43  loss  0.8311393  grad l2 norm  7.0206785\n",
      "iter  44  loss  0.81481844  grad l2 norm  7.101816\n",
      "iter  45  loss  0.79785526  grad l2 norm  7.1798277\n",
      "iter  46  loss  0.78026867  grad l2 norm  7.2547364\n",
      "iter  47  loss  0.7620791  grad l2 norm  7.326567\n",
      "iter  48  loss  0.7433071  grad l2 norm  7.3953433\n",
      "iter  49  loss  0.72397625  grad l2 norm  7.4610925\n",
      "iter  50  loss  0.70410836  grad l2 norm  7.523843\n",
      "iter  51  loss  0.68372583  grad l2 norm  7.5836306\n",
      "iter  52  loss  0.6628536  grad l2 norm  7.640495\n",
      "iter  53  loss  0.6415112  grad l2 norm  7.694478\n",
      "iter  54  loss  0.619712  grad l2 norm  7.745621\n",
      "iter  55  loss  0.5974697  grad l2 norm  7.793962\n",
      "iter  56  loss  0.574797  grad l2 norm  7.839531\n",
      "iter  57  loss  0.55170256  grad l2 norm  7.882343\n",
      "iter  58  loss  0.5282038  grad l2 norm  7.9224105\n",
      "iter  59  loss  0.5043184  grad l2 norm  7.959734\n",
      "iter  60  loss  0.48006612  grad l2 norm  7.994314\n",
      "iter  61  loss  0.45546803  grad l2 norm  8.026147\n",
      "iter  62  loss  0.43055335  grad l2 norm  8.055231\n",
      "iter  63  loss  0.40535286  grad l2 norm  8.081565\n",
      "iter  64  loss  0.3799004  grad l2 norm  8.105149\n",
      "iter  65  loss  0.35423064  grad l2 norm  8.125987\n",
      "iter  66  loss  0.32837954  grad l2 norm  8.144079\n",
      "iter  67  loss  0.30238065  grad l2 norm  8.159432\n",
      "iter  68  loss  0.27626526  grad l2 norm  8.172051\n",
      "iter  69  loss  0.2500633  grad l2 norm  8.18194\n",
      "iter  70  loss  0.22380184  grad l2 norm  8.189107\n",
      "iter  71  loss  0.19750611  grad l2 norm  8.193556\n",
      "iter  72  loss  0.171197  grad l2 norm  8.195291\n",
      "iter  73  loss  0.14489684  grad l2 norm  8.194321\n",
      "iter  74  loss  0.11862553  grad l2 norm  8.19064\n",
      "iter  75  loss  0.092408694  grad l2 norm  8.184268\n",
      "iter  76  loss  0.066271715  grad l2 norm  8.175193\n",
      "iter  77  loss  0.04023996  grad l2 norm  8.163437\n",
      "iter  78  loss  0.014351028  grad l2 norm  8.148988\n",
      "iter  79  loss  -0.011363733  grad l2 norm  8.131864\n",
      "iter  80  loss  -0.036870778  grad l2 norm  8.112066\n",
      "iter  81  loss  -0.062137168  grad l2 norm  8.089603\n",
      "iter  82  loss  -0.08712902  grad l2 norm  8.064479\n",
      "iter  83  loss  -0.11181584  grad l2 norm  8.036693\n",
      "iter  84  loss  -0.13616934  grad l2 norm  8.006199\n",
      "iter  85  loss  -0.16016257  grad l2 norm  7.973034\n",
      "iter  86  loss  -0.18377046  grad l2 norm  7.9371943\n",
      "iter  87  loss  -0.2069672  grad l2 norm  7.898697\n",
      "iter  88  loss  -0.22972721  grad l2 norm  7.857537\n",
      "iter  89  loss  -0.2520268  grad l2 norm  7.8137326\n",
      "iter  90  loss  -0.27384016  grad l2 norm  7.7672796\n",
      "iter  91  loss  -0.29514584  grad l2 norm  7.7182026\n",
      "iter  92  loss  -0.31592122  grad l2 norm  7.6664963\n",
      "iter  93  loss  -0.3361487  grad l2 norm  7.6121855\n",
      "iter  94  loss  -0.35581067  grad l2 norm  7.555269\n",
      "iter  95  loss  -0.37488854  grad l2 norm  7.4957695\n",
      "iter  96  loss  -0.39336228  grad l2 norm  7.4336834\n",
      "iter  97  loss  -0.41121113  grad l2 norm  7.3690434\n",
      "iter  98  loss  -0.42841423  grad l2 norm  7.3018384\n",
      "iter  99  loss  -0.4449552  grad l2 norm  7.232118\n",
      "iter  100  loss  -0.4608137  grad l2 norm  7.1598625\n",
      "iter  101  loss  -0.4759747  grad l2 norm  7.0851226\n",
      "iter  102  loss  -0.49042368  grad l2 norm  7.007882\n",
      "iter  103  loss  -0.50414884  grad l2 norm  6.9281764\n",
      "iter  104  loss  -0.51713777  grad l2 norm  6.8459964\n",
      "iter  105  loss  -0.52938014  grad l2 norm  6.761369\n",
      "iter  106  loss  -0.540865  grad l2 norm  6.6742873\n",
      "iter  107  loss  -0.5515834  grad l2 norm  6.584779\n",
      "iter  108  loss  -0.5615265  grad l2 norm  6.4928346\n",
      "iter  109  loss  -0.5706878  grad l2 norm  6.398488\n",
      "iter  110  loss  -0.57905877  grad l2 norm  6.3017235\n",
      "iter  111  loss  -0.58663225  grad l2 norm  6.202582\n",
      "iter  112  loss  -0.593397  grad l2 norm  6.101046\n",
      "iter  113  loss  -0.5993445  grad l2 norm  5.9971576\n",
      "iter  114  loss  -0.604464  grad l2 norm  5.8909035\n",
      "iter  115  loss  -0.6087446  grad l2 norm  5.7823215\n",
      "iter  116  loss  -0.6121755  grad l2 norm  5.6714077\n",
      "iter  117  loss  -0.6147452  grad l2 norm  5.5581975\n",
      "iter  118  loss  -0.6164429  grad l2 norm  5.442688\n",
      "iter  119  loss  -0.61725897  grad l2 norm  5.324916\n",
      "iter  120  loss  -0.6171838  grad l2 norm  5.20488\n",
      "iter  121  loss  -0.6162093  grad l2 norm  5.0826206\n",
      "iter  122  loss  -0.6143272  grad l2 norm  4.958134\n",
      "iter  123  loss  -0.61152995  grad l2 norm  4.8314643\n",
      "iter  124  loss  -0.60781085  grad l2 norm  4.702611\n",
      "iter  125  loss  -0.60316277  grad l2 norm  4.571616\n",
      "iter  126  loss  -0.59758013  grad l2 norm  4.438485\n",
      "iter  127  loss  -0.59105766  grad l2 norm  4.3032613\n",
      "iter  128  loss  -0.58358705  grad l2 norm  4.1659527\n",
      "iter  129  loss  -0.5751618  grad l2 norm  4.0266056\n",
      "iter  130  loss  -0.5657779  grad l2 norm  3.8852305\n",
      "iter  131  loss  -0.5554305  grad l2 norm  3.7418776\n",
      "iter  132  loss  -0.54411906  grad l2 norm  3.596562\n",
      "iter  133  loss  -0.5318398  grad l2 norm  3.449337\n",
      "iter  134  loss  -0.51859105  grad l2 norm  3.3002243\n",
      "iter  135  loss  -0.504373  grad l2 norm  3.149282\n",
      "iter  136  loss  -0.48918936  grad l2 norm  2.9965394\n",
      "iter  137  loss  -0.47304377  grad l2 norm  2.8420594\n",
      "iter  138  loss  -0.45594394  grad l2 norm  2.685881\n",
      "iter  139  loss  -0.43790016  grad l2 norm  2.528077\n",
      "iter  140  loss  -0.4189249  grad l2 norm  2.3686972\n",
      "iter  141  loss  -0.3990327  grad l2 norm  2.2078269\n",
      "iter  142  loss  -0.3782423  grad l2 norm  2.0455332\n",
      "iter  143  loss  -0.35657707  grad l2 norm  1.8819213\n",
      "iter  144  loss  -0.33406126  grad l2 norm  1.7170825\n",
      "iter  145  loss  -0.3107231  grad l2 norm  1.5511588\n",
      "iter  146  loss  -0.28659752  grad l2 norm  1.3842874\n",
      "iter  147  loss  -0.26172528  grad l2 norm  1.2166827\n",
      "iter  148  loss  -0.23615484  grad l2 norm  1.0485933\n",
      "iter  149  loss  -0.2099439  grad l2 norm  0.88043284\n",
      "iter  150  loss  -0.18316454  grad l2 norm  0.7128497\n",
      "iter  151  loss  -0.15588622  grad l2 norm  0.54716694\n",
      "iter  152  loss  -0.12820332  grad l2 norm  0.3866679\n",
      "iter  153  loss  -0.10022043  grad l2 norm  0.24271491\n",
      "iter  154  loss  -0.07205024  grad l2 norm  0.16648626\n",
      "iter  155  loss  -0.043829497  grad l2 norm  0.2361117\n",
      "iter  156  loss  -0.015713645  grad l2 norm  0.37744668\n",
      "iter  157  loss  0.012123211  grad l2 norm  0.53565806\n",
      "iter  158  loss  0.039488927  grad l2 norm  0.6984352\n",
      "iter  159  loss  0.06617728  grad l2 norm  0.8621483\n",
      "iter  160  loss  0.09197465  grad l2 norm  1.0252807\n",
      "iter  161  loss  0.11667064  grad l2 norm  1.1869383\n",
      "iter  162  loss  0.14007166  grad l2 norm  1.346571\n",
      "iter  163  loss  0.16200846  grad l2 norm  1.5037135\n",
      "iter  164  loss  0.18235394  grad l2 norm  1.6580762\n",
      "iter  165  loss  0.20102277  grad l2 norm  1.8093982\n",
      "iter  166  loss  0.217974  grad l2 norm  1.9575087\n",
      "iter  167  loss  0.23320481  grad l2 norm  2.1022573\n",
      "iter  168  loss  0.24674307  grad l2 norm  2.243579\n",
      "iter  169  loss  0.25863773  grad l2 norm  2.3814037\n",
      "iter  170  loss  0.2689513  grad l2 norm  2.515725\n",
      "iter  171  loss  0.27775255  grad l2 norm  2.646514\n",
      "iter  172  loss  0.2851125  grad l2 norm  2.7737975\n",
      "iter  173  loss  0.2911014  grad l2 norm  2.8975635\n",
      "iter  174  loss  0.29578704  grad l2 norm  3.0178585\n",
      "iter  175  loss  0.29923442  grad l2 norm  3.1346693\n",
      "iter  176  loss  0.3015057  grad l2 norm  3.2480595\n",
      "iter  177  loss  0.30265898  grad l2 norm  3.3580096\n",
      "iter  178  loss  0.30275097  grad l2 norm  3.4645875\n",
      "iter  179  loss  0.30183274  grad l2 norm  3.56778\n",
      "iter  180  loss  0.299954  grad l2 norm  3.667645\n",
      "iter  181  loss  0.29716098  grad l2 norm  3.7641752\n",
      "iter  182  loss  0.29349935  grad l2 norm  3.8574245\n",
      "iter  183  loss  0.28901094  grad l2 norm  3.9473848\n",
      "iter  184  loss  0.28373772  grad l2 norm  4.0341096\n",
      "iter  185  loss  0.2777183  grad l2 norm  4.1175857\n",
      "iter  186  loss  0.27099177  grad l2 norm  4.1978717\n",
      "iter  187  loss  0.26359114  grad l2 norm  4.2750607\n",
      "iter  188  loss  0.25552422  grad l2 norm  4.349283\n",
      "iter  189  loss  0.24684694  grad l2 norm  4.4203253\n",
      "iter  190  loss  0.23758094  grad l2 norm  4.4884415\n",
      "iter  191  loss  0.22771403  grad l2 norm  4.553479\n",
      "iter  192  loss  0.21732242  grad l2 norm  4.615425\n",
      "iter  193  loss  0.20645295  grad l2 norm  4.674257\n",
      "iter  194  loss  0.19513768  grad l2 norm  4.7300324\n",
      "iter  195  loss  0.18340653  grad l2 norm  4.7827277\n",
      "iter  196  loss  0.17128928  grad l2 norm  4.8324037\n",
      "iter  197  loss  0.15881443  grad l2 norm  4.8790307\n",
      "iter  198  loss  0.14601077  grad l2 norm  4.922674\n",
      "iter  199  loss  0.13290572  grad l2 norm  4.9632983\n",
      "iter  200  loss  0.11952563  grad l2 norm  5.0009727\n",
      "iter  201  loss  0.105896525  grad l2 norm  5.0356584\n",
      "iter  202  loss  0.09204531  grad l2 norm  5.0674233\n",
      "iter  203  loss  0.077997364  grad l2 norm  5.0962315\n",
      "iter  204  loss  0.06377856  grad l2 norm  5.122147\n",
      "iter  205  loss  0.04941348  grad l2 norm  5.1451354\n",
      "iter  206  loss  0.034923267  grad l2 norm  5.1652584\n",
      "iter  207  loss  0.020319581  grad l2 norm  5.182669\n",
      "iter  208  loss  0.0055944184  grad l2 norm  5.1973243\n",
      "iter  209  loss  -0.009186581  grad l2 norm  5.2091002\n",
      "iter  210  loss  -0.023997545  grad l2 norm  5.218065\n",
      "iter  211  loss  -0.038822655  grad l2 norm  5.2241745\n",
      "iter  212  loss  -0.053661026  grad l2 norm  5.2274985\n",
      "iter  213  loss  -0.06846704  grad l2 norm  5.227993\n",
      "iter  214  loss  -0.08321621  grad l2 norm  5.2257266\n",
      "iter  215  loss  -0.097884364  grad l2 norm  5.2207\n",
      "iter  216  loss  -0.11249351  grad l2 norm  5.213147\n",
      "iter  217  loss  -0.12700394  grad l2 norm  5.2028112\n",
      "iter  218  loss  -0.14140034  grad l2 norm  5.189758\n",
      "iter  219  loss  -0.15565036  grad l2 norm  5.173945\n",
      "iter  220  loss  -0.16971944  grad l2 norm  5.155439\n",
      "iter  221  loss  -0.18358195  grad l2 norm  5.134196\n",
      "iter  222  loss  -0.19725655  grad l2 norm  5.110482\n",
      "iter  223  loss  -0.21076019  grad l2 norm  5.084126\n",
      "iter  224  loss  -0.22400479  grad l2 norm  5.055121\n",
      "iter  225  loss  -0.2369615  grad l2 norm  5.023422\n",
      "iter  226  loss  -0.24961151  grad l2 norm  4.9890985\n",
      "iter  227  loss  -0.26193586  grad l2 norm  4.9521074\n",
      "iter  228  loss  -0.27391505  grad l2 norm  4.912518\n",
      "iter  229  loss  -0.28553116  grad l2 norm  4.870286\n",
      "iter  230  loss  -0.2967649  grad l2 norm  4.8254824\n",
      "iter  231  loss  -0.30759785  grad l2 norm  4.7780633\n",
      "iter  232  loss  -0.3180116  grad l2 norm  4.7281\n",
      "iter  233  loss  -0.32798856  grad l2 norm  4.675546\n",
      "iter  234  loss  -0.3375103  grad l2 norm  4.6204777\n",
      "iter  235  loss  -0.34655887  grad l2 norm  4.562847\n",
      "iter  236  loss  -0.35511655  grad l2 norm  4.502731\n",
      "iter  237  loss  -0.36316693  grad l2 norm  4.4400835\n",
      "iter  238  loss  -0.37069246  grad l2 norm  4.3749814\n",
      "iter  239  loss  -0.377677  grad l2 norm  4.3073807\n",
      "iter  240  loss  -0.38410443  grad l2 norm  4.2373595\n",
      "iter  241  loss  -0.38995904  grad l2 norm  4.164872\n",
      "iter  242  loss  -0.39522535  grad l2 norm  4.090001\n",
      "iter  243  loss  -0.3998905  grad l2 norm  4.0127006\n",
      "iter  244  loss  -0.4039411  grad l2 norm  3.9330537\n",
      "iter  245  loss  -0.40736386  grad l2 norm  3.851017\n",
      "iter  246  loss  -0.4101458  grad l2 norm  3.7666728\n",
      "iter  247  loss  -0.4122737  grad l2 norm  3.6799848\n",
      "iter  248  loss  -0.4137355  grad l2 norm  3.5910335\n",
      "iter  249  loss  -0.41451952  grad l2 norm  3.4997864\n",
      "iter  250  loss  -0.41461498  grad l2 norm  3.4063268\n",
      "iter  251  loss  -0.4140117  grad l2 norm  3.3106241\n",
      "iter  252  loss  -0.4127002  grad l2 norm  3.2127655\n",
      "iter  253  loss  -0.410672  grad l2 norm  3.1127238\n",
      "iter  254  loss  -0.4079193  grad l2 norm  3.0105906\n",
      "iter  255  loss  -0.4044347  grad l2 norm  2.9063413\n",
      "iter  256  loss  -0.4002116  grad l2 norm  2.800073\n",
      "iter  257  loss  -0.39524645  grad l2 norm  2.691768\n",
      "iter  258  loss  -0.38953602  grad l2 norm  2.581528\n",
      "iter  259  loss  -0.3830782  grad l2 norm  2.4693458\n",
      "iter  260  loss  -0.3758718  grad l2 norm  2.3553298\n",
      "iter  261  loss  -0.36791798  grad l2 norm  2.2394843\n",
      "iter  262  loss  -0.35921988  grad l2 norm  2.1219296\n",
      "iter  263  loss  -0.34978256  grad l2 norm  2.0026846\n",
      "iter  264  loss  -0.33961287  grad l2 norm  1.8818876\n",
      "iter  265  loss  -0.32872078  grad l2 norm  1.7595809\n",
      "iter  266  loss  -0.3171179  grad l2 norm  1.6359295\n",
      "iter  267  loss  -0.30481896  grad l2 norm  1.5110189\n",
      "iter  268  loss  -0.29184148  grad l2 norm  1.3850605\n",
      "iter  269  loss  -0.27820882  grad l2 norm  1.2582203\n",
      "iter  270  loss  -0.2639497  grad l2 norm  1.1308088\n",
      "iter  271  loss  -0.24909756  grad l2 norm  1.0031669\n",
      "iter  272  loss  -0.23369129  grad l2 norm  0.8758602\n",
      "iter  273  loss  -0.21777685  grad l2 norm  0.7497144\n",
      "iter  274  loss  -0.2014086  grad l2 norm  0.62614703\n",
      "iter  275  loss  -0.18465015  grad l2 norm  0.5078077\n",
      "iter  276  loss  -0.16757584  grad l2 norm  0.40003914\n",
      "iter  277  loss  -0.1502729  grad l2 norm  0.31467837\n",
      "iter  278  loss  -0.13284132  grad l2 norm  0.2736854\n",
      "iter  279  loss  -0.11539475  grad l2 norm  0.29621658\n",
      "iter  280  loss  -0.09806039  grad l2 norm  0.37015054\n",
      "iter  281  loss  -0.08097875  grad l2 norm  0.47144756\n",
      "iter  282  loss  -0.06430021  grad l2 norm  0.5850323\n",
      "iter  283  loss  -0.048181158  grad l2 norm  0.7042707\n",
      "iter  284  loss  -0.03277749  grad l2 norm  0.82552785\n",
      "iter  285  loss  -0.018238561  grad l2 norm  0.94721097\n",
      "iter  286  loss  -0.0046998556  grad l2 norm  1.0679522\n",
      "iter  287  loss  0.007723164  grad l2 norm  1.1872231\n",
      "iter  288  loss  0.018941553  grad l2 norm  1.3042582\n",
      "iter  289  loss  0.028895644  grad l2 norm  1.4188739\n",
      "iter  290  loss  0.03755667  grad l2 norm  1.5305594\n",
      "iter  291  loss  0.04492416  grad l2 norm  1.6392988\n",
      "iter  292  loss  0.0510215  grad l2 norm  1.7447283\n",
      "iter  293  loss  0.055890016  grad l2 norm  1.8469331\n",
      "iter  294  loss  0.05958263  grad l2 norm  1.9456408\n",
      "iter  295  loss  0.062158737  grad l2 norm  2.0409963\n",
      "iter  296  loss  0.06368126  grad l2 norm  2.1327813\n",
      "iter  297  loss  0.064213045  grad l2 norm  2.221175\n",
      "iter  298  loss  0.06381495  grad l2 norm  2.3059888\n",
      "iter  299  loss  0.06254595  grad l2 norm  2.38742\n",
      "iter  300  loss  0.060462292  grad l2 norm  2.4652967\n",
      "iter  301  loss  0.05761751  grad l2 norm  2.5397801\n",
      "iter  302  loss  0.0540632  grad l2 norm  2.6107163\n",
      "iter  303  loss  0.049847387  grad l2 norm  2.678331\n",
      "iter  304  loss  0.045015793  grad l2 norm  2.7424796\n",
      "iter  305  loss  0.03961078  grad l2 norm  2.8033652\n",
      "iter  306  loss  0.03367399  grad l2 norm  2.8608537\n",
      "iter  307  loss  0.027244542  grad l2 norm  2.915158\n",
      "iter  308  loss  0.020360136  grad l2 norm  2.966127\n",
      "iter  309  loss  0.013057008  grad l2 norm  3.0139601\n",
      "iter  310  loss  0.005370226  grad l2 norm  3.058505\n",
      "iter  311  loss  -0.0026654433  grad l2 norm  3.0999591\n",
      "iter  312  loss  -0.01101653  grad l2 norm  3.138174\n",
      "iter  313  loss  -0.019651111  grad l2 norm  3.1733432\n",
      "iter  314  loss  -0.0285388  grad l2 norm  3.2053201\n",
      "iter  315  loss  -0.037649725  grad l2 norm  3.234289\n",
      "iter  316  loss  -0.046954468  grad l2 norm  3.260068\n",
      "iter  317  loss  -0.05642393  grad l2 norm  3.282882\n",
      "iter  318  loss  -0.06603046  grad l2 norm  3.302602\n",
      "iter  319  loss  -0.07574687  grad l2 norm  3.3194177\n",
      "iter  320  loss  -0.085546434  grad l2 norm  3.3331895\n",
      "iter  321  loss  -0.0954031  grad l2 norm  3.3440933\n",
      "iter  322  loss  -0.10529108  grad l2 norm  3.3519936\n",
      "iter  323  loss  -0.11518507  grad l2 norm  3.3570602\n",
      "iter  324  loss  -0.12506066  grad l2 norm  3.3591626\n",
      "iter  325  loss  -0.13489361  grad l2 norm  3.3584664\n",
      "iter  326  loss  -0.14465979  grad l2 norm  3.3548467\n",
      "iter  327  loss  -0.15433651  grad l2 norm  3.348464\n",
      "iter  328  loss  -0.16390054  grad l2 norm  3.3391986\n",
      "iter  329  loss  -0.17333004  grad l2 norm  3.3272057\n",
      "iter  330  loss  -0.18260205  grad l2 norm  3.3123724\n",
      "iter  331  loss  -0.19169523  grad l2 norm  3.2948477\n",
      "iter  332  loss  -0.2005873  grad l2 norm  3.274525\n",
      "iter  333  loss  -0.20925707  grad l2 norm  3.2515476\n",
      "iter  334  loss  -0.21768492  grad l2 norm  3.2258165\n",
      "iter  335  loss  -0.22585163  grad l2 norm  3.1974695\n",
      "iter  336  loss  -0.23373795  grad l2 norm  3.166414\n",
      "iter  337  loss  -0.24132565  grad l2 norm  3.1327822\n",
      "iter  338  loss  -0.24859408  grad l2 norm  3.0964892\n",
      "iter  339  loss  -0.25552437  grad l2 norm  3.0576632\n",
      "iter  340  loss  -0.2620979  grad l2 norm  3.016225\n",
      "iter  341  loss  -0.26829728  grad l2 norm  2.972298\n",
      "iter  342  loss  -0.27410573  grad l2 norm  2.925809\n",
      "iter  343  loss  -0.27950808  grad l2 norm  2.8768702\n",
      "iter  344  loss  -0.284489  grad l2 norm  2.825419\n",
      "iter  345  loss  -0.28903383  grad l2 norm  2.7715719\n",
      "iter  346  loss  -0.29312804  grad l2 norm  2.715279\n",
      "iter  347  loss  -0.29675707  grad l2 norm  2.6566546\n",
      "iter  348  loss  -0.29990724  grad l2 norm  2.5956547\n",
      "iter  349  loss  -0.30256566  grad l2 norm  2.5323892\n",
      "iter  350  loss  -0.30471963  grad l2 norm  2.4668205\n",
      "iter  351  loss  -0.30635798  grad l2 norm  2.399056\n",
      "iter  352  loss  -0.30746922  grad l2 norm  2.3290691\n",
      "iter  353  loss  -0.30804372  grad l2 norm  2.256967\n",
      "iter  354  loss  -0.30807212  grad l2 norm  2.1827362\n",
      "iter  355  loss  -0.30754706  grad l2 norm  2.1064851\n",
      "iter  356  loss  -0.3064614  grad l2 norm  2.028215\n",
      "iter  357  loss  -0.30480915  grad l2 norm  1.9480398\n",
      "iter  358  loss  -0.30258533  grad l2 norm  1.8659784\n",
      "iter  359  loss  -0.29978693  grad l2 norm  1.7821548\n",
      "iter  360  loss  -0.29641294  grad l2 norm  1.6966106\n",
      "iter  361  loss  -0.29246452  grad l2 norm  1.6094859\n",
      "iter  362  loss  -0.28794417  grad l2 norm  1.5208536\n",
      "iter  363  loss  -0.28285697  grad l2 norm  1.4308845\n",
      "iter  364  loss  -0.27721035  grad l2 norm  1.3397031\n",
      "iter  365  loss  -0.2710155  grad l2 norm  1.2475365\n",
      "iter  366  loss  -0.26428556  grad l2 norm  1.1545942\n",
      "iter  367  loss  -0.2570382  grad l2 norm  1.0612078\n",
      "iter  368  loss  -0.24929385  grad l2 norm  0.9677422\n",
      "iter  369  loss  -0.24107869  grad l2 norm  0.8747493\n",
      "iter  370  loss  -0.23242396  grad l2 norm  0.7829347\n",
      "iter  371  loss  -0.22336857  grad l2 norm  0.69337296\n",
      "iter  372  loss  -0.21395685  grad l2 norm  0.60763425\n",
      "iter  373  loss  -0.20424165  grad l2 norm  0.52815723\n",
      "iter  374  loss  -0.19428381  grad l2 norm  0.45868874\n",
      "iter  375  loss  -0.18415444  grad l2 norm  0.40478247\n",
      "iter  376  loss  -0.17393284  grad l2 norm  0.37340224\n",
      "iter  377  loss  -0.16370837  grad l2 norm  0.3701972\n",
      "iter  378  loss  -0.15357715  grad l2 norm  0.39555094\n",
      "iter  379  loss  -0.14364137  grad l2 norm  0.44397527\n",
      "iter  380  loss  -0.1340051  grad l2 norm  0.5082528\n",
      "iter  381  loss  -0.12477251  grad l2 norm  0.5823136\n",
      "iter  382  loss  -0.11604319  grad l2 norm  0.6620726\n",
      "iter  383  loss  -0.10791048  grad l2 norm  0.7447646\n",
      "iter  384  loss  -0.100457534  grad l2 norm  0.82866883\n",
      "iter  385  loss  -0.0937571  grad l2 norm  0.9125091\n",
      "iter  386  loss  -0.08786852  grad l2 norm  0.9954892\n",
      "iter  387  loss  -0.08283677  grad l2 norm  1.0769068\n",
      "iter  388  loss  -0.078690074  grad l2 norm  1.1563538\n",
      "iter  389  loss  -0.075440034  grad l2 norm  1.2333951\n",
      "iter  390  loss  -0.07308025  grad l2 norm  1.3078245\n",
      "iter  391  loss  -0.07158973  grad l2 norm  1.3793583\n",
      "iter  392  loss  -0.07093421  grad l2 norm  1.4479147\n",
      "iter  393  loss  -0.071070164  grad l2 norm  1.5133045\n",
      "iter  394  loss  -0.07194764  grad l2 norm  1.5755275\n",
      "iter  395  loss  -0.07351413  grad l2 norm  1.6344513\n",
      "iter  396  loss  -0.075715855  grad l2 norm  1.6901265\n",
      "iter  397  loss  -0.07850013  grad l2 norm  1.7424537\n",
      "iter  398  loss  -0.08181506  grad l2 norm  1.7915134\n",
      "iter  399  loss  -0.08561191  grad l2 norm  1.8372238\n",
      "iter  400  loss  -0.08984336  grad l2 norm  1.879686\n",
      "iter  401  loss  -0.094465144  grad l2 norm  1.9188274\n",
      "iter  402  loss  -0.099434786  grad l2 norm  1.9547573\n",
      "iter  403  loss  -0.10471202  grad l2 norm  1.9874091\n",
      "iter  404  loss  -0.11025908  grad l2 norm  2.016897\n",
      "iter  405  loss  -0.11603878  grad l2 norm  2.0430984\n",
      "iter  406  loss  -0.12201525  grad l2 norm  2.0661774\n",
      "iter  407  loss  -0.12815535  grad l2 norm  2.0860786\n",
      "iter  408  loss  -0.13442707  grad l2 norm  2.1029406\n",
      "iter  409  loss  -0.14079945  grad l2 norm  2.116643\n",
      "iter  410  loss  -0.1472419  grad l2 norm  2.1273274\n",
      "iter  411  loss  -0.15372479  grad l2 norm  2.1349373\n",
      "iter  412  loss  -0.1602197  grad l2 norm  2.1396027\n",
      "iter  413  loss  -0.16670041  grad l2 norm  2.1412697\n",
      "iter  414  loss  -0.17314112  grad l2 norm  2.1400738\n",
      "iter  415  loss  -0.17951757  grad l2 norm  2.1359615\n",
      "iter  416  loss  -0.18580538  grad l2 norm  2.1290596\n",
      "iter  417  loss  -0.19198123  grad l2 norm  2.1193058\n",
      "iter  418  loss  -0.19802184  grad l2 norm  2.1068218\n",
      "iter  419  loss  -0.20390467  grad l2 norm  2.0915456\n",
      "iter  420  loss  -0.2096076  grad l2 norm  2.0735989\n",
      "iter  421  loss  -0.2151097  grad l2 norm  2.0529244\n",
      "iter  422  loss  -0.22039053  grad l2 norm  2.029641\n",
      "iter  423  loss  -0.22543105  grad l2 norm  2.0036995\n",
      "iter  424  loss  -0.2302118  grad l2 norm  1.975218\n",
      "iter  425  loss  -0.23471503  grad l2 norm  1.9441546\n",
      "iter  426  loss  -0.23892243  grad l2 norm  1.9106251\n",
      "iter  427  loss  -0.24281828  grad l2 norm  1.8745967\n",
      "iter  428  loss  -0.24638586  grad l2 norm  1.836184\n",
      "iter  429  loss  -0.24961053  grad l2 norm  1.7953644\n",
      "iter  430  loss  -0.25247788  grad l2 norm  1.7522516\n",
      "iter  431  loss  -0.25497454  grad l2 norm  1.7068375\n",
      "iter  432  loss  -0.25708795  grad l2 norm  1.6592357\n",
      "iter  433  loss  -0.25880712  grad l2 norm  1.6094545\n",
      "iter  434  loss  -0.26012105  grad l2 norm  1.5576096\n",
      "iter  435  loss  -0.26102132  grad l2 norm  1.5037313\n",
      "iter  436  loss  -0.26149917  grad l2 norm  1.447938\n",
      "iter  437  loss  -0.2615489  grad l2 norm  1.3902903\n",
      "iter  438  loss  -0.26116514  grad l2 norm  1.3309119\n",
      "iter  439  loss  -0.26034546  grad l2 norm  1.269904\n",
      "iter  440  loss  -0.25908834  grad l2 norm  1.2074051\n",
      "iter  441  loss  -0.2573951  grad l2 norm  1.1435729\n",
      "iter  442  loss  -0.25526828  grad l2 norm  1.0785738\n",
      "iter  443  loss  -0.2527143  grad l2 norm  1.0126512\n",
      "iter  444  loss  -0.24974172  grad l2 norm  0.94602484\n",
      "iter  445  loss  -0.24636325  grad l2 norm  0.8790758\n",
      "iter  446  loss  -0.24259502  grad l2 norm  0.81213164\n",
      "iter  447  loss  -0.23845802  grad l2 norm  0.7458099\n",
      "iter  448  loss  -0.2339776  grad l2 norm  0.6806625\n",
      "iter  449  loss  -0.22918524  grad l2 norm  0.6177433\n",
      "iter  450  loss  -0.22411798  grad l2 norm  0.55807215\n",
      "iter  451  loss  -0.21882  grad l2 norm  0.50351185\n",
      "iter  452  loss  -0.21334139  grad l2 norm  0.45593742\n",
      "iter  453  loss  -0.20773962  grad l2 norm  0.41836366\n",
      "iter  454  loss  -0.20207718  grad l2 norm  0.39331105\n",
      "iter  455  loss  -0.19642124  grad l2 norm  0.38361216\n",
      "iter  456  loss  -0.19084108  grad l2 norm  0.38962466\n",
      "iter  457  loss  -0.1854062  grad l2 norm  0.41070127\n",
      "iter  458  loss  -0.18018395  grad l2 norm  0.44365034\n",
      "iter  459  loss  -0.17523803  grad l2 norm  0.4858664\n",
      "iter  460  loss  -0.17062747  grad l2 norm  0.5340217\n",
      "iter  461  loss  -0.16640684  grad l2 norm  0.58633524\n",
      "iter  462  loss  -0.16262539  grad l2 norm  0.64065903\n",
      "iter  463  loss  -0.15932798  grad l2 norm  0.6961839\n",
      "iter  464  loss  -0.15655361  grad l2 norm  0.75154036\n",
      "iter  465  loss  -0.1543327  grad l2 norm  0.80633557\n",
      "iter  466  loss  -0.15268777  grad l2 norm  0.8596634\n",
      "iter  467  loss  -0.15163147  grad l2 norm  0.91139\n",
      "iter  468  loss  -0.15116496  grad l2 norm  0.9608611\n",
      "iter  469  loss  -0.15127806  grad l2 norm  1.0080776\n",
      "iter  470  loss  -0.15194935  grad l2 norm  1.0525529\n",
      "iter  471  loss  -0.15314822  grad l2 norm  1.0943737\n",
      "iter  472  loss  -0.15483639  grad l2 norm  1.1331735\n",
      "iter  473  loss  -0.15697105  grad l2 norm  1.1690949\n",
      "iter  474  loss  -0.15950625  grad l2 norm  1.2018572\n",
      "iter  475  loss  -0.16239595  grad l2 norm  1.231634\n",
      "iter  476  loss  -0.16559376  grad l2 norm  1.2582033\n",
      "iter  477  loss  -0.16905554  grad l2 norm  1.2817541\n",
      "iter  478  loss  -0.17273834  grad l2 norm  1.3021042\n",
      "iter  479  loss  -0.17660195  grad l2 norm  1.3194474\n",
      "iter  480  loss  -0.18060742  grad l2 norm  1.3336313\n",
      "iter  481  loss  -0.18471861  grad l2 norm  1.3448478\n",
      "iter  482  loss  -0.18890052  grad l2 norm  1.3529669\n",
      "iter  483  loss  -0.19312048  grad l2 norm  1.3581772\n",
      "iter  484  loss  -0.19734718  grad l2 norm  1.360366\n",
      "iter  485  loss  -0.20155104  grad l2 norm  1.3597164\n",
      "iter  486  loss  -0.20570348  grad l2 norm  1.3561318\n",
      "iter  487  loss  -0.20977768  grad l2 norm  1.3497902\n",
      "iter  488  loss  -0.21374829  grad l2 norm  1.3406093\n",
      "iter  489  loss  -0.2175915  grad l2 norm  1.328762\n",
      "iter  490  loss  -0.22128408  grad l2 norm  1.31418\n",
      "iter  491  loss  -0.22480476  grad l2 norm  1.2970326\n",
      "iter  492  loss  -0.22813329  grad l2 norm  1.2772657\n",
      "iter  493  loss  -0.23125064  grad l2 norm  1.255047\n",
      "iter  494  loss  -0.23413865  grad l2 norm  1.2303375\n",
      "iter  495  loss  -0.23678033  grad l2 norm  1.2033062\n",
      "iter  496  loss  -0.23916009  grad l2 norm  1.1739314\n",
      "iter  497  loss  -0.2412637  grad l2 norm  1.1423867\n",
      "iter  498  loss  -0.24307802  grad l2 norm  1.1086714\n",
      "iter  499  loss  -0.24459203  grad l2 norm  1.0729681\n",
      "iter  500  loss  -0.24579573  grad l2 norm  1.0353019\n",
      "iter  501  loss  -0.24668148  grad l2 norm  0.9958724\n",
      "iter  502  loss  -0.24724299  grad l2 norm  0.95473945\n",
      "iter  503  loss  -0.24747628  grad l2 norm  0.9121292\n",
      "iter  504  loss  -0.24737948  grad l2 norm  0.86812407\n",
      "iter  505  loss  -0.24695365  grad l2 norm  0.82302445\n",
      "iter  506  loss  -0.24620222  grad l2 norm  0.77702296\n",
      "iter  507  loss  -0.24513237  grad l2 norm  0.73045444\n",
      "iter  508  loss  -0.24375425  grad l2 norm  0.68361217\n",
      "iter  509  loss  -0.24208274  grad l2 norm  0.6369554\n",
      "iter  510  loss  -0.24013653  grad l2 norm  0.5909549\n",
      "iter  511  loss  -0.23793972  grad l2 norm  0.54627746\n",
      "iter  512  loss  -0.2355212  grad l2 norm  0.50367737\n",
      "iter  513  loss  -0.23291536  grad l2 norm  0.46414307\n",
      "iter  514  loss  -0.23016128  grad l2 norm  0.42882314\n",
      "iter  515  loss  -0.22730267  grad l2 norm  0.3990708\n",
      "iter  516  loss  -0.22438595  grad l2 norm  0.37630737\n",
      "iter  517  loss  -0.22145924  grad l2 norm  0.36180872\n",
      "iter  518  loss  -0.21856995  grad l2 norm  0.3564253\n",
      "iter  519  loss  -0.21576346  grad l2 norm  0.3602388\n",
      "iter  520  loss  -0.21308169  grad l2 norm  0.3725949\n",
      "iter  521  loss  -0.2105633  grad l2 norm  0.39217094\n",
      "iter  522  loss  -0.2082431  grad l2 norm  0.4174582\n",
      "iter  523  loss  -0.20615341  grad l2 norm  0.44687623\n",
      "iter  524  loss  -0.20432386  grad l2 norm  0.47910875\n",
      "iter  525  loss  -0.2027825  grad l2 norm  0.51297194\n",
      "iter  526  loss  -0.20155415  grad l2 norm  0.5475791\n",
      "iter  527  loss  -0.20066094  grad l2 norm  0.58213806\n",
      "iter  528  loss  -0.20011985  grad l2 norm  0.61607707\n",
      "iter  529  loss  -0.19994225  grad l2 norm  0.6488636\n",
      "iter  530  loss  -0.2001323  grad l2 norm  0.6800732\n",
      "iter  531  loss  -0.20068605  grad l2 norm  0.70936006\n",
      "iter  532  loss  -0.20159161  grad l2 norm  0.7365028\n",
      "iter  533  loss  -0.20282951  grad l2 norm  0.7612726\n",
      "iter  534  loss  -0.20437296  grad l2 norm  0.78353894\n",
      "iter  535  loss  -0.20618895  grad l2 norm  0.80315506\n",
      "iter  536  loss  -0.20824012  grad l2 norm  0.82005405\n",
      "iter  537  loss  -0.21048689  grad l2 norm  0.83416\n",
      "iter  538  loss  -0.21288905  grad l2 norm  0.8454572\n",
      "iter  539  loss  -0.2154072  grad l2 norm  0.8539237\n",
      "iter  540  loss  -0.21800324  grad l2 norm  0.85957515\n",
      "iter  541  loss  -0.22064109  grad l2 norm  0.86242896\n",
      "iter  542  loss  -0.22328675  grad l2 norm  0.8625199\n",
      "iter  543  loss  -0.22590864  grad l2 norm  0.85989594\n",
      "iter  544  loss  -0.2284767  grad l2 norm  0.8546031\n",
      "iter  545  loss  -0.23096396  grad l2 norm  0.8467162\n",
      "iter  546  loss  -0.23334487  grad l2 norm  0.8362891\n",
      "iter  547  loss  -0.23559596  grad l2 norm  0.82342416\n",
      "iter  548  loss  -0.23769613  grad l2 norm  0.8081814\n",
      "iter  549  loss  -0.2396262  grad l2 norm  0.79069394\n",
      "iter  550  loss  -0.2413689  grad l2 norm  0.77102935\n",
      "iter  551  loss  -0.24290936  grad l2 norm  0.749358\n",
      "iter  552  loss  -0.2442348  grad l2 norm  0.72575885\n",
      "iter  553  loss  -0.24533473  grad l2 norm  0.7004493\n",
      "iter  554  loss  -0.24620134  grad l2 norm  0.6735262\n",
      "iter  555  loss  -0.24682952  grad l2 norm  0.64526945\n",
      "iter  556  loss  -0.24721722  grad l2 norm  0.6158042\n",
      "iter  557  loss  -0.24736528  grad l2 norm  0.58549416\n",
      "iter  558  loss  -0.2472787  grad l2 norm  0.55450875\n",
      "iter  559  loss  -0.24696593  grad l2 norm  0.52332526\n",
      "iter  560  loss  -0.24643971  grad l2 norm  0.49217853\n",
      "iter  561  loss  -0.24571747  grad l2 norm  0.46169633\n",
      "iter  562  loss  -0.24482098  grad l2 norm  0.4322004\n",
      "iter  563  loss  -0.24377596  grad l2 norm  0.40450066\n",
      "iter  564  loss  -0.24261242  grad l2 norm  0.37900814\n",
      "iter  565  loss  -0.2413628  grad l2 norm  0.3567011\n",
      "iter  566  loss  -0.24006037  grad l2 norm  0.33799493\n",
      "iter  567  loss  -0.23873785  grad l2 norm  0.32388029\n",
      "iter  568  loss  -0.23742579  grad l2 norm  0.31451598\n",
      "iter  569  loss  -0.23615164  grad l2 norm  0.31055006\n",
      "iter  570  loss  -0.23494002  grad l2 norm  0.31154487\n",
      "iter  571  loss  -0.23381251  grad l2 norm  0.31754112\n",
      "iter  572  loss  -0.23278856  grad l2 norm  0.32752007\n",
      "iter  573  loss  -0.2318866  grad l2 norm  0.3411353\n",
      "iter  574  loss  -0.23112415  grad l2 norm  0.3571707\n",
      "iter  575  loss  -0.2305179  grad l2 norm  0.3751946\n",
      "iter  576  loss  -0.23008418  grad l2 norm  0.39413193\n",
      "iter  577  loss  -0.2298378  grad l2 norm  0.4136604\n",
      "iter  578  loss  -0.22979075  grad l2 norm  0.4329167\n",
      "iter  579  loss  -0.22995126  grad l2 norm  0.4517017\n",
      "iter  580  loss  -0.2303234  grad l2 norm  0.4693453\n",
      "iter  581  loss  -0.23090538  grad l2 norm  0.48574245\n",
      "iter  582  loss  -0.23169029  grad l2 norm  0.5003819\n",
      "iter  583  loss  -0.23266453  grad l2 norm  0.5132332\n",
      "iter  584  loss  -0.23380825  grad l2 norm  0.52392215\n",
      "iter  585  loss  -0.23509623  grad l2 norm  0.5324872\n",
      "iter  586  loss  -0.2364989  grad l2 norm  0.53867376\n",
      "iter  587  loss  -0.23798397  grad l2 norm  0.5425824\n",
      "iter  588  loss  -0.23951861  grad l2 norm  0.5440557\n",
      "iter  589  loss  -0.24107002  grad l2 norm  0.54324186\n",
      "iter  590  loss  -0.24260734  grad l2 norm  0.5400606\n",
      "iter  591  loss  -0.24410191  grad l2 norm  0.5346959\n",
      "iter  592  loss  -0.2455276  grad l2 norm  0.527131\n",
      "iter  593  loss  -0.2468611  grad l2 norm  0.5175278\n",
      "iter  594  loss  -0.24808258  grad l2 norm  0.505992\n",
      "iter  595  loss  -0.24917544  grad l2 norm  0.4927674\n",
      "iter  596  loss  -0.25012678  grad l2 norm  0.4779523\n",
      "iter  597  loss  -0.25092673  grad l2 norm  0.46181434\n",
      "iter  598  loss  -0.2515699  grad l2 norm  0.44451717\n",
      "iter  599  loss  -0.25205436  grad l2 norm  0.426369\n",
      "iter  600  loss  -0.25238308  grad l2 norm  0.40760016\n",
      "iter  601  loss  -0.25256306  grad l2 norm  0.38855845\n",
      "iter  602  loss  -0.2526058  grad l2 norm  0.3695361\n",
      "iter  603  loss  -0.2525268  grad l2 norm  0.35090905\n",
      "iter  604  loss  -0.25234506  grad l2 norm  0.33301312\n",
      "iter  605  loss  -0.2520815  grad l2 norm  0.31622523\n",
      "iter  606  loss  -0.251758  grad l2 norm  0.3008924\n",
      "iter  607  loss  -0.2513953  grad l2 norm  0.2873544\n",
      "iter  608  loss  -0.25101233  grad l2 norm  0.275924\n",
      "iter  609  loss  -0.25062495  grad l2 norm  0.2668461\n",
      "iter  610  loss  -0.2502468  grad l2 norm  0.26032674\n",
      "iter  611  loss  -0.24988903  grad l2 norm  0.2564316\n",
      "iter  612  loss  -0.24956155  grad l2 norm  0.25516954\n",
      "iter  613  loss  -0.24927352  grad l2 norm  0.25636986\n",
      "iter  614  loss  -0.24903393  grad l2 norm  0.25981432\n",
      "iter  615  loss  -0.24885273  grad l2 norm  0.2651372\n",
      "iter  616  loss  -0.24874006  grad l2 norm  0.27196196\n",
      "iter  617  loss  -0.24870642  grad l2 norm  0.27984083\n",
      "iter  618  loss  -0.24876134  grad l2 norm  0.28835112\n",
      "iter  619  loss  -0.24891248  grad l2 norm  0.29707325\n",
      "iter  620  loss  -0.24916537  grad l2 norm  0.30562016\n",
      "iter  621  loss  -0.2495219  grad l2 norm  0.31365886\n",
      "iter  622  loss  -0.24998091  grad l2 norm  0.320873\n",
      "iter  623  loss  -0.2505374  grad l2 norm  0.32703003\n",
      "iter  624  loss  -0.251182  grad l2 norm  0.33188218\n",
      "iter  625  loss  -0.25190046  grad l2 norm  0.3353161\n",
      "iter  626  loss  -0.2526749  grad l2 norm  0.33717054\n",
      "iter  627  loss  -0.25348377  grad l2 norm  0.3374623\n",
      "iter  628  loss  -0.25430426  grad l2 norm  0.33611482\n",
      "iter  629  loss  -0.25511384  grad l2 norm  0.3332697\n",
      "iter  630  loss  -0.2558919  grad l2 norm  0.3289148\n",
      "iter  631  loss  -0.25662068  grad l2 norm  0.3232986\n",
      "iter  632  loss  -0.25728688  grad l2 norm  0.3164446\n",
      "iter  633  loss  -0.25788113  grad l2 norm  0.30868617\n",
      "iter  634  loss  -0.25839925  grad l2 norm  0.3000491\n",
      "iter  635  loss  -0.25884137  grad l2 norm  0.2909229\n",
      "iter  636  loss  -0.25921243  grad l2 norm  0.28129503\n",
      "iter  637  loss  -0.25952017  grad l2 norm  0.27157718\n",
      "iter  638  loss  -0.2597749  grad l2 norm  0.26168576\n",
      "iter  639  loss  -0.25998703  grad l2 norm  0.25203484\n",
      "iter  640  loss  -0.26016626  grad l2 norm  0.2424683\n",
      "iter  641  loss  -0.26031992  grad l2 norm  0.23341462\n",
      "iter  642  loss  -0.26045394  grad l2 norm  0.22467633\n",
      "iter  643  loss  -0.2605715  grad l2 norm  0.2167171\n",
      "iter  644  loss  -0.26067534  grad l2 norm  0.20932753\n",
      "iter  645  loss  -0.26076695  grad l2 norm  0.20300093\n",
      "iter  646  loss  -0.2608485  grad l2 norm  0.19751813\n",
      "iter  647  loss  -0.26092267  grad l2 norm  0.19335349\n",
      "iter  648  loss  -0.26099393  grad l2 norm  0.19023803\n",
      "iter  649  loss  -0.26106784  grad l2 norm  0.18853606\n",
      "iter  650  loss  -0.26115185  grad l2 norm  0.18787688\n",
      "iter  651  loss  -0.2612533  grad l2 norm  0.18845339\n",
      "iter  652  loss  -0.26137972  grad l2 norm  0.18980762\n",
      "iter  653  loss  -0.2615367  grad l2 norm  0.19198972\n",
      "iter  654  loss  -0.26172918  grad l2 norm  0.19452906\n",
      "iter  655  loss  -0.26195997  grad l2 norm  0.19739869\n",
      "iter  656  loss  -0.26223093  grad l2 norm  0.20018\n",
      "iter  657  loss  -0.2625418  grad l2 norm  0.20281988\n",
      "iter  658  loss  -0.26289007  grad l2 norm  0.20499806\n",
      "iter  659  loss  -0.26327077  grad l2 norm  0.2066884\n",
      "iter  660  loss  -0.2636765  grad l2 norm  0.20769562\n",
      "iter  661  loss  -0.26409945  grad l2 norm  0.20804098\n",
      "iter  662  loss  -0.2645321  grad l2 norm  0.20763145\n",
      "iter  663  loss  -0.2649689  grad l2 norm  0.20647582\n",
      "iter  664  loss  -0.26540598  grad l2 norm  0.20452487\n",
      "iter  665  loss  -0.2658416  grad l2 norm  0.20183562\n",
      "iter  666  loss  -0.26627415  grad l2 norm  0.19837807\n",
      "iter  667  loss  -0.2667014  grad l2 norm  0.19413342\n",
      "iter  668  loss  -0.2671194  grad l2 norm  0.18910712\n",
      "iter  669  loss  -0.26752347  grad l2 norm  0.1833077\n",
      "iter  670  loss  -0.26790762  grad l2 norm  0.1768085\n",
      "iter  671  loss  -0.26826584  grad l2 norm  0.16968292\n",
      "iter  672  loss  -0.26859254  grad l2 norm  0.16208874\n",
      "iter  673  loss  -0.2688831  grad l2 norm  0.15420027\n",
      "iter  674  loss  -0.26913434  grad l2 norm  0.14627115\n",
      "iter  675  loss  -0.26934612  grad l2 norm  0.1385966\n",
      "iter  676  loss  -0.26952034  grad l2 norm  0.13149613\n",
      "iter  677  loss  -0.26966202  grad l2 norm  0.12534909\n",
      "iter  678  loss  -0.26977807  grad l2 norm  0.12043386\n",
      "iter  679  loss  -0.2698767  grad l2 norm  0.11710165\n",
      "iter  680  loss  -0.26996684  grad l2 norm  0.11541024\n",
      "iter  681  loss  -0.27005744  grad l2 norm  0.11552389\n",
      "iter  682  loss  -0.27015817  grad l2 norm  0.11712775\n",
      "iter  683  loss  -0.27027917  grad l2 norm  0.12013369\n",
      "iter  684  loss  -0.2704312  grad l2 norm  0.12390699\n",
      "iter  685  loss  -0.2706244  grad l2 norm  0.12822294\n",
      "iter  686  loss  -0.2708668  grad l2 norm  0.13233696\n",
      "iter  687  loss  -0.2711623  grad l2 norm  0.13609433\n",
      "iter  688  loss  -0.27151033  grad l2 norm  0.1388393\n",
      "iter  689  loss  -0.27190432  grad l2 norm  0.14058854\n",
      "iter  690  loss  -0.2723338  grad l2 norm  0.14084879\n",
      "iter  691  loss  -0.2727849  grad l2 norm  0.13981508\n",
      "iter  692  loss  -0.27324268  grad l2 norm  0.13714798\n",
      "iter  693  loss  -0.2736919  grad l2 norm  0.13318628\n",
      "iter  694  loss  -0.274119  grad l2 norm  0.12772134\n",
      "iter  695  loss  -0.2745117  grad l2 norm  0.12122252\n",
      "iter  696  loss  -0.27486065  grad l2 norm  0.11362738\n",
      "iter  697  loss  -0.2751596  grad l2 norm  0.10556424\n",
      "iter  698  loss  -0.27540636  grad l2 norm  0.09715467\n",
      "iter  699  loss  -0.27560335  grad l2 norm  0.08921275\n",
      "iter  700  loss  -0.2757572  grad l2 norm  0.08205587\n",
      "iter  701  loss  -0.2758777  grad l2 norm  0.07660783\n",
      "iter  702  loss  -0.2759774  grad l2 norm  0.07317179\n",
      "iter  703  loss  -0.27606997  grad l2 norm  0.072280206\n",
      "iter  704  loss  -0.27617016  grad l2 norm  0.073598385\n",
      "iter  705  loss  -0.2762925  grad l2 norm  0.07676052\n",
      "iter  706  loss  -0.27644983  grad l2 norm  0.08086902\n",
      "iter  707  loss  -0.27665213  grad l2 norm  0.08524712\n",
      "iter  708  loss  -0.2769042  grad l2 norm  0.08914606\n",
      "iter  709  loss  -0.27720442  grad l2 norm  0.09212345\n",
      "iter  710  loss  -0.27754503  grad l2 norm  0.09379151\n",
      "iter  711  loss  -0.2779133  grad l2 norm  0.0940158\n",
      "iter  712  loss  -0.27829358  grad l2 norm  0.09260745\n",
      "iter  713  loss  -0.27866936  grad l2 norm  0.089775994\n",
      "iter  714  loss  -0.27902633  grad l2 norm  0.08568961\n",
      "iter  715  loss  -0.27935302  grad l2 norm  0.080706574\n",
      "iter  716  loss  -0.27964225  grad l2 norm  0.07507014\n",
      "iter  717  loss  -0.2798925  grad l2 norm  0.06939007\n",
      "iter  718  loss  -0.2801077  grad l2 norm  0.0639223\n",
      "iter  719  loss  -0.2802967  grad l2 norm  0.059419617\n",
      "iter  720  loss  -0.28047085  grad l2 norm  0.055864103\n",
      "iter  721  loss  -0.28064126  grad l2 norm  0.053873528\n",
      "iter  722  loss  -0.28081724  grad l2 norm  0.05287122\n",
      "iter  723  loss  -0.2810046  grad l2 norm  0.053208303\n",
      "iter  724  loss  -0.28120667  grad l2 norm  0.05394527\n",
      "iter  725  loss  -0.28142455  grad l2 norm  0.055321123\n",
      "iter  726  loss  -0.28165847  grad l2 norm  0.056393206\n",
      "iter  727  loss  -0.28190777  grad l2 norm  0.05744535\n",
      "iter  728  loss  -0.28217015  grad l2 norm  0.05769285\n",
      "iter  729  loss  -0.2824405  grad l2 norm  0.05755204\n",
      "iter  730  loss  -0.2827127  grad l2 norm  0.056528453\n",
      "iter  731  loss  -0.28298107  grad l2 norm  0.055180464\n",
      "iter  732  loss  -0.28324208  grad l2 norm  0.05319169\n",
      "iter  733  loss  -0.2834952  grad l2 norm  0.05108609\n",
      "iter  734  loss  -0.2837415  grad l2 norm  0.048539042\n",
      "iter  735  loss  -0.28398204  grad l2 norm  0.045968074\n",
      "iter  736  loss  -0.2842166  grad l2 norm  0.043090217\n",
      "iter  737  loss  -0.28444403  grad l2 norm  0.04043003\n",
      "iter  738  loss  -0.28466266  grad l2 norm  0.037893917\n",
      "iter  739  loss  -0.28487298  grad l2 norm  0.036169708\n",
      "iter  740  loss  -0.2850769  grad l2 norm  0.03510257\n",
      "iter  741  loss  -0.2852791  grad l2 norm  0.035147637\n",
      "iter  742  loss  -0.2854853  grad l2 norm  0.03562622\n",
      "iter  743  loss  -0.28570056  grad l2 norm  0.036611553\n",
      "iter  744  loss  -0.28592718  grad l2 norm  0.03720758\n",
      "iter  745  loss  -0.28616393  grad l2 norm  0.037569627\n",
      "iter  746  loss  -0.2864072  grad l2 norm  0.037073333\n",
      "iter  747  loss  -0.28665206  grad l2 norm  0.03625448\n",
      "iter  748  loss  -0.2868942  grad l2 norm  0.034648307\n",
      "iter  749  loss  -0.2871304  grad l2 norm  0.03295755\n",
      "iter  750  loss  -0.28735933  grad l2 norm  0.030755263\n",
      "iter  751  loss  -0.28758082  grad l2 norm  0.028889664\n",
      "iter  752  loss  -0.28779495  grad l2 norm  0.026940322\n",
      "iter  753  loss  -0.2880027  grad l2 norm  0.02591874\n",
      "iter  754  loss  -0.2882059  grad l2 norm  0.025255566\n",
      "iter  755  loss  -0.288408  grad l2 norm  0.025737958\n",
      "iter  756  loss  -0.28861347  grad l2 norm  0.02628346\n",
      "iter  757  loss  -0.28882572  grad l2 norm  0.027253358\n",
      "iter  758  loss  -0.28904653  grad l2 norm  0.027482297\n",
      "iter  759  loss  -0.28927395  grad l2 norm  0.027441941\n",
      "iter  760  loss  -0.28950405  grad l2 norm  0.026358472\n",
      "iter  761  loss  -0.2897308  grad l2 norm  0.025097694\n",
      "iter  762  loss  -0.28994924  grad l2 norm  0.023340605\n",
      "iter  763  loss  -0.29015753  grad l2 norm  0.022291085\n",
      "iter  764  loss  -0.290358  grad l2 norm  0.02157525\n",
      "iter  765  loss  -0.29055604  grad l2 norm  0.021946706\n",
      "iter  766  loss  -0.2907571  grad l2 norm  0.022278138\n",
      "iter  767  loss  -0.2909643  grad l2 norm  0.02289085\n",
      "iter  768  loss  -0.29117674  grad l2 norm  0.022738865\n",
      "iter  769  loss  -0.29139137  grad l2 norm  0.02259113\n",
      "iter  770  loss  -0.29160428  grad l2 norm  0.021751985\n",
      "iter  771  loss  -0.29181284  grad l2 norm  0.021319527\n",
      "iter  772  loss  -0.29201642  grad l2 norm  0.020623641\n",
      "iter  773  loss  -0.2922165  grad l2 norm  0.020674827\n",
      "iter  774  loss  -0.29241583  grad l2 norm  0.020498415\n",
      "iter  775  loss  -0.29261646  grad l2 norm  0.020824654\n",
      "iter  776  loss  -0.292819  grad l2 norm  0.020618945\n",
      "iter  777  loss  -0.2930224  grad l2 norm  0.020681892\n",
      "iter  778  loss  -0.29322484  grad l2 norm  0.020220075\n",
      "iter  779  loss  -0.2934252  grad l2 norm  0.020198124\n",
      "iter  780  loss  -0.29362327  grad l2 norm  0.019832635\n",
      "iter  781  loss  -0.29382014  grad l2 norm  0.019995907\n",
      "iter  782  loss  -0.2940167  grad l2 norm  0.019742424\n",
      "iter  783  loss  -0.29421335  grad l2 norm  0.01990661\n",
      "iter  784  loss  -0.29440996  grad l2 norm  0.019580819\n",
      "iter  785  loss  -0.29460552  grad l2 norm  0.019688634\n",
      "iter  786  loss  -0.2947998  grad l2 norm  0.01936698\n",
      "iter  787  loss  -0.29499313  grad l2 norm  0.019516125\n",
      "iter  788  loss  -0.29518583  grad l2 norm  0.01922748\n",
      "iter  789  loss  -0.2953779  grad l2 norm  0.019378511\n",
      "iter  790  loss  -0.2955692  grad l2 norm  0.019072644\n",
      "iter  791  loss  -0.29575965  grad l2 norm  0.019220045\n",
      "iter  792  loss  -0.29594922  grad l2 norm  0.018917702\n",
      "iter  793  loss  -0.29613808  grad l2 norm  0.01907815\n",
      "iter  794  loss  -0.29632616  grad l2 norm  0.018778864\n",
      "iter  795  loss  -0.29651362  grad l2 norm  0.018945249\n",
      "iter  796  loss  -0.29670024  grad l2 norm  0.018643621\n",
      "iter  797  loss  -0.29688627  grad l2 norm  0.018809224\n",
      "iter  798  loss  -0.2970717  grad l2 norm  0.018494667\n",
      "iter  799  loss  -0.29725623  grad l2 norm  0.018658353\n",
      "iter  800  loss  -0.29743993  grad l2 norm  0.018354306\n",
      "iter  801  loss  -0.29762253  grad l2 norm  0.018549439\n",
      "iter  802  loss  -0.29780453  grad l2 norm  0.018259844\n",
      "iter  803  loss  -0.29798657  grad l2 norm  0.01839231\n",
      "iter  804  loss  -0.29816777  grad l2 norm  0.018059172\n",
      "iter  805  loss  -0.29834795  grad l2 norm  0.018220767\n",
      "iter  806  loss  -0.29852712  grad l2 norm  0.017926032\n",
      "iter  807  loss  -0.29870534  grad l2 norm  0.018124199\n",
      "iter  808  loss  -0.29888272  grad l2 norm  0.017852008\n",
      "iter  809  loss  -0.2990602  grad l2 norm  0.017989084\n",
      "iter  810  loss  -0.29923695  grad l2 norm  0.01765999\n",
      "iter  811  loss  -0.29941234  grad l2 norm  0.017837547\n",
      "iter  812  loss  -0.2995866  grad l2 norm  0.017563837\n",
      "iter  813  loss  -0.29976007  grad l2 norm  0.017770512\n",
      "iter  814  loss  -0.2999331  grad l2 norm  0.017461486\n",
      "iter  815  loss  -0.300106  grad l2 norm  0.017597493\n",
      "iter  816  loss  -0.30027807  grad l2 norm  0.017255245\n",
      "iter  817  loss  -0.30044925  grad l2 norm  0.017411985\n",
      "iter  818  loss  -0.30061948  grad l2 norm  0.017104192\n",
      "iter  819  loss  -0.3007888  grad l2 norm  0.017286537\n",
      "iter  820  loss  -0.30095747  grad l2 norm  0.016999854\n",
      "iter  821  loss  -0.3011254  grad l2 norm  0.017175833\n",
      "iter  822  loss  -0.30129224  grad l2 norm  0.016911834\n",
      "iter  823  loss  -0.30145803  grad l2 norm  0.017137174\n",
      "iter  824  loss  -0.30162412  grad l2 norm  0.016816735\n",
      "iter  825  loss  -0.30179003  grad l2 norm  0.016920658\n",
      "iter  826  loss  -0.30195493  grad l2 norm  0.016593227\n",
      "iter  827  loss  -0.3021188  grad l2 norm  0.016742162\n",
      "iter  828  loss  -0.30228186  grad l2 norm  0.016438536\n",
      "iter  829  loss  -0.30244416  grad l2 norm  0.016606506\n",
      "iter  830  loss  -0.30260524  grad l2 norm  0.016361631\n",
      "iter  831  loss  -0.30276385  grad l2 norm  0.01669247\n",
      "iter  832  loss  -0.30292574  grad l2 norm  0.016269756\n",
      "iter  833  loss  -0.30308616  grad l2 norm  0.016368847\n",
      "iter  834  loss  -0.30324477  grad l2 norm  0.016094431\n",
      "iter  835  loss  -0.3034026  grad l2 norm  0.016271718\n",
      "iter  836  loss  -0.30356017  grad l2 norm  0.015981149\n",
      "iter  837  loss  -0.3037171  grad l2 norm  0.016111946\n",
      "iter  838  loss  -0.30387348  grad l2 norm  0.015804704\n",
      "iter  839  loss  -0.30402893  grad l2 norm  0.0159532\n",
      "iter  840  loss  -0.3041836  grad l2 norm  0.015666734\n",
      "iter  841  loss  -0.30433753  grad l2 norm  0.015826931\n",
      "iter  842  loss  -0.30449083  grad l2 norm  0.015548412\n",
      "iter  843  loss  -0.30464336  grad l2 norm  0.015709851\n",
      "iter  844  loss  -0.30479518  grad l2 norm  0.015435244\n",
      "iter  845  loss  -0.30494633  grad l2 norm  0.015607349\n",
      "iter  846  loss  -0.3050964  grad l2 norm  0.015375648\n",
      "iter  847  loss  -0.30524522  grad l2 norm  0.015615284\n",
      "iter  848  loss  -0.3053951  grad l2 norm  0.015281711\n",
      "iter  849  loss  -0.30554464  grad l2 norm  0.015359133\n",
      "iter  850  loss  -0.3056927  grad l2 norm  0.015056671\n",
      "iter  851  loss  -0.30583987  grad l2 norm  0.015206161\n",
      "iter  852  loss  -0.30598614  grad l2 norm  0.014926355\n",
      "iter  853  loss  -0.30613178  grad l2 norm  0.015084312\n",
      "iter  854  loss  -0.30627683  grad l2 norm  0.014810426\n",
      "iter  855  loss  -0.30642128  grad l2 norm  0.014969231\n",
      "iter  856  loss  -0.30656517  grad l2 norm  0.014699913\n",
      "iter  857  loss  -0.30670834  grad l2 norm  0.014860761\n",
      "iter  858  loss  -0.30685097  grad l2 norm  0.014600304\n",
      "iter  859  loss  -0.30699253  grad l2 norm  0.014775097\n",
      "iter  860  loss  -0.30713347  grad l2 norm  0.01454134\n",
      "iter  861  loss  -0.30727422  grad l2 norm  0.01468577\n",
      "iter  862  loss  -0.30741483  grad l2 norm  0.014373968\n",
      "iter  863  loss  -0.30755448  grad l2 norm  0.014506906\n",
      "iter  864  loss  -0.3076933  grad l2 norm  0.0142318215\n",
      "iter  865  loss  -0.30783144  grad l2 norm  0.0143898735\n",
      "iter  866  loss  -0.307969  grad l2 norm  0.014124249\n",
      "iter  867  loss  -0.3081059  grad l2 norm  0.014285829\n",
      "iter  868  loss  -0.30824217  grad l2 norm  0.014023903\n",
      "iter  869  loss  -0.30837783  grad l2 norm  0.014188151\n",
      "iter  870  loss  -0.308513  grad l2 norm  0.013931659\n",
      "iter  871  loss  -0.30864745  grad l2 norm  0.014106247\n",
      "iter  872  loss  -0.30878142  grad l2 norm  0.013858731\n",
      "iter  873  loss  -0.308915  grad l2 norm  0.0140145\n",
      "iter  874  loss  -0.30904827  grad l2 norm  0.0137291625\n",
      "iter  875  loss  -0.3091809  grad l2 norm  0.0138719\n",
      "iter  876  loss  -0.30931288  grad l2 norm  0.013601047\n",
      "iter  877  loss  -0.30944428  grad l2 norm  0.013758533\n",
      "iter  878  loss  -0.30957502  grad l2 norm  0.013497207\n",
      "iter  879  loss  -0.30970532  grad l2 norm  0.013659146\n",
      "iter  880  loss  -0.30983496  grad l2 norm  0.013402514\n",
      "iter  881  loss  -0.30996412  grad l2 norm  0.013568232\n",
      "iter  882  loss  -0.3100926  grad l2 norm  0.013318584\n",
      "iter  883  loss  -0.31022057  grad l2 norm  0.013489247\n",
      "iter  884  loss  -0.310348  grad l2 norm  0.013236303\n",
      "iter  885  loss  -0.3104751  grad l2 norm  0.013389489\n",
      "iter  886  loss  -0.31060174  grad l2 norm  0.013121296\n",
      "iter  887  loss  -0.31072786  grad l2 norm  0.013273376\n",
      "iter  888  loss  -0.31085327  grad l2 norm  0.013013712\n",
      "iter  889  loss  -0.31097826  grad l2 norm  0.013172613\n",
      "iter  890  loss  -0.31110272  grad l2 norm  0.012918983\n",
      "iter  891  loss  -0.31122667  grad l2 norm  0.0130797615\n",
      "iter  892  loss  -0.31135005  grad l2 norm  0.0128314085\n",
      "iter  893  loss  -0.3114729  grad l2 norm  0.01299464\n",
      "iter  894  loss  -0.3115952  grad l2 norm  0.012751319\n",
      "iter  895  loss  -0.31171715  grad l2 norm  0.012910629\n",
      "iter  896  loss  -0.31183863  grad l2 norm  0.012661281\n",
      "iter  897  loss  -0.31195968  grad l2 norm  0.012808321\n",
      "iter  898  loss  -0.31208023  grad l2 norm  0.012556617\n",
      "iter  899  loss  -0.31220034  grad l2 norm  0.012703789\n",
      "iter  900  loss  -0.31231987  grad l2 norm  0.012459058\n",
      "iter  901  loss  -0.3124389  grad l2 norm  0.012609132\n",
      "iter  902  loss  -0.31255755  grad l2 norm  0.012369994\n",
      "iter  903  loss  -0.31267554  grad l2 norm  0.012521091\n",
      "iter  904  loss  -0.31279314  grad l2 norm  0.012287182\n",
      "iter  905  loss  -0.3129103  grad l2 norm  0.01243737\n",
      "iter  906  loss  -0.3130269  grad l2 norm  0.012205108\n",
      "iter  907  loss  -0.3131432  grad l2 norm  0.012348094\n",
      "iter  908  loss  -0.31325883  grad l2 norm  0.012112636\n",
      "iter  909  loss  -0.3133743  grad l2 norm  0.012249331\n",
      "iter  910  loss  -0.3134891  grad l2 norm  0.012016413\n",
      "iter  911  loss  -0.31360352  grad l2 norm  0.0121525815\n",
      "iter  912  loss  -0.3137174  grad l2 norm  0.011925214\n",
      "iter  913  loss  -0.3138308  grad l2 norm  0.012061625\n",
      "iter  914  loss  -0.31394383  grad l2 norm  0.011839622\n",
      "iter  915  loss  -0.31405625  grad l2 norm  0.011975903\n",
      "iter  916  loss  -0.3141683  grad l2 norm  0.011758198\n",
      "iter  917  loss  -0.3142799  grad l2 norm  0.011891679\n",
      "iter  918  loss  -0.31439105  grad l2 norm  0.011674553\n",
      "iter  919  loss  -0.3145018  grad l2 norm  0.011802257\n",
      "iter  920  loss  -0.314612  grad l2 norm  0.011583818\n",
      "iter  921  loss  -0.31472164  grad l2 norm  0.011707\n",
      "iter  922  loss  -0.3148308  grad l2 norm  0.011490888\n",
      "iter  923  loss  -0.31493956  grad l2 norm  0.011615909\n",
      "iter  924  loss  -0.31504762  grad l2 norm  0.011404317\n",
      "iter  925  loss  -0.31515524  grad l2 norm  0.011527948\n",
      "iter  926  loss  -0.31526223  grad l2 norm  0.011322295\n",
      "iter  927  loss  -0.31536883  grad l2 norm  0.011444385\n",
      "iter  928  loss  -0.31547478  grad l2 norm  0.011242058\n",
      "iter  929  loss  -0.31558058  grad l2 norm  0.011360042\n",
      "iter  930  loss  -0.31568578  grad l2 norm  0.011159086\n",
      "iter  931  loss  -0.3157908  grad l2 norm  0.011272026\n",
      "iter  932  loss  -0.31589544  grad l2 norm  0.011073253\n",
      "iter  933  loss  -0.3159998  grad l2 norm  0.01118287\n",
      "iter  934  loss  -0.31610355  grad l2 norm  0.01098823\n",
      "iter  935  loss  -0.31620708  grad l2 norm  0.011095726\n",
      "iter  936  loss  -0.31631026  grad l2 norm  0.0109057855\n",
      "iter  937  loss  -0.3164129  grad l2 norm  0.011011582\n",
      "iter  938  loss  -0.3165151  grad l2 norm  0.0108265355\n",
      "iter  939  loss  -0.31661704  grad l2 norm  0.010930221\n",
      "iter  940  loss  -0.31671834  grad l2 norm  0.010748618\n",
      "iter  941  loss  -0.31681937  grad l2 norm  0.010849215\n",
      "iter  942  loss  -0.31691995  grad l2 norm  0.010670106\n",
      "iter  943  loss  -0.3170201  grad l2 norm  0.010768106\n",
      "iter  944  loss  -0.3171199  grad l2 norm  0.010591845\n",
      "iter  945  loss  -0.31721926  grad l2 norm  0.010687867\n",
      "iter  946  loss  -0.3173182  grad l2 norm  0.01051439\n",
      "iter  947  loss  -0.31741685  grad l2 norm  0.010608328\n",
      "iter  948  loss  -0.31751502  grad l2 norm  0.010438093\n",
      "iter  949  loss  -0.31761286  grad l2 norm  0.010529848\n",
      "iter  950  loss  -0.31771028  grad l2 norm  0.010362616\n",
      "iter  951  loss  -0.3178074  grad l2 norm  0.010451261\n",
      "iter  952  loss  -0.31790417  grad l2 norm  0.010286649\n",
      "iter  953  loss  -0.31800053  grad l2 norm  0.010371862\n",
      "iter  954  loss  -0.31809634  grad l2 norm  0.010210157\n",
      "iter  955  loss  -0.31819192  grad l2 norm  0.010292103\n",
      "iter  956  loss  -0.31828704  grad l2 norm  0.01013378\n",
      "iter  957  loss  -0.31838176  grad l2 norm  0.010212684\n",
      "iter  958  loss  -0.31847623  grad l2 norm  0.0100577185\n",
      "iter  959  loss  -0.3185702  grad l2 norm  0.010133407\n",
      "iter  960  loss  -0.31866378  grad l2 norm  0.009981861\n",
      "iter  961  loss  -0.31875712  grad l2 norm  0.0100545315\n",
      "iter  962  loss  -0.31884998  grad l2 norm  0.009906324\n",
      "iter  963  loss  -0.31894243  grad l2 norm  0.009975374\n",
      "iter  964  loss  -0.31903443  grad l2 norm  0.009831018\n",
      "iter  965  loss  -0.31912622  grad l2 norm  0.009897692\n",
      "iter  966  loss  -0.31921756  grad l2 norm  0.009756411\n",
      "iter  967  loss  -0.31930843  grad l2 norm  0.009820299\n",
      "iter  968  loss  -0.31939894  grad l2 norm  0.009681823\n",
      "iter  969  loss  -0.31948924  grad l2 norm  0.009743344\n",
      "iter  970  loss  -0.31957892  grad l2 norm  0.00960801\n",
      "iter  971  loss  -0.3196685  grad l2 norm  0.009667284\n",
      "iter  972  loss  -0.31975752  grad l2 norm  0.009535882\n",
      "iter  973  loss  -0.3198463  grad l2 norm  0.009592895\n",
      "iter  974  loss  -0.31993455  grad l2 norm  0.00946543\n",
      "iter  975  loss  -0.3200226  grad l2 norm  0.009520053\n",
      "iter  976  loss  -0.32011017  grad l2 norm  0.00939647\n",
      "iter  977  loss  -0.3201975  grad l2 norm  0.009448461\n",
      "iter  978  loss  -0.32028437  grad l2 norm  0.009328418\n",
      "iter  979  loss  -0.32037103  grad l2 norm  0.009377406\n",
      "iter  980  loss  -0.32045722  grad l2 norm  0.009260563\n",
      "iter  981  loss  -0.32054323  grad l2 norm  0.009306669\n",
      "iter  982  loss  -0.32062873  grad l2 norm  0.00919326\n",
      "iter  983  loss  -0.32071403  grad l2 norm  0.0092369635\n",
      "iter  984  loss  -0.32079887  grad l2 norm  0.009127356\n",
      "iter  985  loss  -0.32088348  grad l2 norm  0.009168995\n",
      "iter  986  loss  -0.32096764  grad l2 norm  0.009063184\n",
      "iter  987  loss  -0.3210517  grad l2 norm  0.009102856\n",
      "iter  988  loss  -0.32113516  grad l2 norm  0.009000633\n",
      "iter  989  loss  -0.32121864  grad l2 norm  0.009038411\n",
      "iter  990  loss  -0.32130155  grad l2 norm  0.008939097\n",
      "iter  991  loss  -0.3213843  grad l2 norm  0.008974672\n",
      "iter  992  loss  -0.32146665  grad l2 norm  0.008878014\n",
      "iter  993  loss  -0.3215489  grad l2 norm  0.00891156\n",
      "iter  994  loss  -0.3216307  grad l2 norm  0.008817662\n",
      "iter  995  loss  -0.32171214  grad l2 norm  0.008849641\n",
      "iter  996  loss  -0.32179338  grad l2 norm  0.008758754\n",
      "iter  997  loss  -0.3218744  grad l2 norm  0.008789421\n",
      "iter  998  loss  -0.32195503  grad l2 norm  0.008701361\n",
      "iter  999  loss  -0.32203543  grad l2 norm  0.008730551\n",
      "iter  1000  loss  -0.32211545  grad l2 norm  0.008645085\n",
      "iter  1001  loss  -0.3221954  grad l2 norm  0.008672727\n",
      "iter  1002  loss  -0.32227492  grad l2 norm  0.008588989\n",
      "iter  1003  loss  -0.32235426  grad l2 norm  0.008615588\n",
      "iter  1004  loss  -0.32243317  grad l2 norm  0.008532725\n",
      "iter  1005  loss  -0.32251206  grad l2 norm  0.0085579185\n",
      "iter  1006  loss  -0.3225905  grad l2 norm  0.00847568\n",
      "iter  1007  loss  -0.32266882  grad l2 norm  0.008500664\n",
      "iter  1008  loss  -0.32274666  grad l2 norm  0.008420669\n",
      "iter  1009  loss  -0.32282448  grad l2 norm  0.008445233\n",
      "iter  1010  loss  -0.32290182  grad l2 norm  0.008367562\n",
      "iter  1011  loss  -0.3229791  grad l2 norm  0.008392277\n",
      "iter  1012  loss  -0.32305598  grad l2 norm  0.008316835\n",
      "iter  1013  loss  -0.3231327  grad l2 norm  0.008341374\n",
      "iter  1014  loss  -0.32320902  grad l2 norm  0.008267278\n",
      "iter  1015  loss  -0.3232853  grad l2 norm  0.008291226\n",
      "iter  1016  loss  -0.3233611  grad l2 norm  0.008217912\n",
      "iter  1017  loss  -0.32343683  grad l2 norm  0.008241275\n",
      "iter  1018  loss  -0.32351202  grad l2 norm  0.008167064\n",
      "iter  1019  loss  -0.32358715  grad l2 norm  0.008188935\n",
      "iter  1020  loss  -0.32366177  grad l2 norm  0.008116124\n",
      "iter  1021  loss  -0.32373637  grad l2 norm  0.008139409\n",
      "iter  1022  loss  -0.3238104  grad l2 norm  0.008066471\n",
      "iter  1023  loss  -0.3238843  grad l2 norm  0.008089388\n",
      "iter  1024  loss  -0.32395783  grad l2 norm  0.008017646\n",
      "iter  1025  loss  -0.32403123  grad l2 norm  0.008038617\n",
      "iter  1026  loss  -0.32410416  grad l2 norm  0.0079680085\n",
      "iter  1027  loss  -0.32417706  grad l2 norm  0.007989701\n",
      "iter  1028  loss  -0.3242494  grad l2 norm  0.007921522\n",
      "iter  1029  loss  -0.32432172  grad l2 norm  0.007942562\n",
      "iter  1030  loss  -0.32439363  grad l2 norm  0.00787381\n",
      "iter  1031  loss  -0.3244655  grad l2 norm  0.0078938855\n",
      "iter  1032  loss  -0.32453698  grad l2 norm  0.007824338\n",
      "iter  1033  loss  -0.32460845  grad l2 norm  0.007843929\n",
      "iter  1034  loss  -0.32467932  grad l2 norm  0.0077741654\n",
      "iter  1035  loss  -0.32475027  grad l2 norm  0.007793789\n",
      "iter  1036  loss  -0.32482076  grad l2 norm  0.007724911\n",
      "iter  1037  loss  -0.3248912  grad l2 norm  0.0077452213\n",
      "iter  1038  loss  -0.32496113  grad l2 norm  0.007677372\n",
      "iter  1039  loss  -0.32503095  grad l2 norm  0.0076981843\n",
      "iter  1040  loss  -0.32510036  grad l2 norm  0.007631592\n",
      "iter  1041  loss  -0.32516968  grad l2 norm  0.0076526883\n",
      "iter  1042  loss  -0.32523847  grad l2 norm  0.0075871814\n",
      "iter  1043  loss  -0.32530734  grad l2 norm  0.0076081613\n",
      "iter  1044  loss  -0.32537553  grad l2 norm  0.007543522\n",
      "iter  1045  loss  -0.3254438  grad l2 norm  0.0075642713\n",
      "iter  1046  loss  -0.3255116  grad l2 norm  0.0075006518\n",
      "iter  1047  loss  -0.32557917  grad l2 norm  0.0075205024\n",
      "iter  1048  loss  -0.3256465  grad l2 norm  0.007456991\n",
      "iter  1049  loss  -0.32571375  grad l2 norm  0.00747548\n",
      "iter  1050  loss  -0.32578048  grad l2 norm  0.0074123824\n",
      "iter  1051  loss  -0.3258472  grad l2 norm  0.007428814\n",
      "iter  1052  loss  -0.32591343  grad l2 norm  0.0073641217\n",
      "iter  1053  loss  -0.3259796  grad l2 norm  0.007379303\n",
      "iter  1054  loss  -0.32604548  grad l2 norm  0.007315383\n",
      "iter  1055  loss  -0.32611117  grad l2 norm  0.007331143\n",
      "iter  1056  loss  -0.32617635  grad l2 norm  0.0072700037\n",
      "iter  1057  loss  -0.3262416  grad l2 norm  0.0072871586\n",
      "iter  1058  loss  -0.32630637  grad l2 norm  0.007229121\n",
      "iter  1059  loss  -0.32637095  grad l2 norm  0.007247388\n",
      "iter  1060  loss  -0.32643524  grad l2 norm  0.0071915365\n",
      "iter  1061  loss  -0.3264994  grad l2 norm  0.007209999\n",
      "iter  1062  loss  -0.32656303  grad l2 norm  0.0071547674\n",
      "iter  1063  loss  -0.32662672  grad l2 norm  0.0071723685\n",
      "iter  1064  loss  -0.32668984  grad l2 norm  0.007117153\n",
      "iter  1065  loss  -0.32675305  grad l2 norm  0.0071335514\n",
      "iter  1066  loss  -0.32681572  grad l2 norm  0.0070782495\n",
      "iter  1067  loss  -0.32687846  grad l2 norm  0.0070943036\n",
      "iter  1068  loss  -0.32694063  grad l2 norm  0.007039607\n",
      "iter  1069  loss  -0.32700282  grad l2 norm  0.007055826\n",
      "iter  1070  loss  -0.32706445  grad l2 norm  0.0070016943\n",
      "iter  1071  loss  -0.32712615  grad l2 norm  0.007017945\n",
      "iter  1072  loss  -0.32718736  grad l2 norm  0.0069633573\n",
      "iter  1073  loss  -0.32724857  grad l2 norm  0.0069808415\n",
      "iter  1074  loss  -0.32730922  grad l2 norm  0.006925194\n",
      "iter  1075  loss  -0.32737  grad l2 norm  0.00694388\n",
      "iter  1076  loss  -0.3274302  grad l2 norm  0.0068894867\n",
      "iter  1077  loss  -0.32749036  grad l2 norm  0.006912599\n",
      "iter  1078  loss  -0.32755  grad l2 norm  0.0068628825\n",
      "iter  1079  loss  -0.32760975  grad l2 norm  0.006887232\n",
      "iter  1080  loss  -0.32766894  grad l2 norm  0.006837064\n",
      "iter  1081  loss  -0.32772815  grad l2 norm  0.0068614623\n",
      "iter  1082  loss  -0.32778698  grad l2 norm  0.0068102162\n",
      "iter  1083  loss  -0.32784566  grad l2 norm  0.0068325177\n",
      "iter  1084  loss  -0.32790402  grad l2 norm  0.0067782803\n",
      "iter  1085  loss  -0.3279623  grad l2 norm  0.0067973076\n",
      "iter  1086  loss  -0.3280202  grad l2 norm  0.006742986\n",
      "iter  1087  loss  -0.32807815  grad l2 norm  0.006760604\n",
      "iter  1088  loss  -0.32813567  grad l2 norm  0.006707791\n",
      "iter  1089  loss  -0.32819316  grad l2 norm  0.0067281295\n",
      "iter  1090  loss  -0.32825023  grad l2 norm  0.006678765\n",
      "iter  1091  loss  -0.32830733  grad l2 norm  0.006702704\n",
      "iter  1092  loss  -0.32836398  grad l2 norm  0.0066554206\n",
      "iter  1093  loss  -0.3284207  grad l2 norm  0.0066810944\n",
      "iter  1094  loss  -0.3284769  grad l2 norm  0.0066334386\n",
      "iter  1095  loss  -0.32853314  grad l2 norm  0.0066593667\n",
      "iter  1096  loss  -0.32858905  grad l2 norm  0.0066101733\n",
      "iter  1097  loss  -0.32864508  grad l2 norm  0.006636438\n",
      "iter  1098  loss  -0.32870054  grad l2 norm  0.006585683\n",
      "iter  1099  loss  -0.32875618  grad l2 norm  0.0066130115\n",
      "iter  1100  loss  -0.3288113  grad l2 norm  0.006561679\n",
      "iter  1101  loss  -0.3288665  grad l2 norm  0.006590724\n",
      "iter  1102  loss  -0.32892132  grad l2 norm  0.006538986\n",
      "iter  1103  loss  -0.32897618  grad l2 norm  0.0065696263\n",
      "iter  1104  loss  -0.32903057  grad l2 norm  0.00651692\n",
      "iter  1105  loss  -0.32908508  grad l2 norm  0.0065486985\n",
      "iter  1106  loss  -0.32913902  grad l2 norm  0.006494529\n",
      "iter  1107  loss  -0.32919317  grad l2 norm  0.0065273684\n",
      "iter  1108  loss  -0.32924682  grad l2 norm  0.0064718183\n",
      "iter  1109  loss  -0.32930067  grad l2 norm  0.006506552\n",
      "iter  1110  loss  -0.32935384  grad l2 norm  0.006450049\n",
      "iter  1111  loss  -0.3294072  grad l2 norm  0.006487497\n",
      "iter  1112  loss  -0.3294601  grad l2 norm  0.0064300257\n",
      "iter  1113  loss  -0.32951313  grad l2 norm  0.0064701037\n",
      "iter  1114  loss  -0.32956558  grad l2 norm  0.0064108665\n",
      "iter  1115  loss  -0.32961804  grad l2 norm  0.0064522983\n",
      "iter  1116  loss  -0.32967022  grad l2 norm  0.006390168\n",
      "iter  1117  loss  -0.32972243  grad l2 norm  0.006432429\n",
      "iter  1118  loss  -0.3297742  grad l2 norm  0.006367199\n",
      "iter  1119  loss  -0.329826  grad l2 norm  0.006410805\n",
      "iter  1120  loss  -0.32987732  grad l2 norm  0.0063434276\n",
      "iter  1121  loss  -0.32992882  grad l2 norm  0.0063899546\n",
      "iter  1122  loss  -0.32997975  grad l2 norm  0.0063218614\n",
      "iter  1123  loss  -0.33003077  grad l2 norm  0.006372175\n",
      "iter  1124  loss  -0.3300814  grad l2 norm  0.0063036727\n",
      "iter  1125  loss  -0.330132  grad l2 norm  0.0063572633\n",
      "iter  1126  loss  -0.33018214  grad l2 norm  0.006287541\n",
      "iter  1127  loss  -0.33023247  grad l2 norm  0.0063434565\n",
      "iter  1128  loss  -0.3302822  grad l2 norm  0.0062714666\n",
      "iter  1129  loss  -0.3303321  grad l2 norm  0.0063288957\n",
      "iter  1130  loss  -0.3303815  grad l2 norm  0.006254401\n",
      "iter  1131  loss  -0.33043104  grad l2 norm  0.00631344\n",
      "iter  1132  loss  -0.33048016  grad l2 norm  0.006237333\n",
      "iter  1133  loss  -0.3305292  grad l2 norm  0.006298934\n",
      "iter  1134  loss  -0.33057782  grad l2 norm  0.006221894\n",
      "iter  1135  loss  -0.33062667  grad l2 norm  0.006286033\n",
      "iter  1136  loss  -0.3306749  grad l2 norm  0.006207509\n",
      "iter  1137  loss  -0.3307234  grad l2 norm  0.006272805\n",
      "iter  1138  loss  -0.3307714  grad l2 norm  0.0061915447\n",
      "iter  1139  loss  -0.33081943  grad l2 norm  0.006257161\n",
      "iter  1140  loss  -0.33086717  grad l2 norm  0.006173203\n",
      "iter  1141  loss  -0.3309148  grad l2 norm  0.006240328\n",
      "iter  1142  loss  -0.33096212  grad l2 norm  0.006155422\n",
      "iter  1143  loss  -0.33100945  grad l2 norm  0.0062255887\n",
      "iter  1144  loss  -0.33105642  grad l2 norm  0.006141109\n",
      "iter  1145  loss  -0.33110356  grad l2 norm  0.006214667\n",
      "iter  1146  loss  -0.33115003  grad l2 norm  0.0061306125\n",
      "iter  1147  loss  -0.3311968  grad l2 norm  0.006206789\n",
      "iter  1148  loss  -0.33124304  grad l2 norm  0.0061220634\n",
      "iter  1149  loss  -0.33128953  grad l2 norm  0.0061990437\n",
      "iter  1150  loss  -0.33133548  grad l2 norm  0.006112343\n",
      "iter  1151  loss  -0.33138147  grad l2 norm  0.006189342\n",
      "iter  1152  loss  -0.33142716  grad l2 norm  0.0061004036\n",
      "iter  1153  loss  -0.331473  grad l2 norm  0.0061780396\n",
      "iter  1154  loss  -0.33151838  grad l2 norm  0.006088151\n",
      "iter  1155  loss  -0.33156377  grad l2 norm  0.0061673773\n",
      "iter  1156  loss  -0.3316088  grad l2 norm  0.0060768775\n",
      "iter  1157  loss  -0.331654  grad l2 norm  0.0061574504\n",
      "iter  1158  loss  -0.33169872  grad l2 norm  0.006065584\n",
      "iter  1159  loss  -0.3317436  grad l2 norm  0.0061467746\n",
      "iter  1160  loss  -0.33178806  grad l2 norm  0.0060531567\n",
      "iter  1161  loss  -0.33183265  grad l2 norm  0.006135264\n",
      "iter  1162  loss  -0.33187675  grad l2 norm  0.0060405545\n",
      "iter  1163  loss  -0.3319211  grad l2 norm  0.006124576\n",
      "iter  1164  loss  -0.3319649  grad l2 norm  0.0060298108\n",
      "iter  1165  loss  -0.33200893  grad l2 norm  0.006116375\n",
      "iter  1166  loss  -0.33205235  grad l2 norm  0.006022565\n",
      "iter  1167  loss  -0.33209604  grad l2 norm  0.0061113625\n",
      "iter  1168  loss  -0.33213934  grad l2 norm  0.0060175494\n",
      "iter  1169  loss  -0.33218274  grad l2 norm  0.0061072167\n",
      "iter  1170  loss  -0.33222568  grad l2 norm  0.0060116113\n",
      "iter  1171  loss  -0.33226883  grad l2 norm  0.006100717\n",
      "iter  1172  loss  -0.33231148  grad l2 norm  0.0060026306\n",
      "iter  1173  loss  -0.3323544  grad l2 norm  0.0060910936\n",
      "iter  1174  loss  -0.33239686  grad l2 norm  0.005991616\n",
      "iter  1175  loss  -0.33243954  grad l2 norm  0.006081155\n",
      "iter  1176  loss  -0.33248162  grad l2 norm  0.0059819827\n",
      "iter  1177  loss  -0.33252388  grad l2 norm  0.0060734646\n",
      "iter  1178  loss  -0.33256584  grad l2 norm  0.0059747337\n",
      "iter  1179  loss  -0.33260807  grad l2 norm  0.006067335\n",
      "iter  1180  loss  -0.33264956  grad l2 norm  0.005967526\n",
      "iter  1181  loss  -0.33269143  grad l2 norm  0.006060135\n",
      "iter  1182  loss  -0.33273277  grad l2 norm  0.005958497\n",
      "iter  1183  loss  -0.33277437  grad l2 norm  0.006051328\n",
      "iter  1184  loss  -0.3328156  grad l2 norm  0.0059488113\n",
      "iter  1185  loss  -0.33285683  grad l2 norm  0.006042927\n",
      "iter  1186  loss  -0.33289766  grad l2 norm  0.005940476\n",
      "iter  1187  loss  -0.33293888  grad l2 norm  0.0060361545\n",
      "iter  1188  loss  -0.33297944  grad l2 norm  0.0059336787\n",
      "iter  1189  loss  -0.33302018  grad l2 norm  0.0060301726\n",
      "iter  1190  loss  -0.33306068  grad l2 norm  0.005926986\n",
      "iter  1191  loss  -0.3331012  grad l2 norm  0.0060235737\n",
      "iter  1192  loss  -0.33314136  grad l2 norm  0.005919493\n",
      "iter  1193  loss  -0.33318165  grad l2 norm  0.006016257\n",
      "iter  1194  loss  -0.33322155  grad l2 norm  0.005911823\n",
      "iter  1195  loss  -0.3332617  grad l2 norm  0.0060095536\n",
      "iter  1196  loss  -0.33330145  grad l2 norm  0.00590531\n",
      "iter  1197  loss  -0.3333413  grad l2 norm  0.006004139\n",
      "iter  1198  loss  -0.33338064  grad l2 norm  0.005900262\n",
      "iter  1199  loss  -0.33342028  grad l2 norm  0.005999151\n",
      "iter  1200  loss  -0.33345947  grad l2 norm  0.0058944435\n",
      "iter  1201  loss  -0.33349887  grad l2 norm  0.005992105\n",
      "iter  1202  loss  -0.33353788  grad l2 norm  0.005885407\n",
      "iter  1203  loss  -0.33357707  grad l2 norm  0.005981447\n",
      "iter  1204  loss  -0.33361575  grad l2 norm  0.005873197\n",
      "iter  1205  loss  -0.33365476  grad l2 norm  0.00596906\n",
      "iter  1206  loss  -0.33369324  grad l2 norm  0.0058609126\n",
      "iter  1207  loss  -0.333732  grad l2 norm  0.005958229\n",
      "iter  1208  loss  -0.33377025  grad l2 norm  0.005851766\n",
      "iter  1209  loss  -0.3338088  grad l2 norm  0.0059512495\n",
      "iter  1210  loss  -0.33384684  grad l2 norm  0.0058465623\n",
      "iter  1211  loss  -0.333885  grad l2 norm  0.0059477766\n",
      "iter  1212  loss  -0.33392283  grad l2 norm  0.005843847\n",
      "iter  1213  loss  -0.33396083  grad l2 norm  0.00594486\n",
      "iter  1214  loss  -0.33399838  grad l2 norm  0.0058403593\n",
      "iter  1215  loss  -0.33403617  grad l2 norm  0.0059398646\n",
      "iter  1216  loss  -0.33407354  grad l2 norm  0.0058342647\n",
      "iter  1217  loss  -0.33411124  grad l2 norm  0.005932756\n",
      "iter  1218  loss  -0.33414826  grad l2 norm  0.0058274856\n",
      "iter  1219  loss  -0.33418572  grad l2 norm  0.005925744\n",
      "iter  1220  loss  -0.33422273  grad l2 norm  0.0058206944\n",
      "iter  1221  loss  -0.3342599  grad l2 norm  0.0059177554\n",
      "iter  1222  loss  -0.3342966  grad l2 norm  0.0058120717\n",
      "iter  1223  loss  -0.3343336  grad l2 norm  0.005907218\n",
      "iter  1224  loss  -0.3343701  grad l2 norm  0.0058006328\n",
      "iter  1225  loss  -0.33440694  grad l2 norm  0.0058950977\n",
      "iter  1226  loss  -0.33444315  grad l2 norm  0.0057889824\n",
      "iter  1227  loss  -0.33447978  grad l2 norm  0.005884526\n",
      "iter  1228  loss  -0.3345159  grad l2 norm  0.0057803434\n",
      "iter  1229  loss  -0.33455226  grad l2 norm  0.0058777933\n",
      "iter  1230  loss  -0.3345882  grad l2 norm  0.0057759057\n",
      "iter  1231  loss  -0.33462438  grad l2 norm  0.0058749286\n",
      "iter  1232  loss  -0.33465996  grad l2 norm  0.005774516\n",
      "iter  1233  loss  -0.33469585  grad l2 norm  0.0058732624\n",
      "iter  1234  loss  -0.33473137  grad l2 norm  0.00577229\n",
      "iter  1235  loss  -0.33476713  grad l2 norm  0.0058690854\n",
      "iter  1236  loss  -0.33480242  grad l2 norm  0.0057670125\n",
      "iter  1237  loss  -0.33483797  grad l2 norm  0.005861786\n",
      "iter  1238  loss  -0.33487308  grad l2 norm  0.005759318\n",
      "iter  1239  loss  -0.33490852  grad l2 norm  0.0058534667\n",
      "iter  1240  loss  -0.33494338  grad l2 norm  0.0057522845\n",
      "iter  1241  loss  -0.33497858  grad l2 norm  0.0058466536\n",
      "iter  1242  loss  -0.33501333  grad l2 norm  0.0057472126\n",
      "iter  1243  loss  -0.33504835  grad l2 norm  0.00584107\n",
      "iter  1244  loss  -0.33508283  grad l2 norm  0.0057415045\n",
      "iter  1245  loss  -0.3351177  grad l2 norm  0.0058334684\n",
      "iter  1246  loss  -0.33515194  grad l2 norm  0.0057330015\n",
      "iter  1247  loss  -0.3351867  grad l2 norm  0.0058235875\n",
      "iter  1248  loss  -0.3352208  grad l2 norm  0.005723239\n",
      "iter  1249  loss  -0.33525527  grad l2 norm  0.005814026\n",
      "iter  1250  loss  -0.33528927  grad l2 norm  0.0057153828\n",
      "iter  1251  loss  -0.33532348  grad l2 norm  0.0058074365\n",
      "iter  1252  loss  -0.33535728  grad l2 norm  0.005710807\n",
      "iter  1253  loss  -0.33539137  grad l2 norm  0.0058036353\n",
      "iter  1254  loss  -0.335425  grad l2 norm  0.0057086144\n",
      "iter  1255  loss  -0.33545884  grad l2 norm  0.0058009555\n",
      "iter  1256  loss  -0.33549228  grad l2 norm  0.0057061063\n",
      "iter  1257  loss  -0.335526  grad l2 norm  0.0057963785\n",
      "iter  1258  loss  -0.33555922  grad l2 norm  0.0057009473\n",
      "iter  1259  loss  -0.3355928  grad l2 norm  0.005789325\n",
      "iter  1260  loss  -0.3356259  grad l2 norm  0.0056940746\n",
      "iter  1261  loss  -0.33565924  grad l2 norm  0.005782173\n",
      "iter  1262  loss  -0.3356921  grad l2 norm  0.0056887525\n",
      "iter  1263  loss  -0.3357253  grad l2 norm  0.0057770577\n",
      "iter  1264  loss  -0.33575812  grad l2 norm  0.005684911\n",
      "iter  1265  loss  -0.33579105  grad l2 norm  0.0057721115\n",
      "iter  1266  loss  -0.33582366  grad l2 norm  0.0056793373\n",
      "iter  1267  loss  -0.33585647  grad l2 norm  0.0057641775\n",
      "iter  1268  loss  -0.33588892  grad l2 norm  0.005670209\n",
      "iter  1269  loss  -0.3359215  grad l2 norm  0.005753941\n",
      "iter  1270  loss  -0.3359538  grad l2 norm  0.0056604557\n",
      "iter  1271  loss  -0.33598632  grad l2 norm  0.0057440866\n",
      "iter  1272  loss  -0.3360184  grad l2 norm  0.0056528486\n",
      "iter  1273  loss  -0.33605075  grad l2 norm  0.0057374286\n",
      "iter  1274  loss  -0.33608258  grad l2 norm  0.0056490153\n",
      "iter  1275  loss  -0.33611473  grad l2 norm  0.005734346\n",
      "iter  1276  loss  -0.3361464  grad l2 norm  0.0056480397\n",
      "iter  1277  loss  -0.33617836  grad l2 norm  0.0057326313\n",
      "iter  1278  loss  -0.33620995  grad l2 norm  0.0056468113\n",
      "iter  1279  loss  -0.33624172  grad l2 norm  0.0057289903\n",
      "iter  1280  loss  -0.3362731  grad l2 norm  0.0056422767\n",
      "iter  1281  loss  -0.3363048  grad l2 norm  0.005721471\n",
      "iter  1282  loss  -0.336336  grad l2 norm  0.005634998\n",
      "iter  1283  loss  -0.3363676  grad l2 norm  0.005714051\n",
      "iter  1284  loss  -0.33639872  grad l2 norm  0.005630431\n",
      "iter  1285  loss  -0.33642995  grad l2 norm  0.005710289\n",
      "iter  1286  loss  -0.33646092  grad l2 norm  0.005628956\n",
      "iter  1287  loss  -0.33649206  grad l2 norm  0.00570734\n",
      "iter  1288  loss  -0.3365228  grad l2 norm  0.0056251115\n",
      "iter  1289  loss  -0.33655378  grad l2 norm  0.005699751\n",
      "iter  1290  loss  -0.33658445  grad l2 norm  0.0056153904\n",
      "iter  1291  loss  -0.3366153  grad l2 norm  0.0056870733\n",
      "iter  1292  loss  -0.3366458  grad l2 norm  0.005602558\n",
      "iter  1293  loss  -0.33667657  grad l2 norm  0.005674803\n",
      "iter  1294  loss  -0.33670688  grad l2 norm  0.005593449\n",
      "iter  1295  loss  -0.33673736  grad l2 norm  0.005668067\n",
      "iter  1296  loss  -0.33676755  grad l2 norm  0.0055910745\n",
      "iter  1297  loss  -0.3367979  grad l2 norm  0.005666908\n",
      "iter  1298  loss  -0.3368278  grad l2 norm  0.0055918326\n",
      "iter  1299  loss  -0.33685806  grad l2 norm  0.005666274\n",
      "iter  1300  loss  -0.3368879  grad l2 norm  0.0055906037\n",
      "iter  1301  loss  -0.33691785  grad l2 norm  0.005661422\n",
      "iter  1302  loss  -0.33694756  grad l2 norm  0.005584363\n",
      "iter  1303  loss  -0.33697754  grad l2 norm  0.005652834\n",
      "iter  1304  loss  -0.33700714  grad l2 norm  0.005576824\n",
      "iter  1305  loss  -0.33703694  grad l2 norm  0.0056455703\n",
      "iter  1306  loss  -0.33706632  grad l2 norm  0.0055727507\n",
      "iter  1307  loss  -0.337096  grad l2 norm  0.005641766\n",
      "iter  1308  loss  -0.33712527  grad l2 norm  0.005570517\n",
      "iter  1309  loss  -0.33715466  grad l2 norm  0.005637827\n",
      "iter  1310  loss  -0.3371838  grad l2 norm  0.0055654817\n",
      "iter  1311  loss  -0.3372132  grad l2 norm  0.0056294478\n",
      "iter  1312  loss  -0.33724213  grad l2 norm  0.005555455\n",
      "iter  1313  loss  -0.3372713  grad l2 norm  0.0056173597\n",
      "iter  1314  loss  -0.33730024  grad l2 norm  0.0055442024\n",
      "iter  1315  loss  -0.33732927  grad l2 norm  0.005607044\n",
      "iter  1316  loss  -0.33735788  grad l2 norm  0.0055375895\n",
      "iter  1317  loss  -0.33738682  grad l2 norm  0.0056024725\n",
      "iter  1318  loss  -0.33741537  grad l2 norm  0.0055366666\n",
      "iter  1319  loss  -0.33744413  grad l2 norm  0.005601788\n",
      "iter  1320  loss  -0.33747253  grad l2 norm  0.0055371593\n",
      "iter  1321  loss  -0.3375011  grad l2 norm  0.005600115\n",
      "iter  1322  loss  -0.3375294  grad l2 norm  0.00553472\n",
      "iter  1323  loss  -0.33755794  grad l2 norm  0.005594095\n",
      "iter  1324  loss  -0.33758602  grad l2 norm  0.005527905\n",
      "iter  1325  loss  -0.33761442  grad l2 norm  0.0055854246\n",
      "iter  1326  loss  -0.33764234  grad l2 norm  0.005521093\n",
      "iter  1327  loss  -0.33767053  grad l2 norm  0.0055794106\n",
      "iter  1328  loss  -0.33769843  grad l2 norm  0.0055183694\n",
      "iter  1329  loss  -0.3377266  grad l2 norm  0.0055768583\n",
      "iter  1330  loss  -0.33775422  grad l2 norm  0.005517098\n",
      "iter  1331  loss  -0.33778223  grad l2 norm  0.0055730827\n",
      "iter  1332  loss  -0.33780983  grad l2 norm  0.0055236057\n",
      "iter  1333  loss  -0.3378375  grad l2 norm  0.0055935956\n",
      "iter  1334  loss  -0.33786494  grad l2 norm  0.005546262\n",
      "iter  1335  loss  -0.33789262  grad l2 norm  0.0056038015\n",
      "iter  1336  loss  -0.33791998  grad l2 norm  0.005532208\n",
      "iter  1337  loss  -0.33794764  grad l2 norm  0.005565579\n",
      "iter  1338  loss  -0.3379749  grad l2 norm  0.0054889168\n",
      "iter  1339  loss  -0.33800232  grad l2 norm  0.0055297506\n",
      "iter  1340  loss  -0.3380294  grad l2 norm  0.005471586\n",
      "iter  1341  loss  -0.33805656  grad l2 norm  0.005529011\n",
      "iter  1342  loss  -0.33808357  grad l2 norm  0.0054858346\n",
      "iter  1343  loss  -0.33811054  grad l2 norm  0.005550859\n",
      "iter  1344  loss  -0.3381372  grad l2 norm  0.0055104704\n",
      "iter  1345  loss  -0.33816412  grad l2 norm  0.0055691716\n",
      "iter  1346  loss  -0.33819082  grad l2 norm  0.005518926\n",
      "iter  1347  loss  -0.3382178  grad l2 norm  0.0055607283\n",
      "iter  1348  loss  -0.33824444  grad l2 norm  0.005497125\n",
      "iter  1349  loss  -0.33827123  grad l2 norm  0.005529828\n",
      "iter  1350  loss  -0.33829767  grad l2 norm  0.0054701115\n",
      "iter  1351  loss  -0.3383243  grad l2 norm  0.005512575\n",
      "iter  1352  loss  -0.3383506  grad l2 norm  0.005463838\n",
      "iter  1353  loss  -0.3383769  grad l2 norm  0.0055128196\n",
      "iter  1354  loss  -0.33840308  grad l2 norm  0.0054674903\n",
      "iter  1355  loss  -0.33842942  grad l2 norm  0.0055156793\n",
      "iter  1356  loss  -0.33845547  grad l2 norm  0.0054727783\n",
      "iter  1357  loss  -0.3384817  grad l2 norm  0.0055218874\n",
      "iter  1358  loss  -0.3385076  grad l2 norm  0.005479286\n",
      "iter  1359  loss  -0.3385337  grad l2 norm  0.0055187135\n",
      "iter  1360  loss  -0.33855957  grad l2 norm  0.005462439\n",
      "iter  1361  loss  -0.33858564  grad l2 norm  0.005491362\n",
      "iter  1362  loss  -0.33861127  grad l2 norm  0.005438057\n",
      "iter  1363  loss  -0.3386371  grad l2 norm  0.005475738\n",
      "iter  1364  loss  -0.33866268  grad l2 norm  0.00543363\n",
      "iter  1365  loss  -0.33868843  grad l2 norm  0.0054762606\n",
      "iter  1366  loss  -0.33871382  grad l2 norm  0.005438284\n",
      "iter  1367  loss  -0.33873937  grad l2 norm  0.0054822355\n",
      "iter  1368  loss  -0.33876467  grad l2 norm  0.0054504722\n",
      "iter  1369  loss  -0.33879012  grad l2 norm  0.0054994985\n",
      "iter  1370  loss  -0.33881533  grad l2 norm  0.005470366\n",
      "iter  1371  loss  -0.33884075  grad l2 norm  0.005505641\n",
      "iter  1372  loss  -0.3388658  grad l2 norm  0.0054544793\n",
      "iter  1373  loss  -0.33889118  grad l2 norm  0.005472847\n",
      "iter  1374  loss  -0.3389161  grad l2 norm  0.005423362\n",
      "iter  1375  loss  -0.33894125  grad l2 norm  0.005451058\n",
      "iter  1376  loss  -0.33896613  grad l2 norm  0.0054120016\n",
      "iter  1377  loss  -0.3389911  grad l2 norm  0.005443446\n",
      "iter  1378  loss  -0.33901587  grad l2 norm  0.0054076537\n",
      "iter  1379  loss  -0.33904076  grad l2 norm  0.0054425867\n",
      "iter  1380  loss  -0.33906528  grad l2 norm  0.005418107\n",
      "iter  1381  loss  -0.33909014  grad l2 norm  0.005466929\n",
      "iter  1382  loss  -0.3391145  grad l2 norm  0.0054507516\n",
      "iter  1383  loss  -0.33913913  grad l2 norm  0.005480799\n",
      "iter  1384  loss  -0.33916363  grad l2 norm  0.0054302067\n",
      "iter  1385  loss  -0.33918828  grad l2 norm  0.0054371185\n",
      "iter  1386  loss  -0.3392125  grad l2 norm  0.0053922976\n",
      "iter  1387  loss  -0.339237  grad l2 norm  0.005414417\n",
      "iter  1388  loss  -0.3392611  grad l2 norm  0.0053833453\n",
      "iter  1389  loss  -0.33928537  grad l2 norm  0.0054093506\n",
      "iter  1390  loss  -0.33930948  grad l2 norm  0.0053813886\n",
      "iter  1391  loss  -0.33933368  grad l2 norm  0.005411151\n",
      "iter  1392  loss  -0.33935744  grad l2 norm  0.0053982385\n",
      "iter  1393  loss  -0.33938152  grad l2 norm  0.005450476\n",
      "iter  1394  loss  -0.33940515  grad l2 norm  0.0054562613\n",
      "iter  1395  loss  -0.33942914  grad l2 norm  0.0054851943\n",
      "iter  1396  loss  -0.33945295  grad l2 norm  0.0054321894\n",
      "iter  1397  loss  -0.33947688  grad l2 norm  0.005419221\n",
      "iter  1398  loss  -0.33950052  grad l2 norm  0.005372958\n",
      "iter  1399  loss  -0.33952442  grad l2 norm  0.00538351\n",
      "iter  1400  loss  -0.33954784  grad l2 norm  0.0053546443\n",
      "iter  1401  loss  -0.33957145  grad l2 norm  0.0053700497\n",
      "iter  1402  loss  -0.33959484  grad l2 norm  0.005343897\n",
      "iter  1403  loss  -0.33961833  grad l2 norm  0.005363329\n",
      "iter  1404  loss  -0.33964154  grad l2 norm  0.0053531216\n",
      "iter  1405  loss  -0.3396648  grad l2 norm  0.005404863\n",
      "iter  1406  loss  -0.33968762  grad l2 norm  0.005453872\n",
      "iter  1407  loss  -0.33971053  grad l2 norm  0.0055111344\n",
      "iter  1408  loss  -0.33973354  grad l2 norm  0.0054589957\n",
      "iter  1409  loss  -0.33975688  grad l2 norm  0.0054137018\n",
      "iter  1410  loss  -0.33977985  grad l2 norm  0.005349219\n",
      "iter  1411  loss  -0.33980283  grad l2 norm  0.0053463154\n",
      "iter  1412  loss  -0.33982554  grad l2 norm  0.005318056\n",
      "iter  1413  loss  -0.3398485  grad l2 norm  0.005330482\n",
      "iter  1414  loss  -0.33987105  grad l2 norm  0.005311621\n",
      "iter  1415  loss  -0.33989382  grad l2 norm  0.0053262124\n",
      "iter  1416  loss  -0.33991623  grad l2 norm  0.0053161997\n",
      "iter  1417  loss  -0.33993885  grad l2 norm  0.0053413725\n",
      "iter  1418  loss  -0.339961  grad l2 norm  0.00536565\n",
      "iter  1419  loss  -0.33998322  grad l2 norm  0.0054631988\n",
      "iter  1420  loss  -0.34000495  grad l2 norm  0.0055547603\n",
      "iter  1421  loss  -0.34002757  grad l2 norm  0.005509887\n",
      "iter  1422  loss  -0.34005025  grad l2 norm  0.0053701154\n",
      "iter  1423  loss  -0.3400728  grad l2 norm  0.005314735\n",
      "iter  1424  loss  -0.34009498  grad l2 norm  0.0052864444\n",
      "iter  1425  loss  -0.3401172  grad l2 norm  0.00529437\n",
      "iter  1426  loss  -0.34013897  grad l2 norm  0.0052876733\n",
      "iter  1427  loss  -0.34016103  grad l2 norm  0.005299385\n",
      "iter  1428  loss  -0.34018287  grad l2 norm  0.0052919677\n",
      "iter  1429  loss  -0.3402048  grad l2 norm  0.0053026774\n",
      "iter  1430  loss  -0.34022644  grad l2 norm  0.0053091864\n",
      "iter  1431  loss  -0.340248  grad l2 norm  0.005379717\n",
      "iter  1432  loss  -0.3402689  grad l2 norm  0.0055620647\n",
      "iter  1433  loss  -0.34029  grad l2 norm  0.005681709\n",
      "iter  1434  loss  -0.34031224  grad l2 norm  0.005480755\n",
      "iter  1435  loss  -0.34033456  grad l2 norm  0.0053493883\n",
      "iter  1436  loss  -0.34035617  grad l2 norm  0.0053032255\n",
      "iter  1437  loss  -0.34037775  grad l2 norm  0.005284588\n",
      "iter  1438  loss  -0.34039918  grad l2 norm  0.005270277\n",
      "iter  1439  loss  -0.34042063  grad l2 norm  0.0052708047\n",
      "iter  1440  loss  -0.34044185  grad l2 norm  0.005279204\n",
      "iter  1441  loss  -0.34046307  grad l2 norm  0.0053053796\n",
      "iter  1442  loss  -0.34048402  grad l2 norm  0.0053445436\n",
      "iter  1443  loss  -0.34050494  grad l2 norm  0.0053964662\n",
      "iter  1444  loss  -0.34052578  grad l2 norm  0.0054309666\n",
      "iter  1445  loss  -0.34054688  grad l2 norm  0.005423755\n",
      "iter  1446  loss  -0.34056792  grad l2 norm  0.0053849635\n",
      "iter  1447  loss  -0.3405891  grad l2 norm  0.005343884\n",
      "iter  1448  loss  -0.34061003  grad l2 norm  0.0053153555\n",
      "iter  1449  loss  -0.34063104  grad l2 norm  0.0052964715\n",
      "iter  1450  loss  -0.34065184  grad l2 norm  0.0052924957\n",
      "iter  1451  loss  -0.3406726  grad l2 norm  0.0052946275\n",
      "iter  1452  loss  -0.34069318  grad l2 norm  0.005311699\n",
      "iter  1453  loss  -0.3407138  grad l2 norm  0.0053276983\n",
      "iter  1454  loss  -0.34073418  grad l2 norm  0.0053509898\n",
      "iter  1455  loss  -0.34075484  grad l2 norm  0.0053571826\n",
      "iter  1456  loss  -0.34077525  grad l2 norm  0.0053603533\n",
      "iter  1457  loss  -0.34079576  grad l2 norm  0.005342081\n",
      "iter  1458  loss  -0.3408161  grad l2 norm  0.0053313803\n",
      "iter  1459  loss  -0.34083664  grad l2 norm  0.005309987\n",
      "iter  1460  loss  -0.34085697  grad l2 norm  0.0053071277\n",
      "iter  1461  loss  -0.34087732  grad l2 norm  0.005297326\n",
      "iter  1462  loss  -0.34089735  grad l2 norm  0.005308804\n",
      "iter  1463  loss  -0.3409177  grad l2 norm  0.005309481\n",
      "iter  1464  loss  -0.3409376  grad l2 norm  0.0053279703\n",
      "iter  1465  loss  -0.34095782  grad l2 norm  0.0053271423\n",
      "iter  1466  loss  -0.34097767  grad l2 norm  0.005338833\n",
      "iter  1467  loss  -0.34099776  grad l2 norm  0.005325292\n",
      "iter  1468  loss  -0.3410176  grad l2 norm  0.0053249714\n",
      "iter  1469  loss  -0.3410377  grad l2 norm  0.0053017642\n",
      "iter  1470  loss  -0.34105745  grad l2 norm  0.0053001596\n",
      "iter  1471  loss  -0.3410774  grad l2 norm  0.0052806227\n",
      "iter  1472  loss  -0.3410971  grad l2 norm  0.005288609\n",
      "iter  1473  loss  -0.3411169  grad l2 norm  0.0052784393\n",
      "iter  1474  loss  -0.3411365  grad l2 norm  0.0052961633\n",
      "iter  1475  loss  -0.3411561  grad l2 norm  0.0052902023\n",
      "iter  1476  loss  -0.34117544  grad l2 norm  0.005309625\n",
      "iter  1477  loss  -0.3411951  grad l2 norm  0.0052992045\n",
      "iter  1478  loss  -0.34121448  grad l2 norm  0.005313105\n",
      "iter  1479  loss  -0.3412339  grad l2 norm  0.005294575\n",
      "iter  1480  loss  -0.34125322  grad l2 norm  0.0053037484\n",
      "iter  1481  loss  -0.34127265  grad l2 norm  0.0052816947\n",
      "iter  1482  loss  -0.34129193  grad l2 norm  0.0052928575\n",
      "iter  1483  loss  -0.34131128  grad l2 norm  0.005274083\n",
      "iter  1484  loss  -0.34133038  grad l2 norm  0.005291261\n",
      "iter  1485  loss  -0.34134963  grad l2 norm  0.005276763\n",
      "iter  1486  loss  -0.34136865  grad l2 norm  0.005297513\n",
      "iter  1487  loss  -0.34138775  grad l2 norm  0.005282169\n",
      "iter  1488  loss  -0.34140667  grad l2 norm  0.0053006727\n",
      "iter  1489  loss  -0.34142575  grad l2 norm  0.0052796714\n",
      "iter  1490  loss  -0.34144455  grad l2 norm  0.0052927113\n",
      "iter  1491  loss  -0.34146354  grad l2 norm  0.005265603\n",
      "iter  1492  loss  -0.34148234  grad l2 norm  0.0052765603\n",
      "iter  1493  loss  -0.34150133  grad l2 norm  0.005250302\n",
      "iter  1494  loss  -0.34151995  grad l2 norm  0.0052672047\n",
      "iter  1495  loss  -0.34153876  grad l2 norm  0.005246852\n",
      "iter  1496  loss  -0.34155726  grad l2 norm  0.005270621\n",
      "iter  1497  loss  -0.34157598  grad l2 norm  0.0052532875\n",
      "iter  1498  loss  -0.34159437  grad l2 norm  0.0052791005\n",
      "iter  1499  loss  -0.34161294  grad l2 norm  0.005259057\n",
      "iter  1500  loss  -0.34163132  grad l2 norm  0.0052821334\n",
      "iter  1501  loss  -0.34164983  grad l2 norm  0.0052566007\n",
      "iter  1502  loss  -0.34166822  grad l2 norm  0.005276235\n",
      "iter  1503  loss  -0.34168658  grad l2 norm  0.0052469946\n",
      "iter  1504  loss  -0.34170488  grad l2 norm  0.0052669863\n",
      "iter  1505  loss  -0.34172323  grad l2 norm  0.0052393894\n",
      "iter  1506  loss  -0.34174138  grad l2 norm  0.005263963\n",
      "iter  1507  loss  -0.34175956  grad l2 norm  0.005239905\n",
      "iter  1508  loss  -0.34177765  grad l2 norm  0.0052682306\n",
      "iter  1509  loss  -0.34179568  grad l2 norm  0.0052447985\n",
      "iter  1510  loss  -0.34181362  grad l2 norm  0.0052723396\n",
      "iter  1511  loss  -0.34183168  grad l2 norm  0.0052449494\n",
      "iter  1512  loss  -0.34184948  grad l2 norm  0.005268361\n",
      "iter  1513  loss  -0.34186757  grad l2 norm  0.00523468\n",
      "iter  1514  loss  -0.34188533  grad l2 norm  0.0052545047\n",
      "iter  1515  loss  -0.34190327  grad l2 norm  0.0052214535\n",
      "iter  1516  loss  -0.341921  grad l2 norm  0.0052464823\n",
      "iter  1517  loss  -0.34193882  grad l2 norm  0.005215662\n",
      "iter  1518  loss  -0.34195644  grad l2 norm  0.005245754\n",
      "iter  1519  loss  -0.3419741  grad l2 norm  0.0052190633\n",
      "iter  1520  loss  -0.34199157  grad l2 norm  0.0052517825\n",
      "iter  1521  loss  -0.3420091  grad l2 norm  0.0052224062\n",
      "iter  1522  loss  -0.3420265  grad l2 norm  0.0052531525\n",
      "iter  1523  loss  -0.3420441  grad l2 norm  0.0052208933\n",
      "iter  1524  loss  -0.34206137  grad l2 norm  0.005249914\n",
      "iter  1525  loss  -0.34207883  grad l2 norm  0.005214286\n",
      "iter  1526  loss  -0.34209606  grad l2 norm  0.0052429917\n",
      "iter  1527  loss  -0.34211347  grad l2 norm  0.0052083293\n",
      "iter  1528  loss  -0.34213063  grad l2 norm  0.0052404054\n",
      "iter  1529  loss  -0.34214792  grad l2 norm  0.0052077607\n",
      "iter  1530  loss  -0.342165  grad l2 norm  0.005242863\n",
      "iter  1531  loss  -0.34218213  grad l2 norm  0.0052114106\n",
      "iter  1532  loss  -0.34219906  grad l2 norm  0.005246945\n",
      "iter  1533  loss  -0.34221607  grad l2 norm  0.0052128015\n",
      "iter  1534  loss  -0.34223297  grad l2 norm  0.005245358\n",
      "iter  1535  loss  -0.34225002  grad l2 norm  0.005206159\n",
      "iter  1536  loss  -0.34226686  grad l2 norm  0.005234487\n",
      "iter  1537  loss  -0.34228376  grad l2 norm  0.0051913867\n",
      "iter  1538  loss  -0.34230053  grad l2 norm  0.0052204523\n",
      "iter  1539  loss  -0.3423175  grad l2 norm  0.0051807375\n",
      "iter  1540  loss  -0.3423341  grad l2 norm  0.0052165645\n",
      "iter  1541  loss  -0.34235078  grad l2 norm  0.0051818183\n",
      "iter  1542  loss  -0.3423673  grad l2 norm  0.0052225864\n",
      "iter  1543  loss  -0.3423839  grad l2 norm  0.005188856\n",
      "iter  1544  loss  -0.3424004  grad l2 norm  0.005229339\n",
      "iter  1545  loss  -0.34241685  grad l2 norm  0.0051916777\n",
      "iter  1546  loss  -0.34243324  grad l2 norm  0.0052290135\n",
      "iter  1547  loss  -0.34244972  grad l2 norm  0.005186827\n",
      "iter  1548  loss  -0.342466  grad l2 norm  0.005222221\n",
      "iter  1549  loss  -0.3424824  grad l2 norm  0.0051783924\n",
      "iter  1550  loss  -0.34249863  grad l2 norm  0.005215593\n",
      "iter  1551  loss  -0.342515  grad l2 norm  0.0051743225\n",
      "iter  1552  loss  -0.3425311  grad l2 norm  0.005215431\n",
      "iter  1553  loss  -0.34254736  grad l2 norm  0.0051769083\n",
      "iter  1554  loss  -0.3425634  grad l2 norm  0.0052203196\n",
      "iter  1555  loss  -0.3425794  grad l2 norm  0.00518127\n",
      "iter  1556  loss  -0.34259534  grad l2 norm  0.005223338\n",
      "iter  1557  loss  -0.3426114  grad l2 norm  0.0051800692\n",
      "iter  1558  loss  -0.34262723  grad l2 norm  0.0052172868\n",
      "iter  1559  loss  -0.34264323  grad l2 norm  0.005167778\n",
      "iter  1560  loss  -0.34265903  grad l2 norm  0.005201962\n",
      "iter  1561  loss  -0.342675  grad l2 norm  0.00515249\n",
      "iter  1562  loss  -0.34269077  grad l2 norm  0.0051920153\n",
      "iter  1563  loss  -0.3427066  grad l2 norm  0.0051487563\n",
      "iter  1564  loss  -0.34272212  grad l2 norm  0.0051953704\n",
      "iter  1565  loss  -0.34273782  grad l2 norm  0.005155313\n",
      "iter  1566  loss  -0.34275332  grad l2 norm  0.0052036936\n",
      "iter  1567  loss  -0.3427689  grad l2 norm  0.0051613604\n",
      "iter  1568  loss  -0.34278435  grad l2 norm  0.0052070455\n",
      "iter  1569  loss  -0.3427999  grad l2 norm  0.005159755\n",
      "iter  1570  loss  -0.34281525  grad l2 norm  0.0052022845\n",
      "iter  1571  loss  -0.34283075  grad l2 norm  0.005151915\n",
      "iter  1572  loss  -0.34284598  grad l2 norm  0.0051945266\n",
      "iter  1573  loss  -0.34286144  grad l2 norm  0.00514522\n",
      "iter  1574  loss  -0.34287664  grad l2 norm  0.0051912596\n",
      "iter  1575  loss  -0.3428919  grad l2 norm  0.0051453197\n",
      "iter  1576  loss  -0.342907  grad l2 norm  0.0051951557\n",
      "iter  1577  loss  -0.34292224  grad l2 norm  0.0051508485\n",
      "iter  1578  loss  -0.3429372  grad l2 norm  0.005201079\n",
      "iter  1579  loss  -0.3429523  grad l2 norm  0.005154329\n",
      "iter  1580  loss  -0.3429672  grad l2 norm  0.0052006375\n",
      "iter  1581  loss  -0.3429823  grad l2 norm  0.005147315\n",
      "iter  1582  loss  -0.34299713  grad l2 norm  0.0051876944\n",
      "iter  1583  loss  -0.34301218  grad l2 norm  0.0051301783\n",
      "iter  1584  loss  -0.34302706  grad l2 norm  0.0051722033\n",
      "iter  1585  loss  -0.34304196  grad l2 norm  0.0051195025\n",
      "iter  1586  loss  -0.34305662  grad l2 norm  0.0051693395\n",
      "iter  1587  loss  -0.3430715  grad l2 norm  0.0051225107\n",
      "iter  1588  loss  -0.34308603  grad l2 norm  0.005176814\n",
      "iter  1589  loss  -0.34310073  grad l2 norm  0.0051302286\n",
      "iter  1590  loss  -0.3431152  grad l2 norm  0.0051835277\n",
      "iter  1591  loss  -0.34312984  grad l2 norm  0.0051328223\n",
      "iter  1592  loss  -0.34314424  grad l2 norm  0.005182229\n",
      "iter  1593  loss  -0.34315893  grad l2 norm  0.0051270253\n",
      "iter  1594  loss  -0.34317333  grad l2 norm  0.0051748557\n",
      "iter  1595  loss  -0.3431878  grad l2 norm  0.00511903\n",
      "iter  1596  loss  -0.34320205  grad l2 norm  0.005168902\n",
      "iter  1597  loss  -0.3432165  grad l2 norm  0.005116055\n",
      "iter  1598  loss  -0.34323075  grad l2 norm  0.005169957\n",
      "iter  1599  loss  -0.343245  grad l2 norm  0.0051199435\n",
      "iter  1600  loss  -0.3432592  grad l2 norm  0.0051757945\n",
      "iter  1601  loss  -0.34327337  grad l2 norm  0.0051255072\n",
      "iter  1602  loss  -0.34328738  grad l2 norm  0.0051794723\n",
      "iter  1603  loss  -0.34330145  grad l2 norm  0.005124261\n",
      "iter  1604  loss  -0.34331548  grad l2 norm  0.0051721632\n",
      "iter  1605  loss  -0.34332964  grad l2 norm  0.005109916\n",
      "iter  1606  loss  -0.34334362  grad l2 norm  0.005154604\n",
      "iter  1607  loss  -0.34335765  grad l2 norm  0.0050937864\n",
      "iter  1608  loss  -0.3433715  grad l2 norm  0.005145505\n",
      "iter  1609  loss  -0.34338543  grad l2 norm  0.005092178\n",
      "iter  1610  loss  -0.3433991  grad l2 norm  0.005151293\n",
      "iter  1611  loss  -0.343413  grad l2 norm  0.0051011182\n",
      "iter  1612  loss  -0.34342656  grad l2 norm  0.005160815\n",
      "iter  1613  loss  -0.34344032  grad l2 norm  0.0051074787\n",
      "iter  1614  loss  -0.34345385  grad l2 norm  0.0051634135\n",
      "iter  1615  loss  -0.34346756  grad l2 norm  0.0051050354\n",
      "iter  1616  loss  -0.34348112  grad l2 norm  0.005157585\n",
      "iter  1617  loss  -0.34349462  grad l2 norm  0.0050963266\n",
      "iter  1618  loss  -0.34350818  grad l2 norm  0.0051487945\n",
      "iter  1619  loss  -0.3435217  grad l2 norm  0.0050894907\n",
      "iter  1620  loss  -0.34353513  grad l2 norm  0.005146062\n",
      "iter  1621  loss  -0.3435486  grad l2 norm  0.005090655\n",
      "iter  1622  loss  -0.34356183  grad l2 norm  0.0051511163\n",
      "iter  1623  loss  -0.3435751  grad l2 norm  0.0050979033\n",
      "iter  1624  loss  -0.34358826  grad l2 norm  0.0051588835\n",
      "iter  1625  loss  -0.34360152  grad l2 norm  0.0051033553\n",
      "iter  1626  loss  -0.34361467  grad l2 norm  0.0051596854\n",
      "iter  1627  loss  -0.34362787  grad l2 norm  0.00509623\n",
      "iter  1628  loss  -0.34364095  grad l2 norm  0.0051450045\n",
      "iter  1629  loss  -0.34365425  grad l2 norm  0.005076692\n",
      "iter  1630  loss  -0.34366724  grad l2 norm  0.0051276633\n",
      "iter  1631  loss  -0.34368032  grad l2 norm  0.00506597\n",
      "iter  1632  loss  -0.34369323  grad l2 norm  0.0051263697\n",
      "iter  1633  loss  -0.34370613  grad l2 norm  0.005071611\n",
      "iter  1634  loss  -0.34371895  grad l2 norm  0.005136521\n",
      "iter  1635  loss  -0.3437318  grad l2 norm  0.0050812983\n",
      "iter  1636  loss  -0.34374443  grad l2 norm  0.0051438254\n",
      "iter  1637  loss  -0.3437573  grad l2 norm  0.005083511\n",
      "iter  1638  loss  -0.34376997  grad l2 norm  0.0051418357\n",
      "iter  1639  loss  -0.34378272  grad l2 norm  0.005076914\n",
      "iter  1640  loss  -0.34379542  grad l2 norm  0.005133726\n",
      "iter  1641  loss  -0.34380805  grad l2 norm  0.005069218\n",
      "iter  1642  loss  -0.34382066  grad l2 norm  0.005128942\n",
      "iter  1643  loss  -0.3438332  grad l2 norm  0.005068058\n",
      "iter  1644  loss  -0.34384564  grad l2 norm  0.0051320493\n",
      "iter  1645  loss  -0.34385815  grad l2 norm  0.005073975\n",
      "iter  1646  loss  -0.34387046  grad l2 norm  0.0051398664\n",
      "iter  1647  loss  -0.34388283  grad l2 norm  0.005081084\n",
      "iter  1648  loss  -0.3438951  grad l2 norm  0.00514435\n",
      "iter  1649  loss  -0.34390748  grad l2 norm  0.005079675\n",
      "iter  1650  loss  -0.34391972  grad l2 norm  0.005135465\n",
      "iter  1651  loss  -0.34393203  grad l2 norm  0.005062244\n",
      "iter  1652  loss  -0.34394422  grad l2 norm  0.00511396\n",
      "iter  1653  loss  -0.3439565  grad l2 norm  0.0050439327\n",
      "iter  1654  loss  -0.34396863  grad l2 norm  0.0051057395\n",
      "iter  1655  loss  -0.34398082  grad l2 norm  0.005045592\n",
      "iter  1656  loss  -0.34399274  grad l2 norm  0.005115626\n",
      "iter  1657  loss  -0.34400472  grad l2 norm  0.0050581014\n",
      "iter  1658  loss  -0.34401658  grad l2 norm  0.005127986\n",
      "iter  1659  loss  -0.34402853  grad l2 norm  0.005065633\n",
      "iter  1660  loss  -0.34404048  grad l2 norm  0.005129876\n",
      "iter  1661  loss  -0.34405228  grad l2 norm  0.0050615342\n",
      "iter  1662  loss  -0.3440641  grad l2 norm  0.005122015\n",
      "iter  1663  loss  -0.34407595  grad l2 norm  0.005051947\n",
      "iter  1664  loss  -0.34408775  grad l2 norm  0.0051137866\n",
      "iter  1665  loss  -0.34409955  grad l2 norm  0.0050467784\n",
      "iter  1666  loss  -0.34411114  grad l2 norm  0.0051135486\n",
      "iter  1667  loss  -0.3441228  grad l2 norm  0.005050915\n",
      "iter  1668  loss  -0.3441344  grad l2 norm  0.0051220222\n",
      "iter  1669  loss  -0.34414592  grad l2 norm  0.0050608856\n",
      "iter  1670  loss  -0.3441574  grad l2 norm  0.0051305555\n",
      "iter  1671  loss  -0.34416896  grad l2 norm  0.005064707\n",
      "iter  1672  loss  -0.34418035  grad l2 norm  0.005125854\n",
      "iter  1673  loss  -0.34419185  grad l2 norm  0.0050496305\n",
      "iter  1674  loss  -0.34420335  grad l2 norm  0.005104101\n",
      "iter  1675  loss  -0.34421483  grad l2 norm  0.00502768\n",
      "iter  1676  loss  -0.3442261  grad l2 norm  0.0050903056\n",
      "iter  1677  loss  -0.34423748  grad l2 norm  0.0050247996\n",
      "iter  1678  loss  -0.34424856  grad l2 norm  0.0050981436\n",
      "iter  1679  loss  -0.34425977  grad l2 norm  0.0050379685\n",
      "iter  1680  loss  -0.34427088  grad l2 norm  0.0051127784\n",
      "iter  1681  loss  -0.34428212  grad l2 norm  0.005048768\n",
      "iter  1682  loss  -0.34429315  grad l2 norm  0.005117966\n",
      "iter  1683  loss  -0.3443042  grad l2 norm  0.0050470596\n",
      "iter  1684  loss  -0.34431517  grad l2 norm  0.005111158\n",
      "iter  1685  loss  -0.3443263  grad l2 norm  0.0050368444\n",
      "iter  1686  loss  -0.34433728  grad l2 norm  0.0051014433\n",
      "iter  1687  loss  -0.34434828  grad l2 norm  0.005029548\n",
      "iter  1688  loss  -0.3443592  grad l2 norm  0.005098414\n",
      "iter  1689  loss  -0.34437007  grad l2 norm  0.0050311177\n",
      "iter  1690  loss  -0.34438086  grad l2 norm  0.0051041893\n",
      "iter  1691  loss  -0.34439155  grad l2 norm  0.005039767\n",
      "iter  1692  loss  -0.3444024  grad l2 norm  0.005113726\n",
      "iter  1693  loss  -0.34441304  grad l2 norm  0.0050473744\n",
      "iter  1694  loss  -0.34442368  grad l2 norm  0.0051160315\n",
      "iter  1695  loss  -0.34443438  grad l2 norm  0.005040336\n",
      "iter  1696  loss  -0.34444502  grad l2 norm  0.0050992263\n",
      "iter  1697  loss  -0.34445575  grad l2 norm  0.0050179404\n",
      "iter  1698  loss  -0.3444663  grad l2 norm  0.0050801737\n",
      "iter  1699  loss  -0.34447697  grad l2 norm  0.005007727\n",
      "iter  1700  loss  -0.34448746  grad l2 norm  0.0050817397\n",
      "iter  1701  loss  -0.3444978  grad l2 norm  0.0050175874\n",
      "iter  1702  loss  -0.3445082  grad l2 norm  0.0050961706\n",
      "iter  1703  loss  -0.34451854  grad l2 norm  0.005030657\n",
      "iter  1704  loss  -0.3445288  grad l2 norm  0.0051049488\n",
      "iter  1705  loss  -0.3445392  grad l2 norm  0.0050322316\n",
      "iter  1706  loss  -0.3445494  grad l2 norm  0.005100991\n",
      "iter  1707  loss  -0.34455976  grad l2 norm  0.005023924\n",
      "iter  1708  loss  -0.34457004  grad l2 norm  0.005092079\n",
      "iter  1709  loss  -0.3445802  grad l2 norm  0.005016983\n",
      "iter  1710  loss  -0.34459037  grad l2 norm  0.005089379\n",
      "iter  1711  loss  -0.3446005  grad l2 norm  0.0050192527\n",
      "iter  1712  loss  -0.34461057  grad l2 norm  0.0050952416\n",
      "iter  1713  loss  -0.34462065  grad l2 norm  0.0050268825\n",
      "iter  1714  loss  -0.34463057  grad l2 norm  0.0051022926\n",
      "iter  1715  loss  -0.34464058  grad l2 norm  0.0050313435\n",
      "iter  1716  loss  -0.34465048  grad l2 norm  0.005102329\n",
      "iter  1717  loss  -0.3446604  grad l2 norm  0.0050242324\n",
      "iter  1718  loss  -0.34467041  grad l2 norm  0.005086614\n",
      "iter  1719  loss  -0.34468037  grad l2 norm  0.0050013973\n",
      "iter  1720  loss  -0.34469026  grad l2 norm  0.005065528\n",
      "iter  1721  loss  -0.3447001  grad l2 norm  0.004989069\n",
      "iter  1722  loss  -0.34470987  grad l2 norm  0.0050664935\n",
      "iter  1723  loss  -0.3447195  grad l2 norm  0.0050011487\n",
      "iter  1724  loss  -0.34472921  grad l2 norm  0.005085731\n",
      "iter  1725  loss  -0.34473875  grad l2 norm  0.0050198706\n",
      "iter  1726  loss  -0.34474835  grad l2 norm  0.005099629\n",
      "iter  1727  loss  -0.34475797  grad l2 norm  0.0050244955\n",
      "iter  1728  loss  -0.34476754  grad l2 norm  0.0050955093\n",
      "iter  1729  loss  -0.34477717  grad l2 norm  0.0050133797\n",
      "iter  1730  loss  -0.34478664  grad l2 norm  0.005082387\n",
      "iter  1731  loss  -0.34479615  grad l2 norm  0.0050025526\n",
      "iter  1732  loss  -0.34480566  grad l2 norm  0.0050775358\n",
      "iter  1733  loss  -0.3448151  grad l2 norm  0.0050049294\n",
      "iter  1734  loss  -0.34482443  grad l2 norm  0.0050852266\n",
      "iter  1735  loss  -0.34483382  grad l2 norm  0.005015036\n",
      "iter  1736  loss  -0.3448431  grad l2 norm  0.0050937817\n",
      "iter  1737  loss  -0.3448523  grad l2 norm  0.005019923\n",
      "iter  1738  loss  -0.3448615  grad l2 norm  0.0050947485\n",
      "iter  1739  loss  -0.34487078  grad l2 norm  0.005015941\n",
      "iter  1740  loss  -0.34487993  grad l2 norm  0.0050840443\n",
      "iter  1741  loss  -0.3448892  grad l2 norm  0.004997211\n",
      "iter  1742  loss  -0.3448985  grad l2 norm  0.0050628507\n",
      "iter  1743  loss  -0.34490767  grad l2 norm  0.0049806586\n",
      "iter  1744  loss  -0.34491673  grad l2 norm  0.0050581917\n",
      "iter  1745  loss  -0.3449257  grad l2 norm  0.004987875\n",
      "iter  1746  loss  -0.3449346  grad l2 norm  0.0050744344\n",
      "iter  1747  loss  -0.34494358  grad l2 norm  0.00500603\n",
      "iter  1748  loss  -0.34495246  grad l2 norm  0.0050897934\n",
      "iter  1749  loss  -0.34496137  grad l2 norm  0.0050138\n",
      "iter  1750  loss  -0.34497026  grad l2 norm  0.005089583\n",
      "iter  1751  loss  -0.34497914  grad l2 norm  0.0050061895\n",
      "iter  1752  loss  -0.34498808  grad l2 norm  0.005078541\n",
      "iter  1753  loss  -0.3449969  grad l2 norm  0.004995262\n",
      "iter  1754  loss  -0.34500566  grad l2 norm  0.0050714845\n",
      "iter  1755  loss  -0.34501448  grad l2 norm  0.0049942234\n",
      "iter  1756  loss  -0.3450231  grad l2 norm  0.0050758864\n",
      "iter  1757  loss  -0.34503186  grad l2 norm  0.0050023915\n",
      "iter  1758  loss  -0.34504044  grad l2 norm  0.0050845956\n",
      "iter  1759  loss  -0.345049  grad l2 norm  0.005009665\n",
      "iter  1760  loss  -0.34505758  grad l2 norm  0.005089472\n",
      "iter  1761  loss  -0.34506604  grad l2 norm  0.005011264\n",
      "iter  1762  loss  -0.3450745  grad l2 norm  0.0050855223\n",
      "iter  1763  loss  -0.34508312  grad l2 norm  0.0049982076\n",
      "iter  1764  loss  -0.3450917  grad l2 norm  0.0050649773\n",
      "iter  1765  loss  -0.34510028  grad l2 norm  0.0049765003\n",
      "iter  1766  loss  -0.34510872  grad l2 norm  0.0050518336\n",
      "iter  1767  loss  -0.34511706  grad l2 norm  0.0049757473\n",
      "iter  1768  loss  -0.34512538  grad l2 norm  0.0050633657\n",
      "iter  1769  loss  -0.3451336  grad l2 norm  0.004993269\n",
      "iter  1770  loss  -0.34514186  grad l2 norm  0.005081468\n",
      "iter  1771  loss  -0.3451501  grad l2 norm  0.0050059743\n",
      "iter  1772  loss  -0.3451583  grad l2 norm  0.0050865617\n",
      "iter  1773  loss  -0.34516653  grad l2 norm  0.0050027072\n",
      "iter  1774  loss  -0.34517473  grad l2 norm  0.0050776373\n",
      "iter  1775  loss  -0.34518296  grad l2 norm  0.004991427\n",
      "iter  1776  loss  -0.34519115  grad l2 norm  0.0050683045\n",
      "iter  1777  loss  -0.34519923  grad l2 norm  0.0049871583\n",
      "iter  1778  loss  -0.34520733  grad l2 norm  0.0050700586\n",
      "iter  1779  loss  -0.34521532  grad l2 norm  0.004994611\n",
      "iter  1780  loss  -0.34522325  grad l2 norm  0.0050799763\n",
      "iter  1781  loss  -0.3452312  grad l2 norm  0.0050045066\n",
      "iter  1782  loss  -0.34523907  grad l2 norm  0.0050871284\n",
      "iter  1783  loss  -0.34524694  grad l2 norm  0.0050080554\n",
      "iter  1784  loss  -0.3452548  grad l2 norm  0.0050855605\n",
      "iter  1785  loss  -0.34526277  grad l2 norm  0.004998548\n",
      "iter  1786  loss  -0.3452707  grad l2 norm  0.0050666747\n",
      "iter  1787  loss  -0.34527862  grad l2 norm  0.0049741264\n",
      "iter  1788  loss  -0.34528643  grad l2 norm  0.005047411\n",
      "iter  1789  loss  -0.34529424  grad l2 norm  0.004966906\n",
      "iter  1790  loss  -0.3453019  grad l2 norm  0.005054378\n",
      "iter  1791  loss  -0.3453095  grad l2 norm  0.0049834037\n",
      "iter  1792  loss  -0.3453171  grad l2 norm  0.005074615\n",
      "iter  1793  loss  -0.34532467  grad l2 norm  0.005000552\n",
      "iter  1794  loss  -0.3453323  grad l2 norm  0.005084904\n",
      "iter  1795  loss  -0.34533983  grad l2 norm  0.00500165\n",
      "iter  1796  loss  -0.3453474  grad l2 norm  0.005078458\n",
      "iter  1797  loss  -0.34535494  grad l2 norm  0.0049905176\n",
      "iter  1798  loss  -0.3453626  grad l2 norm  0.0050667664\n",
      "iter  1799  loss  -0.34537002  grad l2 norm  0.0049822796\n",
      "iter  1800  loss  -0.34537753  grad l2 norm  0.0050640097\n",
      "iter  1801  loss  -0.34538496  grad l2 norm  0.0049864505\n",
      "iter  1802  loss  -0.34539235  grad l2 norm  0.005072886\n",
      "iter  1803  loss  -0.34539956  grad l2 norm  0.0049982443\n",
      "iter  1804  loss  -0.34540686  grad l2 norm  0.005083574\n",
      "iter  1805  loss  -0.34541407  grad l2 norm  0.005006305\n",
      "iter  1806  loss  -0.34542134  grad l2 norm  0.0050868155\n",
      "iter  1807  loss  -0.3454285  grad l2 norm  0.0050021666\n",
      "iter  1808  loss  -0.34543592  grad l2 norm  0.0050716856\n",
      "iter  1809  loss  -0.3454432  grad l2 norm  0.004977087\n",
      "iter  1810  loss  -0.3454505  grad l2 norm  0.005046587\n",
      "iter  1811  loss  -0.34545773  grad l2 norm  0.0049611926\n",
      "iter  1812  loss  -0.34546477  grad l2 norm  0.005045685\n",
      "iter  1813  loss  -0.3454718  grad l2 norm  0.0049734153\n",
      "iter  1814  loss  -0.34547883  grad l2 norm  0.0050658295\n",
      "iter  1815  loss  -0.3454858  grad l2 norm  0.0049940594\n",
      "iter  1816  loss  -0.34549275  grad l2 norm  0.0050812615\n",
      "iter  1817  loss  -0.3454997  grad l2 norm  0.00500081\n",
      "iter  1818  loss  -0.3455067  grad l2 norm  0.0050790734\n",
      "iter  1819  loss  -0.34551364  grad l2 norm  0.0049914466\n",
      "iter  1820  loss  -0.3455206  grad l2 norm  0.0050662057\n",
      "iter  1821  loss  -0.34552747  grad l2 norm  0.0049798638\n",
      "iter  1822  loss  -0.34553432  grad l2 norm  0.005059083\n",
      "iter  1823  loss  -0.34554112  grad l2 norm  0.0049803923\n",
      "iter  1824  loss  -0.3455479  grad l2 norm  0.0050662127\n",
      "iter  1825  loss  -0.34555465  grad l2 norm  0.0049930373\n",
      "iter  1826  loss  -0.34556136  grad l2 norm  0.0050795306\n",
      "iter  1827  loss  -0.3455679  grad l2 norm  0.0050050374\n",
      "iter  1828  loss  -0.34557465  grad l2 norm  0.0050870483\n",
      "iter  1829  loss  -0.34558123  grad l2 norm  0.005005703\n",
      "iter  1830  loss  -0.34558797  grad l2 norm  0.005076304\n",
      "iter  1831  loss  -0.34559464  grad l2 norm  0.0049815904\n",
      "iter  1832  loss  -0.3456014  grad l2 norm  0.0050465786\n",
      "iter  1833  loss  -0.34560803  grad l2 norm  0.0049577365\n",
      "iter  1834  loss  -0.34561464  grad l2 norm  0.005037839\n",
      "iter  1835  loss  -0.34562105  grad l2 norm  0.0049651815\n",
      "iter  1836  loss  -0.34562755  grad l2 norm  0.005056896\n",
      "iter  1837  loss  -0.3456338  grad l2 norm  0.004987867\n",
      "iter  1838  loss  -0.34564018  grad l2 norm  0.0050764102\n",
      "iter  1839  loss  -0.34564656  grad l2 norm  0.0049995445\n",
      "iter  1840  loss  -0.34565297  grad l2 norm  0.005078227\n",
      "iter  1841  loss  -0.34565935  grad l2 norm  0.0049925684\n",
      "iter  1842  loss  -0.34566572  grad l2 norm  0.00506581\n",
      "iter  1843  loss  -0.3456721  grad l2 norm  0.0049791443\n",
      "iter  1844  loss  -0.34567845  grad l2 norm  0.005055048\n",
      "iter  1845  loss  -0.34568468  grad l2 norm  0.004975463\n",
      "iter  1846  loss  -0.34569088  grad l2 norm  0.005058566\n",
      "iter  1847  loss  -0.3456971  grad l2 norm  0.004986824\n",
      "iter  1848  loss  -0.34570315  grad l2 norm  0.0050739213\n",
      "iter  1849  loss  -0.34570926  grad l2 norm  0.005003091\n",
      "iter  1850  loss  -0.3457153  grad l2 norm  0.0050854553\n",
      "iter  1851  loss  -0.34572136  grad l2 norm  0.0050087874\n",
      "iter  1852  loss  -0.3457275  grad l2 norm  0.0050809453\n",
      "iter  1853  loss  -0.3457336  grad l2 norm  0.0049891896\n",
      "iter  1854  loss  -0.34573984  grad l2 norm  0.0050500706\n",
      "iter  1855  loss  -0.34574595  grad l2 norm  0.004958595\n",
      "iter  1856  loss  -0.345752  grad l2 norm  0.0050319447\n",
      "iter  1857  loss  -0.34575793  grad l2 norm  0.004957644\n",
      "iter  1858  loss  -0.3457638  grad l2 norm  0.00504641\n",
      "iter  1859  loss  -0.34576958  grad l2 norm  0.0049804887\n",
      "iter  1860  loss  -0.34577546  grad l2 norm  0.0050695306\n",
      "iter  1861  loss  -0.34578124  grad l2 norm  0.004997609\n",
      "iter  1862  loss  -0.34578708  grad l2 norm  0.005077018\n",
      "iter  1863  loss  -0.34579298  grad l2 norm  0.004994981\n",
      "iter  1864  loss  -0.3457988  grad l2 norm  0.0050666085\n",
      "iter  1865  loss  -0.34580457  grad l2 norm  0.0049812873\n",
      "iter  1866  loss  -0.34581044  grad l2 norm  0.0050537214\n",
      "iter  1867  loss  -0.34581614  grad l2 norm  0.004973982\n",
      "iter  1868  loss  -0.34582183  grad l2 norm  0.0050545544\n",
      "iter  1869  loss  -0.34582737  grad l2 norm  0.0049852123\n",
      "iter  1870  loss  -0.34583294  grad l2 norm  0.005071617\n",
      "iter  1871  loss  -0.34583846  grad l2 norm  0.0050023473\n",
      "iter  1872  loss  -0.34584418  grad l2 norm  0.0050823786\n",
      "iter  1873  loss  -0.3458496  grad l2 norm  0.005009515\n",
      "iter  1874  loss  -0.34585515  grad l2 norm  0.005084159\n",
      "iter  1875  loss  -0.34586072  grad l2 norm  0.004998308\n",
      "iter  1876  loss  -0.34586635  grad l2 norm  0.005057064\n",
      "iter  1877  loss  -0.34587193  grad l2 norm  0.00496379\n",
      "iter  1878  loss  -0.3458776  grad l2 norm  0.0050295587\n",
      "iter  1879  loss  -0.34588307  grad l2 norm  0.00495261\n",
      "iter  1880  loss  -0.34588847  grad l2 norm  0.0050361725\n",
      "iter  1881  loss  -0.34589374  grad l2 norm  0.0049721915\n",
      "iter  1882  loss  -0.34589908  grad l2 norm  0.005060947\n",
      "iter  1883  loss  -0.3459043  grad l2 norm  0.004994921\n",
      "iter  1884  loss  -0.34590963  grad l2 norm  0.0050758272\n",
      "iter  1885  loss  -0.34591493  grad l2 norm  0.004999757\n",
      "iter  1886  loss  -0.3459202  grad l2 norm  0.005070842\n",
      "iter  1887  loss  -0.34592548  grad l2 norm  0.004988305\n",
      "iter  1888  loss  -0.34593076  grad l2 norm  0.005056298\n",
      "iter  1889  loss  -0.34593606  grad l2 norm  0.0049761967\n",
      "iter  1890  loss  -0.3459413  grad l2 norm  0.0050493004\n",
      "iter  1891  loss  -0.34594652  grad l2 norm  0.004976546\n",
      "iter  1892  loss  -0.34595168  grad l2 norm  0.0050566816\n",
      "iter  1893  loss  -0.3459567  grad l2 norm  0.004990929\n",
      "iter  1894  loss  -0.34596178  grad l2 norm  0.005074446\n",
      "iter  1895  loss  -0.34596673  grad l2 norm  0.005009705\n",
      "iter  1896  loss  -0.34597164  grad l2 norm  0.0050876443\n",
      "iter  1897  loss  -0.34597665  grad l2 norm  0.005009524\n",
      "iter  1898  loss  -0.3459818  grad l2 norm  0.0050671473\n",
      "iter  1899  loss  -0.34598696  grad l2 norm  0.004974194\n",
      "iter  1900  loss  -0.34599212  grad l2 norm  0.0050315987\n",
      "iter  1901  loss  -0.34599707  grad l2 norm  0.004951596\n",
      "iter  1902  loss  -0.34600204  grad l2 norm  0.005027423\n",
      "iter  1903  loss  -0.34600687  grad l2 norm  0.004963627\n",
      "iter  1904  loss  -0.34601167  grad l2 norm  0.005049196\n",
      "iter  1905  loss  -0.3460165  grad l2 norm  0.00498787\n",
      "iter  1906  loss  -0.34602135  grad l2 norm  0.0050688786\n",
      "iter  1907  loss  -0.3460261  grad l2 norm  0.0049989372\n",
      "iter  1908  loss  -0.34603098  grad l2 norm  0.0050699874\n",
      "iter  1909  loss  -0.3460358  grad l2 norm  0.0049924166\n",
      "iter  1910  loss  -0.34604064  grad l2 norm  0.005058747\n",
      "iter  1911  loss  -0.34604537  grad l2 norm  0.0049808146\n",
      "iter  1912  loss  -0.3460502  grad l2 norm  0.0050494885\n",
      "iter  1913  loss  -0.3460549  grad l2 norm  0.0049771094\n",
      "iter  1914  loss  -0.34605965  grad l2 norm  0.005051775\n",
      "iter  1915  loss  -0.3460642  grad l2 norm  0.004986807\n",
      "iter  1916  loss  -0.34606886  grad l2 norm  0.005066489\n",
      "iter  1917  loss  -0.34607327  grad l2 norm  0.0050054397\n",
      "iter  1918  loss  -0.34607786  grad l2 norm  0.005083609\n",
      "iter  1919  loss  -0.3460823  grad l2 norm  0.0050136074\n",
      "iter  1920  loss  -0.34608692  grad l2 norm  0.0050728656\n",
      "iter  1921  loss  -0.3460915  grad l2 norm  0.004984617\n",
      "iter  1922  loss  -0.34609625  grad l2 norm  0.005037136\n",
      "iter  1923  loss  -0.34610078  grad l2 norm  0.00495599\n",
      "iter  1924  loss  -0.34610546  grad l2 norm  0.005024214\n",
      "iter  1925  loss  -0.34610975  grad l2 norm  0.004959785\n",
      "iter  1926  loss  -0.34611428  grad l2 norm  0.005040556\n",
      "iter  1927  loss  -0.3461185  grad l2 norm  0.004981943\n",
      "iter  1928  loss  -0.34612295  grad l2 norm  0.005061395\n",
      "iter  1929  loss  -0.34612724  grad l2 norm  0.004997032\n",
      "iter  1930  loss  -0.34613168  grad l2 norm  0.005067669\n",
      "iter  1931  loss  -0.34613594  grad l2 norm  0.004995166\n",
      "iter  1932  loss  -0.34614035  grad l2 norm  0.00505965\n",
      "iter  1933  loss  -0.34614465  grad l2 norm  0.0049850317\n",
      "iter  1934  loss  -0.346149  grad l2 norm  0.005049875\n",
      "iter  1935  loss  -0.34615332  grad l2 norm  0.0049793976\n",
      "iter  1936  loss  -0.34615755  grad l2 norm  0.0050492138\n",
      "iter  1937  loss  -0.3461618  grad l2 norm  0.0049851933\n",
      "iter  1938  loss  -0.346166  grad l2 norm  0.005060117\n",
      "iter  1939  loss  -0.34616998  grad l2 norm  0.0050014704\n",
      "iter  1940  loss  -0.34617412  grad l2 norm  0.005077832\n",
      "iter  1941  loss  -0.34617814  grad l2 norm  0.0050144913\n",
      "iter  1942  loss  -0.34618238  grad l2 norm  0.005074915\n",
      "iter  1943  loss  -0.34618655  grad l2 norm  0.0049929908\n",
      "iter  1944  loss  -0.34619087  grad l2 norm  0.0050433576\n",
      "iter  1945  loss  -0.34619492  grad l2 norm  0.004964243\n",
      "iter  1946  loss  -0.3461991  grad l2 norm  0.0050260215\n",
      "iter  1947  loss  -0.3462031  grad l2 norm  0.004960966\n",
      "iter  1948  loss  -0.34620717  grad l2 norm  0.0050352546\n",
      "iter  1949  loss  -0.34621096  grad l2 norm  0.0049780775\n",
      "iter  1950  loss  -0.34621507  grad l2 norm  0.0050538154\n",
      "iter  1951  loss  -0.34621888  grad l2 norm  0.0049939337\n",
      "iter  1952  loss  -0.3462229  grad l2 norm  0.0050631305\n",
      "iter  1953  loss  -0.3462268  grad l2 norm  0.004996295\n",
      "iter  1954  loss  -0.3462308  grad l2 norm  0.0050591654\n",
      "iter  1955  loss  -0.34623477  grad l2 norm  0.004989003\n",
      "iter  1956  loss  -0.3462386  grad l2 norm  0.00505119\n",
      "iter  1957  loss  -0.3462425  grad l2 norm  0.00498365\n",
      "iter  1958  loss  -0.34624645  grad l2 norm  0.0050490764\n",
      "iter  1959  loss  -0.34625018  grad l2 norm  0.0049867253\n",
      "iter  1960  loss  -0.34625405  grad l2 norm  0.005056948\n",
      "iter  1961  loss  -0.34625766  grad l2 norm  0.0050001456\n",
      "iter  1962  loss  -0.3462614  grad l2 norm  0.0050730933\n",
      "iter  1963  loss  -0.34626505  grad l2 norm  0.005014765\n",
      "iter  1964  loss  -0.3462688  grad l2 norm  0.0050751227\n",
      "iter  1965  loss  -0.3462726  grad l2 norm  0.0049989037\n",
      "iter  1966  loss  -0.34627646  grad l2 norm  0.0050478396\n",
      "iter  1967  loss  -0.34628013  grad l2 norm  0.0049726223\n",
      "iter  1968  loss  -0.34628382  grad l2 norm  0.0050305286\n",
      "iter  1969  loss  -0.3462875  grad l2 norm  0.004966678\n",
      "iter  1970  loss  -0.34629112  grad l2 norm  0.005034413\n",
      "iter  1971  loss  -0.34629473  grad l2 norm  0.0049775867\n",
      "iter  1972  loss  -0.34629837  grad l2 norm  0.005048068\n",
      "iter  1973  loss  -0.34630176  grad l2 norm  0.004990965\n",
      "iter  1974  loss  -0.34630552  grad l2 norm  0.005057784\n",
      "iter  1975  loss  -0.34630898  grad l2 norm  0.0049959165\n",
      "iter  1976  loss  -0.34631264  grad l2 norm  0.005057796\n",
      "iter  1977  loss  -0.34631607  grad l2 norm  0.004993055\n",
      "iter  1978  loss  -0.34631968  grad l2 norm  0.005053316\n",
      "iter  1979  loss  -0.34632313  grad l2 norm  0.004989276\n",
      "iter  1980  loss  -0.34632668  grad l2 norm  0.0050511146\n",
      "iter  1981  loss  -0.34633008  grad l2 norm  0.0049909805\n",
      "iter  1982  loss  -0.3463336  grad l2 norm  0.0050565633\n",
      "iter  1983  loss  -0.34633684  grad l2 norm  0.005001378\n",
      "iter  1984  loss  -0.34634027  grad l2 norm  0.00507016\n",
      "iter  1985  loss  -0.3463435  grad l2 norm  0.005015334\n",
      "iter  1986  loss  -0.34634694  grad l2 norm  0.0050736894\n",
      "iter  1987  loss  -0.34635037  grad l2 norm  0.005001952\n",
      "iter  1988  loss  -0.34635377  grad l2 norm  0.005048621\n",
      "iter  1989  loss  -0.34635717  grad l2 norm  0.004977893\n",
      "iter  1990  loss  -0.3463605  grad l2 norm  0.0050336635\n",
      "iter  1991  loss  -0.34636375  grad l2 norm  0.0049738484\n",
      "iter  1992  loss  -0.34636715  grad l2 norm  0.005037454\n",
      "iter  1993  loss  -0.34637025  grad l2 norm  0.0049822433\n",
      "iter  1994  loss  -0.34637353  grad l2 norm  0.0050473404\n",
      "iter  1995  loss  -0.34637675  grad l2 norm  0.004991619\n",
      "iter  1996  loss  -0.34638003  grad l2 norm  0.0050543016\n",
      "iter  1997  loss  -0.34638312  grad l2 norm  0.0049954634\n",
      "iter  1998  loss  -0.34638637  grad l2 norm  0.005054517\n",
      "iter  1999  loss  -0.34638956  grad l2 norm  0.004994099\n",
      "iter  2000  loss  -0.3463928  grad l2 norm  0.0050525926\n",
      "iter  2001  loss  -0.34639588  grad l2 norm  0.0049933083\n",
      "iter  2002  loss  -0.34639913  grad l2 norm  0.005052988\n",
      "iter  2003  loss  -0.3464022  grad l2 norm  0.0049962597\n",
      "iter  2004  loss  -0.3464053  grad l2 norm  0.0050580315\n",
      "iter  2005  loss  -0.34640825  grad l2 norm  0.0050050355\n",
      "iter  2006  loss  -0.34641138  grad l2 norm  0.0050696842\n",
      "iter  2007  loss  -0.34641424  grad l2 norm  0.005017716\n",
      "iter  2008  loss  -0.3464173  grad l2 norm  0.0050734864\n",
      "iter  2009  loss  -0.3464205  grad l2 norm  0.0050051813\n",
      "iter  2010  loss  -0.34642363  grad l2 norm  0.005048449\n",
      "iter  2011  loss  -0.34642658  grad l2 norm  0.0049807504\n",
      "iter  2012  loss  -0.3464296  grad l2 norm  0.005034592\n",
      "iter  2013  loss  -0.3464325  grad l2 norm  0.0049798274\n",
      "iter  2014  loss  -0.34643546  grad l2 norm  0.00504197\n",
      "iter  2015  loss  -0.34643835  grad l2 norm  0.0049900715\n",
      "iter  2016  loss  -0.34644127  grad l2 norm  0.005050393\n",
      "iter  2017  loss  -0.34644422  grad l2 norm  0.0049955943\n",
      "iter  2018  loss  -0.3464471  grad l2 norm  0.005053013\n",
      "iter  2019  loss  -0.34644988  grad l2 norm  0.0049961335\n",
      "iter  2020  loss  -0.3464529  grad l2 norm  0.0050517833\n",
      "iter  2021  loss  -0.34645572  grad l2 norm  0.004994926\n",
      "iter  2022  loss  -0.34645864  grad l2 norm  0.0050511556\n",
      "iter  2023  loss  -0.3464615  grad l2 norm  0.004996104\n",
      "iter  2024  loss  -0.34646437  grad l2 norm  0.005053667\n",
      "iter  2025  loss  -0.34646714  grad l2 norm  0.005000936\n",
      "iter  2026  loss  -0.34646982  grad l2 norm  0.0050602737\n",
      "iter  2027  loss  -0.34647262  grad l2 norm  0.0050104163\n",
      "iter  2028  loss  -0.34647524  grad l2 norm  0.005071643\n",
      "iter  2029  loss  -0.34647796  grad l2 norm  0.0050225463\n",
      "iter  2030  loss  -0.34648073  grad l2 norm  0.0050754524\n",
      "iter  2031  loss  -0.34648356  grad l2 norm  0.0050099785\n",
      "iter  2032  loss  -0.3464864  grad l2 norm  0.0050491313\n",
      "iter  2033  loss  -0.34648907  grad l2 norm  0.0049832426\n",
      "iter  2034  loss  -0.34649187  grad l2 norm  0.005033758\n",
      "iter  2035  loss  -0.34649444  grad l2 norm  0.004983288\n",
      "iter  2036  loss  -0.34649703  grad l2 norm  0.0050444095\n",
      "iter  2037  loss  -0.3464996  grad l2 norm  0.004997457\n",
      "iter  2038  loss  -0.34650227  grad l2 norm  0.0050551984\n",
      "iter  2039  loss  -0.34650487  grad l2 norm  0.0050024088\n",
      "iter  2040  loss  -0.3465076  grad l2 norm  0.0050549144\n",
      "iter  2041  loss  -0.3465101  grad l2 norm  0.004999336\n",
      "iter  2042  loss  -0.34651273  grad l2 norm  0.0050506387\n",
      "iter  2043  loss  -0.3465153  grad l2 norm  0.0049961642\n",
      "iter  2044  loss  -0.3465179  grad l2 norm  0.0050493903\n",
      "iter  2045  loss  -0.34652042  grad l2 norm  0.0049982145\n",
      "iter  2046  loss  -0.34652296  grad l2 norm  0.0050540105\n",
      "iter  2047  loss  -0.34652537  grad l2 norm  0.005005384\n",
      "iter  2048  loss  -0.34652796  grad l2 norm  0.0050624525\n",
      "iter  2049  loss  -0.34653047  grad l2 norm  0.005015985\n",
      "iter  2050  loss  -0.34653288  grad l2 norm  0.0050743646\n",
      "iter  2051  loss  -0.3465352  grad l2 norm  0.005028268\n",
      "iter  2052  loss  -0.34653768  grad l2 norm  0.0050783316\n",
      "iter  2053  loss  -0.34654015  grad l2 norm  0.005015382\n",
      "iter  2054  loss  -0.34654284  grad l2 norm  0.0050507216\n",
      "iter  2055  loss  -0.34654525  grad l2 norm  0.004986654\n",
      "iter  2056  loss  -0.34654772  grad l2 norm  0.0050333757\n",
      "iter  2057  loss  -0.34655  grad l2 norm  0.004985954\n",
      "iter  2058  loss  -0.34655237  grad l2 norm  0.0050458345\n",
      "iter  2059  loss  -0.34655464  grad l2 norm  0.0050039953\n",
      "iter  2060  loss  -0.34655702  grad l2 norm  0.0050602127\n",
      "iter  2061  loss  -0.34655932  grad l2 norm  0.0050104097\n",
      "iter  2062  loss  -0.3465618  grad l2 norm  0.005058701\n",
      "iter  2063  loss  -0.3465641  grad l2 norm  0.0050044395\n",
      "iter  2064  loss  -0.3465665  grad l2 norm  0.005051503\n",
      "iter  2065  loss  -0.34656864  grad l2 norm  0.00499881\n",
      "iter  2066  loss  -0.34657112  grad l2 norm  0.005048891\n",
      "iter  2067  loss  -0.34657335  grad l2 norm  0.005000755\n",
      "iter  2068  loss  -0.3465757  grad l2 norm  0.0050542015\n",
      "iter  2069  loss  -0.3465779  grad l2 norm  0.005009565\n",
      "iter  2070  loss  -0.34658015  grad l2 norm  0.0050646253\n",
      "iter  2071  loss  -0.34658223  grad l2 norm  0.005021774\n",
      "iter  2072  loss  -0.3465845  grad l2 norm  0.0050776177\n",
      "iter  2073  loss  -0.34658656  grad l2 norm  0.0050345273\n",
      "iter  2074  loss  -0.34658885  grad l2 norm  0.005082194\n",
      "iter  2075  loss  -0.3465911  grad l2 norm  0.005021814\n",
      "iter  2076  loss  -0.3465934  grad l2 norm  0.0050532697\n",
      "iter  2077  loss  -0.34659562  grad l2 norm  0.0049906652\n",
      "iter  2078  loss  -0.34659788  grad l2 norm  0.005033689\n",
      "iter  2079  loss  -0.34659994  grad l2 norm  0.004988959\n",
      "iter  2080  loss  -0.3466021  grad l2 norm  0.005046802\n",
      "iter  2081  loss  -0.34660402  grad l2 norm  0.0050094654\n",
      "iter  2082  loss  -0.34660617  grad l2 norm  0.0050646043\n",
      "iter  2083  loss  -0.34660822  grad l2 norm  0.005018547\n",
      "iter  2084  loss  -0.34661046  grad l2 norm  0.0050635636\n",
      "iter  2085  loss  -0.34661257  grad l2 norm  0.005010739\n",
      "iter  2086  loss  -0.34661466  grad l2 norm  0.0050534955\n",
      "iter  2087  loss  -0.34661677  grad l2 norm  0.00500261\n",
      "iter  2088  loss  -0.34661886  grad l2 norm  0.005049114\n",
      "iter  2089  loss  -0.34662086  grad l2 norm  0.0050036437\n",
      "iter  2090  loss  -0.34662288  grad l2 norm  0.0050547253\n",
      "iter  2091  loss  -0.34662482  grad l2 norm  0.005013348\n",
      "iter  2092  loss  -0.34662694  grad l2 norm  0.005066643\n",
      "iter  2093  loss  -0.34662887  grad l2 norm  0.0050274236\n",
      "iter  2094  loss  -0.34663084  grad l2 norm  0.0050812946\n",
      "iter  2095  loss  -0.3466326  grad l2 norm  0.0050413897\n",
      "iter  2096  loss  -0.34663466  grad l2 norm  0.005086395\n",
      "iter  2097  loss  -0.3466367  grad l2 norm  0.005028346\n",
      "iter  2098  loss  -0.34663886  grad l2 norm  0.0050563687\n",
      "iter  2099  loss  -0.34664083  grad l2 norm  0.0049955\n",
      "iter  2100  loss  -0.34664288  grad l2 norm  0.005035167\n",
      "iter  2101  loss  -0.3466447  grad l2 norm  0.0049927267\n",
      "iter  2102  loss  -0.34664658  grad l2 norm  0.005048023\n",
      "iter  2103  loss  -0.34664825  grad l2 norm  0.0050145276\n",
      "iter  2104  loss  -0.34665015  grad l2 norm  0.0050687073\n",
      "iter  2105  loss  -0.34665194  grad l2 norm  0.0050265584\n",
      "iter  2106  loss  -0.346654  grad l2 norm  0.0050688037\n",
      "iter  2107  loss  -0.3466559  grad l2 norm  0.0050179507\n",
      "iter  2108  loss  -0.34665787  grad l2 norm  0.005056647\n",
      "iter  2109  loss  -0.3466597  grad l2 norm  0.005007095\n",
      "iter  2110  loss  -0.34666166  grad l2 norm  0.005050287\n",
      "iter  2111  loss  -0.3466634  grad l2 norm  0.005007071\n",
      "iter  2112  loss  -0.34666523  grad l2 norm  0.0050556576\n",
      "iter  2113  loss  -0.34666702  grad l2 norm  0.00501783\n",
      "iter  2114  loss  -0.34666884  grad l2 norm  0.0050692237\n",
      "iter  2115  loss  -0.34667054  grad l2 norm  0.005033402\n",
      "iter  2116  loss  -0.34667227  grad l2 norm  0.005085396\n",
      "iter  2117  loss  -0.34667394  grad l2 norm  0.0050485865\n",
      "iter  2118  loss  -0.34667566  grad l2 norm  0.005091048\n",
      "iter  2119  loss  -0.34667745  grad l2 norm  0.0050352714\n",
      "iter  2120  loss  -0.34667954  grad l2 norm  0.0050598774\n",
      "iter  2121  loss  -0.34668124  grad l2 norm  0.0050008087\n",
      "iter  2122  loss  -0.3466831  grad l2 norm  0.0050370204\n",
      "iter  2123  loss  -0.3466847  grad l2 norm  0.004996692\n",
      "iter  2124  loss  -0.34668642  grad l2 norm  0.0050493856\n",
      "iter  2125  loss  -0.34668797  grad l2 norm  0.0050192084\n",
      "iter  2126  loss  -0.34668967  grad l2 norm  0.0050718365\n",
      "iter  2127  loss  -0.34669134  grad l2 norm  0.005033448\n",
      "iter  2128  loss  -0.34669304  grad l2 norm  0.0050736023\n",
      "iter  2129  loss  -0.3466948  grad l2 norm  0.005025086\n",
      "iter  2130  loss  -0.34669656  grad l2 norm  0.005060644\n",
      "iter  2131  loss  -0.34669814  grad l2 norm  0.005013265\n",
      "iter  2132  loss  -0.3466999  grad l2 norm  0.005053159\n",
      "iter  2133  loss  -0.34670147  grad l2 norm  0.0050122607\n",
      "iter  2134  loss  -0.3467032  grad l2 norm  0.0050582197\n",
      "iter  2135  loss  -0.34670484  grad l2 norm  0.005022818\n",
      "iter  2136  loss  -0.3467064  grad l2 norm  0.0050719324\n",
      "iter  2137  loss  -0.34670788  grad l2 norm  0.0050390507\n",
      "iter  2138  loss  -0.3467095  grad l2 norm  0.005088679\n",
      "iter  2139  loss  -0.3467109  grad l2 norm  0.005054893\n",
      "iter  2140  loss  -0.34671256  grad l2 norm  0.0050950204\n",
      "iter  2141  loss  -0.3467141  grad l2 norm  0.005041522\n",
      "iter  2142  loss  -0.34671596  grad l2 norm  0.0050631263\n",
      "iter  2143  loss  -0.34671754  grad l2 norm  0.0050062886\n",
      "iter  2144  loss  -0.34671918  grad l2 norm  0.0050397846\n",
      "iter  2145  loss  -0.34672067  grad l2 norm  0.0050020614\n",
      "iter  2146  loss  -0.34672222  grad l2 norm  0.0050526215\n",
      "iter  2147  loss  -0.3467235  grad l2 norm  0.0050252816\n",
      "iter  2148  loss  -0.34672502  grad l2 norm  0.005076035\n",
      "iter  2149  loss  -0.34672642  grad l2 norm  0.005040084\n",
      "iter  2150  loss  -0.34672803  grad l2 norm  0.0050775404\n",
      "iter  2151  loss  -0.34672958  grad l2 norm  0.0050312965\n",
      "iter  2152  loss  -0.34673116  grad l2 norm  0.0050640036\n",
      "iter  2153  loss  -0.3467327  grad l2 norm  0.0050190045\n",
      "iter  2154  loss  -0.34673426  grad l2 norm  0.005056481\n",
      "iter  2155  loss  -0.3467356  grad l2 norm  0.005018191\n",
      "iter  2156  loss  -0.34673718  grad l2 norm  0.0050617554\n",
      "iter  2157  loss  -0.34673852  grad l2 norm  0.0050287335\n",
      "iter  2158  loss  -0.34674  grad l2 norm  0.0050752917\n",
      "iter  2159  loss  -0.34674132  grad l2 norm  0.0050447867\n",
      "iter  2160  loss  -0.3467428  grad l2 norm  0.005092024\n",
      "iter  2161  loss  -0.34674403  grad l2 norm  0.0050610513\n",
      "iter  2162  loss  -0.34674552  grad l2 norm  0.005098953\n",
      "iter  2163  loss  -0.34674698  grad l2 norm  0.00504781\n",
      "iter  2164  loss  -0.34674853  grad l2 norm  0.0050669685\n",
      "iter  2165  loss  -0.34675002  grad l2 norm  0.0050123716\n",
      "iter  2166  loss  -0.34675148  grad l2 norm  0.005043356\n",
      "iter  2167  loss  -0.34675276  grad l2 norm  0.0050081266\n",
      "iter  2168  loss  -0.3467541  grad l2 norm  0.0050565186\n",
      "iter  2169  loss  -0.34675536  grad l2 norm  0.0050316243\n",
      "iter  2170  loss  -0.34675673  grad l2 norm  0.0050796964\n",
      "iter  2171  loss  -0.3467579  grad l2 norm  0.005046032\n",
      "iter  2172  loss  -0.3467594  grad l2 norm  0.0050810524\n",
      "iter  2173  loss  -0.34676078  grad l2 norm  0.005037114\n",
      "iter  2174  loss  -0.3467622  grad l2 norm  0.0050675697\n",
      "iter  2175  loss  -0.34676346  grad l2 norm  0.0050248792\n",
      "iter  2176  loss  -0.3467649  grad l2 norm  0.0050597796\n",
      "iter  2177  loss  -0.3467662  grad l2 norm  0.0050238455\n",
      "iter  2178  loss  -0.34676751  grad l2 norm  0.00506515\n",
      "iter  2179  loss  -0.3467688  grad l2 norm  0.00503499\n",
      "iter  2180  loss  -0.34677014  grad l2 norm  0.0050793863\n",
      "iter  2181  loss  -0.34677124  grad l2 norm  0.0050514177\n",
      "iter  2182  loss  -0.34677252  grad l2 norm  0.005096673\n",
      "iter  2183  loss  -0.34677365  grad l2 norm  0.0050680484\n",
      "iter  2184  loss  -0.34677494  grad l2 norm  0.005103221\n",
      "iter  2185  loss  -0.3467763  grad l2 norm  0.0050536343\n",
      "iter  2186  loss  -0.34677783  grad l2 norm  0.00507\n",
      "iter  2187  loss  -0.34677908  grad l2 norm  0.0050177956\n",
      "iter  2188  loss  -0.34678042  grad l2 norm  0.005046961\n",
      "iter  2189  loss  -0.3467815  grad l2 norm  0.0050146473\n",
      "iter  2190  loss  -0.34678268  grad l2 norm  0.005061131\n",
      "iter  2191  loss  -0.34678376  grad l2 norm  0.0050386144\n",
      "iter  2192  loss  -0.34678498  grad l2 norm  0.0050836783\n",
      "iter  2193  loss  -0.34678608  grad l2 norm  0.005051216\n",
      "iter  2194  loss  -0.3467875  grad l2 norm  0.005083041\n",
      "iter  2195  loss  -0.34678867  grad l2 norm  0.0050415494\n",
      "iter  2196  loss  -0.34679008  grad l2 norm  0.0050704586\n",
      "iter  2197  loss  -0.3467912  grad l2 norm  0.0050309463\n",
      "iter  2198  loss  -0.34679243  grad l2 norm  0.0050647086\n",
      "iter  2199  loss  -0.34679356  grad l2 norm  0.0050316886\n",
      "iter  2200  loss  -0.34679478  grad l2 norm  0.005071045\n",
      "iter  2201  loss  -0.34679595  grad l2 norm  0.0050428296\n",
      "iter  2202  loss  -0.34679708  grad l2 norm  0.0050844806\n",
      "iter  2203  loss  -0.34679815  grad l2 norm  0.005058189\n",
      "iter  2204  loss  -0.3467992  grad l2 norm  0.005100704\n",
      "iter  2205  loss  -0.34680027  grad l2 norm  0.0050737415\n",
      "iter  2206  loss  -0.3468014  grad l2 norm  0.005106326\n",
      "iter  2207  loss  -0.3468026  grad l2 norm  0.0050589005\n",
      "iter  2208  loss  -0.34680393  grad l2 norm  0.0050734733\n",
      "iter  2209  loss  -0.346805  grad l2 norm  0.005024472\n",
      "iter  2210  loss  -0.34680632  grad l2 norm  0.0050520734\n",
      "iter  2211  loss  -0.3468073  grad l2 norm  0.0050219125\n",
      "iter  2212  loss  -0.34680837  grad l2 norm  0.00506533\n",
      "iter  2213  loss  -0.34680924  grad l2 norm  0.005044099\n",
      "iter  2214  loss  -0.34681043  grad l2 norm  0.005085605\n",
      "iter  2215  loss  -0.3468115  grad l2 norm  0.005055043\n",
      "iter  2216  loss  -0.34681264  grad l2 norm  0.0050860625\n",
      "iter  2217  loss  -0.34681368  grad l2 norm  0.0050481833\n",
      "iter  2218  loss  -0.34681484  grad l2 norm  0.005076206\n",
      "iter  2219  loss  -0.34681597  grad l2 norm  0.0050393874\n",
      "iter  2220  loss  -0.3468171  grad l2 norm  0.0050707585\n",
      "iter  2221  loss  -0.34681812  grad l2 norm  0.005039048\n",
      "iter  2222  loss  -0.34681925  grad l2 norm  0.005075121\n",
      "iter  2223  loss  -0.3468202  grad l2 norm  0.0050479434\n",
      "iter  2224  loss  -0.3468213  grad l2 norm  0.0050870213\n",
      "iter  2225  loss  -0.34682217  grad l2 norm  0.005063426\n",
      "iter  2226  loss  -0.34682325  grad l2 norm  0.005105392\n",
      "iter  2227  loss  -0.346824  grad l2 norm  0.0050819367\n",
      "iter  2228  loss  -0.346825  grad l2 norm  0.005111526\n",
      "iter  2229  loss  -0.34682617  grad l2 norm  0.0050644856\n",
      "iter  2230  loss  -0.34682742  grad l2 norm  0.0050756396\n",
      "iter  2231  loss  -0.3468285  grad l2 norm  0.0050284686\n",
      "iter  2232  loss  -0.34682962  grad l2 norm  0.0050549065\n",
      "iter  2233  loss  -0.34683046  grad l2 norm  0.005028542\n",
      "iter  2234  loss  -0.34683138  grad l2 norm  0.0050713927\n",
      "iter  2235  loss  -0.34683225  grad l2 norm  0.0050519477\n",
      "iter  2236  loss  -0.34683323  grad l2 norm  0.0050903433\n",
      "iter  2237  loss  -0.34683424  grad l2 norm  0.005060567\n",
      "iter  2238  loss  -0.34683526  grad l2 norm  0.0050892406\n",
      "iter  2239  loss  -0.3468362  grad l2 norm  0.0050530997\n",
      "iter  2240  loss  -0.34683728  grad l2 norm  0.0050790077\n",
      "iter  2241  loss  -0.34683827  grad l2 norm  0.0050442438\n",
      "iter  2242  loss  -0.3468393  grad l2 norm  0.0050740195\n",
      "iter  2243  loss  -0.34684017  grad l2 norm  0.005044954\n",
      "iter  2244  loss  -0.3468411  grad l2 norm  0.0050805183\n",
      "iter  2245  loss  -0.3468419  grad l2 norm  0.0050574616\n",
      "iter  2246  loss  -0.34684289  grad l2 norm  0.0050953683\n",
      "iter  2247  loss  -0.34684375  grad l2 norm  0.005072039\n",
      "iter  2248  loss  -0.34684464  grad l2 norm  0.0051082857\n",
      "iter  2249  loss  -0.34684536  grad l2 norm  0.005084678\n",
      "iter  2250  loss  -0.3468464  grad l2 norm  0.0051122257\n",
      "iter  2251  loss  -0.34684741  grad l2 norm  0.0050682453\n",
      "iter  2252  loss  -0.34684852  grad l2 norm  0.0050801137\n",
      "iter  2253  loss  -0.34684947  grad l2 norm  0.0050371774\n",
      "iter  2254  loss  -0.3468504  grad l2 norm  0.005063739\n",
      "iter  2255  loss  -0.34685117  grad l2 norm  0.00503913\n",
      "iter  2256  loss  -0.34685203  grad l2 norm  0.0050773155\n",
      "iter  2257  loss  -0.3468528  grad l2 norm  0.005055386\n",
      "iter  2258  loss  -0.34685382  grad l2 norm  0.005089343\n",
      "iter  2259  loss  -0.3468547  grad l2 norm  0.005061016\n",
      "iter  2260  loss  -0.34685558  grad l2 norm  0.005088889\n",
      "iter  2261  loss  -0.34685642  grad l2 norm  0.005056728\n",
      "iter  2262  loss  -0.34685737  grad l2 norm  0.0050837817\n",
      "iter  2263  loss  -0.34685814  grad l2 norm  0.0050544934\n",
      "iter  2264  loss  -0.346859  grad l2 norm  0.0050855926\n",
      "iter  2265  loss  -0.34685978  grad l2 norm  0.0050585754\n",
      "iter  2266  loss  -0.3468608  grad l2 norm  0.005087796\n",
      "iter  2267  loss  -0.3468616  grad l2 norm  0.0050617806\n",
      "iter  2268  loss  -0.34686258  grad l2 norm  0.005095177\n",
      "iter  2269  loss  -0.3468633  grad l2 norm  0.005074313\n",
      "iter  2270  loss  -0.34686404  grad l2 norm  0.005112512\n",
      "iter  2271  loss  -0.3468648  grad l2 norm  0.0050930292\n",
      "iter  2272  loss  -0.34686562  grad l2 norm  0.0051196385\n",
      "iter  2273  loss  -0.34686646  grad l2 norm  0.005077164\n",
      "iter  2274  loss  -0.34686744  grad l2 norm  0.0050867633\n",
      "iter  2275  loss  -0.3468683  grad l2 norm  0.005044855\n",
      "iter  2276  loss  -0.3468691  grad l2 norm  0.005068015\n",
      "iter  2277  loss  -0.34686986  grad l2 norm  0.005041793\n",
      "iter  2278  loss  -0.34687072  grad l2 norm  0.00507483\n",
      "iter  2279  loss  -0.34687147  grad l2 norm  0.0050535533\n",
      "iter  2280  loss  -0.34687233  grad l2 norm  0.005087925\n",
      "iter  2281  loss  -0.34687296  grad l2 norm  0.0050651473\n",
      "iter  2282  loss  -0.34687385  grad l2 norm  0.0050965026\n",
      "iter  2283  loss  -0.34687445  grad l2 norm  0.005070904\n",
      "iter  2284  loss  -0.34687528  grad l2 norm  0.0050970437\n",
      "iter  2285  loss  -0.34687606  grad l2 norm  0.0050660963\n",
      "iter  2286  loss  -0.34687695  grad l2 norm  0.005089328\n",
      "iter  2287  loss  -0.34687778  grad l2 norm  0.0050611515\n",
      "iter  2288  loss  -0.3468786  grad l2 norm  0.005090044\n",
      "iter  2289  loss  -0.34687933  grad l2 norm  0.0050674304\n",
      "iter  2290  loss  -0.34688014  grad l2 norm  0.005100986\n",
      "iter  2291  loss  -0.3468807  grad l2 norm  0.0050828583\n",
      "iter  2292  loss  -0.34688142  grad l2 norm  0.0051198653\n",
      "iter  2293  loss  -0.34688196  grad l2 norm  0.0051013813\n",
      "iter  2294  loss  -0.3468828  grad l2 norm  0.0051246528\n",
      "iter  2295  loss  -0.34688365  grad l2 norm  0.0050820922\n",
      "iter  2296  loss  -0.34688455  grad l2 norm  0.0050890786\n",
      "iter  2297  loss  -0.34688526  grad l2 norm  0.005048054\n",
      "iter  2298  loss  -0.3468861  grad l2 norm  0.005067724\n",
      "iter  2299  loss  -0.34688675  grad l2 norm  0.0050422368\n",
      "iter  2300  loss  -0.34688753  grad l2 norm  0.0050757364\n",
      "iter  2301  loss  -0.34688812  grad l2 norm  0.0050610704\n",
      "iter  2302  loss  -0.34688878  grad l2 norm  0.005099169\n",
      "iter  2303  loss  -0.34688932  grad l2 norm  0.0050791563\n",
      "iter  2304  loss  -0.3468902  grad l2 norm  0.005104777\n",
      "iter  2305  loss  -0.346891  grad l2 norm  0.00507664\n",
      "iter  2306  loss  -0.3468917  grad l2 norm  0.0050995853\n",
      "iter  2307  loss  -0.34689242  grad l2 norm  0.0050707967\n",
      "iter  2308  loss  -0.34689325  grad l2 norm  0.005094734\n",
      "iter  2309  loss  -0.34689385  grad l2 norm  0.005069043\n",
      "iter  2310  loss  -0.34689465  grad l2 norm  0.005096544\n",
      "iter  2311  loss  -0.3468953  grad l2 norm  0.0050746338\n",
      "iter  2312  loss  -0.34689593  grad l2 norm  0.0051056673\n",
      "iter  2313  loss  -0.34689653  grad l2 norm  0.0050884956\n",
      "iter  2314  loss  -0.34689716  grad l2 norm  0.005123917\n",
      "iter  2315  loss  -0.3468976  grad l2 norm  0.0051068496\n",
      "iter  2316  loss  -0.34689838  grad l2 norm  0.0051279534\n",
      "iter  2317  loss  -0.34689906  grad l2 norm  0.005087057\n",
      "iter  2318  loss  -0.34689996  grad l2 norm  0.0050923405\n",
      "iter  2319  loss  -0.34690067  grad l2 norm  0.005052678\n",
      "iter  2320  loss  -0.3469014  grad l2 norm  0.0050718496\n",
      "iter  2321  loss  -0.34690195  grad l2 norm  0.0050487407\n",
      "iter  2322  loss  -0.3469027  grad l2 norm  0.0050795106\n",
      "iter  2323  loss  -0.34690323  grad l2 norm  0.0050655343\n",
      "iter  2324  loss  -0.34690392  grad l2 norm  0.005101324\n",
      "iter  2325  loss  -0.34690443  grad l2 norm  0.0050850464\n",
      "iter  2326  loss  -0.34690517  grad l2 norm  0.005113915\n",
      "iter  2327  loss  -0.3469057  grad l2 norm  0.005089216\n",
      "iter  2328  loss  -0.3469064  grad l2 norm  0.0051105483\n",
      "iter  2329  loss  -0.34690705  grad l2 norm  0.0050815605\n",
      "iter  2330  loss  -0.34690782  grad l2 norm  0.0051012775\n",
      "iter  2331  loss  -0.3469084  grad l2 norm  0.0050739585\n",
      "iter  2332  loss  -0.34690908  grad l2 norm  0.005097505\n",
      "iter  2333  loss  -0.3469098  grad l2 norm  0.005076142\n",
      "iter  2334  loss  -0.3469104  grad l2 norm  0.005106163\n",
      "iter  2335  loss  -0.3469108  grad l2 norm  0.0050923754\n",
      "iter  2336  loss  -0.34691128  grad l2 norm  0.0051287673\n",
      "iter  2337  loss  -0.34691167  grad l2 norm  0.0051163253\n",
      "iter  2338  loss  -0.3469123  grad l2 norm  0.0051378976\n",
      "iter  2339  loss  -0.34691292  grad l2 norm  0.005097759\n",
      "iter  2340  loss  -0.34691384  grad l2 norm  0.0050971145\n",
      "iter  2341  loss  -0.3469145  grad l2 norm  0.0050535235\n",
      "iter  2342  loss  -0.34691525  grad l2 norm  0.005067583\n",
      "iter  2343  loss  -0.34691584  grad l2 norm  0.0050477087\n",
      "iter  2344  loss  -0.34691644  grad l2 norm  0.005083921\n",
      "iter  2345  loss  -0.3469168  grad l2 norm  0.005076688\n",
      "iter  2346  loss  -0.34691745  grad l2 norm  0.0051145023\n",
      "iter  2347  loss  -0.3469179  grad l2 norm  0.005100119\n",
      "iter  2348  loss  -0.34691855  grad l2 norm  0.0051256553\n",
      "iter  2349  loss  -0.34691912  grad l2 norm  0.0050987992\n",
      "iter  2350  loss  -0.34691975  grad l2 norm  0.00511444\n",
      "iter  2351  loss  -0.34692043  grad l2 norm  0.005083117\n",
      "iter  2352  loss  -0.34692106  grad l2 norm  0.00509933\n",
      "iter  2353  loss  -0.34692165  grad l2 norm  0.005073112\n",
      "iter  2354  loss  -0.34692228  grad l2 norm  0.005096845\n",
      "iter  2355  loss  -0.34692276  grad l2 norm  0.005080351\n",
      "iter  2356  loss  -0.34692332  grad l2 norm  0.0051137465\n",
      "iter  2357  loss  -0.34692362  grad l2 norm  0.0051066526\n",
      "iter  2358  loss  -0.34692392  grad l2 norm  0.005145168\n",
      "iter  2359  loss  -0.34692433  grad l2 norm  0.0051303036\n",
      "iter  2360  loss  -0.3469251  grad l2 norm  0.0051396773\n",
      "iter  2361  loss  -0.3469259  grad l2 norm  0.0050925696\n",
      "iter  2362  loss  -0.3469267  grad l2 norm  0.0050897216\n",
      "iter  2363  loss  -0.3469273  grad l2 norm  0.0050520077\n",
      "iter  2364  loss  -0.34692788  grad l2 norm  0.005072469\n",
      "iter  2365  loss  -0.34692827  grad l2 norm  0.005059683\n",
      "iter  2366  loss  -0.34692883  grad l2 norm  0.005097749\n",
      "iter  2367  loss  -0.3469292  grad l2 norm  0.005092182\n",
      "iter  2368  loss  -0.34692976  grad l2 norm  0.0051264726\n",
      "iter  2369  loss  -0.34693024  grad l2 norm  0.0051095053\n",
      "iter  2370  loss  -0.34693086  grad l2 norm  0.005129111\n",
      "iter  2371  loss  -0.34693134  grad l2 norm  0.005099983\n",
      "iter  2372  loss  -0.34693196  grad l2 norm  0.005112464\n",
      "iter  2373  loss  -0.34693244  grad l2 norm  0.005083139\n",
      "iter  2374  loss  -0.34693304  grad l2 norm  0.00510081\n",
      "iter  2375  loss  -0.3469335  grad l2 norm  0.00508117\n",
      "iter  2376  loss  -0.3469341  grad l2 norm  0.0051088096\n",
      "iter  2377  loss  -0.3469344  grad l2 norm  0.00509532\n",
      "iter  2378  loss  -0.34693494  grad l2 norm  0.005124799\n",
      "iter  2379  loss  -0.3469353  grad l2 norm  0.00511482\n",
      "iter  2380  loss  -0.34693572  grad l2 norm  0.005145963\n",
      "iter  2381  loss  -0.3469361  grad l2 norm  0.005128193\n",
      "iter  2382  loss  -0.34693685  grad l2 norm  0.0051374985\n",
      "iter  2383  loss  -0.3469375  grad l2 norm  0.005094144\n",
      "iter  2384  loss  -0.34693822  grad l2 norm  0.0050944723\n",
      "iter  2385  loss  -0.3469387  grad l2 norm  0.005061947\n",
      "iter  2386  loss  -0.34693924  grad l2 norm  0.0050834343\n",
      "iter  2387  loss  -0.34693968  grad l2 norm  0.0050720037\n",
      "iter  2388  loss  -0.34694013  grad l2 norm  0.005107292\n",
      "iter  2389  loss  -0.34694043  grad l2 norm  0.0051000607\n",
      "iter  2390  loss  -0.34694102  grad l2 norm  0.005129953\n",
      "iter  2391  loss  -0.3469413  grad l2 norm  0.0051122103\n",
      "iter  2392  loss  -0.34694192  grad l2 norm  0.0051302775\n",
      "iter  2393  loss  -0.3469423  grad l2 norm  0.00510436\n",
      "iter  2394  loss  -0.34694296  grad l2 norm  0.005118777\n",
      "iter  2395  loss  -0.34694332  grad l2 norm  0.0050916867\n",
      "iter  2396  loss  -0.34694394  grad l2 norm  0.005106505\n",
      "iter  2397  loss  -0.3469444  grad l2 norm  0.00508767\n",
      "iter  2398  loss  -0.346945  grad l2 norm  0.0051140413\n",
      "iter  2399  loss  -0.34694526  grad l2 norm  0.005103309\n",
      "iter  2400  loss  -0.34694573  grad l2 norm  0.005132365\n",
      "iter  2401  loss  -0.34694606  grad l2 norm  0.0051211943\n",
      "iter  2402  loss  -0.34694642  grad l2 norm  0.005149575\n",
      "iter  2403  loss  -0.3469468  grad l2 norm  0.005132158\n",
      "iter  2404  loss  -0.34694743  grad l2 norm  0.0051405905\n",
      "iter  2405  loss  -0.34694806  grad l2 norm  0.0050991895\n",
      "iter  2406  loss  -0.34694868  grad l2 norm  0.0050995164\n",
      "iter  2407  loss  -0.3469492  grad l2 norm  0.0050692903\n",
      "iter  2408  loss  -0.34694964  grad l2 norm  0.0050899107\n",
      "iter  2409  loss  -0.34694993  grad l2 norm  0.005079994\n",
      "iter  2410  loss  -0.34695047  grad l2 norm  0.0051143444\n",
      "iter  2411  loss  -0.3469507  grad l2 norm  0.0051090783\n",
      "iter  2412  loss  -0.34695107  grad l2 norm  0.0051378696\n",
      "iter  2413  loss  -0.34695148  grad l2 norm  0.0051194406\n",
      "iter  2414  loss  -0.34695202  grad l2 norm  0.005132516\n",
      "iter  2415  loss  -0.34695244  grad l2 norm  0.0051060957\n",
      "iter  2416  loss  -0.34695306  grad l2 norm  0.0051196604\n",
      "iter  2417  loss  -0.3469534  grad l2 norm  0.0050974702\n",
      "iter  2418  loss  -0.3469539  grad l2 norm  0.0051176255\n",
      "iter  2419  loss  -0.34695426  grad l2 norm  0.0051021175\n",
      "iter  2420  loss  -0.34695476  grad l2 norm  0.0051250486\n",
      "iter  2421  loss  -0.34695515  grad l2 norm  0.005109288\n",
      "iter  2422  loss  -0.34695557  grad l2 norm  0.0051326356\n",
      "iter  2423  loss  -0.3469559  grad l2 norm  0.005120983\n",
      "iter  2424  loss  -0.34695628  grad l2 norm  0.005149627\n",
      "iter  2425  loss  -0.34695655  grad l2 norm  0.005135461\n",
      "iter  2426  loss  -0.34695706  grad l2 norm  0.0051446315\n",
      "iter  2427  loss  -0.3469576  grad l2 norm  0.005106586\n",
      "iter  2428  loss  -0.34695825  grad l2 norm  0.005108083\n",
      "iter  2429  loss  -0.34695858  grad l2 norm  0.0050813374\n",
      "iter  2430  loss  -0.34695905  grad l2 norm  0.0051014875\n",
      "iter  2431  loss  -0.34695932  grad l2 norm  0.0050902534\n",
      "iter  2432  loss  -0.3469598  grad l2 norm  0.00511806\n",
      "iter  2433  loss  -0.3469601  grad l2 norm  0.0051101805\n",
      "iter  2434  loss  -0.34696054  grad l2 norm  0.005136769\n",
      "iter  2435  loss  -0.3469609  grad l2 norm  0.0051220926\n",
      "iter  2436  loss  -0.34696135  grad l2 norm  0.005140341\n",
      "iter  2437  loss  -0.34696168  grad l2 norm  0.005119205\n",
      "iter  2438  loss  -0.34696212  grad l2 norm  0.0051341127\n",
      "iter  2439  loss  -0.34696242  grad l2 norm  0.0051119174\n",
      "iter  2440  loss  -0.34696305  grad l2 norm  0.005125541\n",
      "iter  2441  loss  -0.34696335  grad l2 norm  0.005103262\n",
      "iter  2442  loss  -0.34696394  grad l2 norm  0.0051200525\n",
      "iter  2443  loss  -0.3469642  grad l2 norm  0.0051045516\n",
      "iter  2444  loss  -0.34696472  grad l2 norm  0.005129922\n",
      "iter  2445  loss  -0.34696487  grad l2 norm  0.005124596\n",
      "iter  2446  loss  -0.3469651  grad l2 norm  0.0051579103\n",
      "iter  2447  loss  -0.3469653  grad l2 norm  0.0051491577\n",
      "iter  2448  loss  -0.34696588  grad l2 norm  0.005158555\n",
      "iter  2449  loss  -0.34696633  grad l2 norm  0.0051197414\n",
      "iter  2450  loss  -0.346967  grad l2 norm  0.0051154057\n",
      "iter  2451  loss  -0.34696734  grad l2 norm  0.005085278\n",
      "iter  2452  loss  -0.34696788  grad l2 norm  0.0051021697\n",
      "iter  2453  loss  -0.3469682  grad l2 norm  0.00509275\n",
      "iter  2454  loss  -0.3469685  grad l2 norm  0.0051239408\n",
      "iter  2455  loss  -0.3469687  grad l2 norm  0.00512029\n",
      "iter  2456  loss  -0.34696907  grad l2 norm  0.0051481016\n",
      "iter  2457  loss  -0.3469693  grad l2 norm  0.0051354445\n",
      "iter  2458  loss  -0.34696972  grad l2 norm  0.005152017\n",
      "iter  2459  loss  -0.34697005  grad l2 norm  0.0051282267\n",
      "iter  2460  loss  -0.34697062  grad l2 norm  0.00513528\n",
      "iter  2461  loss  -0.34697106  grad l2 norm  0.005107144\n",
      "iter  2462  loss  -0.34697163  grad l2 norm  0.005117025\n",
      "iter  2463  loss  -0.34697187  grad l2 norm  0.005098783\n",
      "iter  2464  loss  -0.34697226  grad l2 norm  0.005121311\n",
      "iter  2465  loss  -0.34697247  grad l2 norm  0.005114468\n",
      "iter  2466  loss  -0.34697282  grad l2 norm  0.0051436834\n",
      "iter  2467  loss  -0.34697297  grad l2 norm  0.0051408196\n",
      "iter  2468  loss  -0.34697327  grad l2 norm  0.0051705274\n",
      "iter  2469  loss  -0.34697357  grad l2 norm  0.005158904\n",
      "iter  2470  loss  -0.34697407  grad l2 norm  0.0051638074\n",
      "iter  2471  loss  -0.34697458  grad l2 norm  0.0051243776\n",
      "iter  2472  loss  -0.3469751  grad l2 norm  0.005120355\n",
      "iter  2473  loss  -0.34697554  grad l2 norm  0.0050921417\n",
      "iter  2474  loss  -0.34697592  grad l2 norm  0.0051089125\n",
      "iter  2475  loss  -0.34697613  grad l2 norm  0.005101353\n",
      "iter  2476  loss  -0.34697643  grad l2 norm  0.005132152\n",
      "iter  2477  loss  -0.34697652  grad l2 norm  0.0051295953\n",
      "iter  2478  loss  -0.34697685  grad l2 norm  0.005155578\n",
      "iter  2479  loss  -0.34697717  grad l2 norm  0.005140547\n",
      "iter  2480  loss  -0.34697768  grad l2 norm  0.005150714\n",
      "iter  2481  loss  -0.34697804  grad l2 norm  0.005124301\n",
      "iter  2482  loss  -0.3469784  grad l2 norm  0.005131728\n",
      "iter  2483  loss  -0.34697872  grad l2 norm  0.005109153\n",
      "iter  2484  loss  -0.34697923  grad l2 norm  0.0051227026\n",
      "iter  2485  loss  -0.3469796  grad l2 norm  0.005108835\n",
      "iter  2486  loss  -0.34697992  grad l2 norm  0.005131351\n",
      "iter  2487  loss  -0.34698018  grad l2 norm  0.0051254914\n",
      "iter  2488  loss  -0.3469805  grad l2 norm  0.005154509\n",
      "iter  2489  loss  -0.3469806  grad l2 norm  0.005153054\n",
      "iter  2490  loss  -0.3469809  grad l2 norm  0.0051823882\n",
      "iter  2491  loss  -0.3469811  grad l2 norm  0.0051694387\n",
      "iter  2492  loss  -0.34698159  grad l2 norm  0.0051703877\n",
      "iter  2493  loss  -0.3469821  grad l2 norm  0.0051292432\n",
      "iter  2494  loss  -0.3469826  grad l2 norm  0.005123055\n",
      "iter  2495  loss  -0.3469829  grad l2 norm  0.0050958265\n",
      "iter  2496  loss  -0.34698325  grad l2 norm  0.005112864\n",
      "iter  2497  loss  -0.34698343  grad l2 norm  0.0051080263\n",
      "iter  2498  loss  -0.34698367  grad l2 norm  0.005139053\n",
      "iter  2499  loss  -0.34698385  grad l2 norm  0.0051360987\n",
      "iter  2500  loss  -0.3469842  grad l2 norm  0.005158002\n",
      "iter  2501  loss  -0.34698445  grad l2 norm  0.005140648\n",
      "iter  2502  loss  -0.34698492  grad l2 norm  0.0051487056\n",
      "iter  2503  loss  -0.3469852  grad l2 norm  0.005125263\n",
      "iter  2504  loss  -0.34698576  grad l2 norm  0.0051349336\n",
      "iter  2505  loss  -0.346986  grad l2 norm  0.005117041\n",
      "iter  2506  loss  -0.34698635  grad l2 norm  0.005134228\n",
      "iter  2507  loss  -0.34698662  grad l2 norm  0.0051241713\n",
      "iter  2508  loss  -0.34698695  grad l2 norm  0.005147331\n",
      "iter  2509  loss  -0.3469872  grad l2 norm  0.005140975\n",
      "iter  2510  loss  -0.34698752  grad l2 norm  0.0051655825\n",
      "iter  2511  loss  -0.34698766  grad l2 norm  0.005160669\n",
      "iter  2512  loss  -0.34698778  grad l2 norm  0.0051851785\n",
      "iter  2513  loss  -0.34698805  grad l2 norm  0.0051703267\n",
      "iter  2514  loss  -0.3469886  grad l2 norm  0.0051696906\n",
      "iter  2515  loss  -0.34698892  grad l2 norm  0.005131085\n",
      "iter  2516  loss  -0.34698942  grad l2 norm  0.0051276255\n",
      "iter  2517  loss  -0.34698963  grad l2 norm  0.0051055104\n",
      "iter  2518  loss  -0.34698993  grad l2 norm  0.0051231063\n",
      "iter  2519  loss  -0.34699008  grad l2 norm  0.005115232\n",
      "iter  2520  loss  -0.34699053  grad l2 norm  0.005138329\n",
      "iter  2521  loss  -0.34699073  grad l2 norm  0.005132445\n",
      "iter  2522  loss  -0.34699103  grad l2 norm  0.005154479\n",
      "iter  2523  loss  -0.34699133  grad l2 norm  0.005143041\n",
      "iter  2524  loss  -0.34699166  grad l2 norm  0.005158016\n",
      "iter  2525  loss  -0.34699187  grad l2 norm  0.005140756\n",
      "iter  2526  loss  -0.34699225  grad l2 norm  0.005152358\n",
      "iter  2527  loss  -0.34699258  grad l2 norm  0.005134755\n",
      "iter  2528  loss  -0.3469929  grad l2 norm  0.0051481696\n",
      "iter  2529  loss  -0.34699315  grad l2 norm  0.005134022\n",
      "iter  2530  loss  -0.34699357  grad l2 norm  0.00515107\n",
      "iter  2531  loss  -0.34699374  grad l2 norm  0.005141258\n",
      "iter  2532  loss  -0.34699404  grad l2 norm  0.005163198\n",
      "iter  2533  loss  -0.34699413  grad l2 norm  0.0051600165\n",
      "iter  2534  loss  -0.34699425  grad l2 norm  0.00518765\n",
      "iter  2535  loss  -0.34699437  grad l2 norm  0.0051784753\n",
      "iter  2536  loss  -0.34699485  grad l2 norm  0.0051805573\n",
      "iter  2537  loss  -0.34699526  grad l2 norm  0.005144063\n",
      "iter  2538  loss  -0.3469957  grad l2 norm  0.005136056\n",
      "iter  2539  loss  -0.34699604  grad l2 norm  0.00510767\n",
      "iter  2540  loss  -0.34699646  grad l2 norm  0.005119003\n",
      "iter  2541  loss  -0.34699667  grad l2 norm  0.0051118657\n",
      "iter  2542  loss  -0.346997  grad l2 norm  0.005140187\n",
      "iter  2543  loss  -0.346997  grad l2 norm  0.0051415274\n",
      "iter  2544  loss  -0.3469973  grad l2 norm  0.0051680305\n",
      "iter  2545  loss  -0.34699747  grad l2 norm  0.0051596127\n",
      "iter  2546  loss  -0.3469979  grad l2 norm  0.0051732683\n",
      "iter  2547  loss  -0.34699807  grad l2 norm  0.0051538143\n",
      "iter  2548  loss  -0.34699848  grad l2 norm  0.0051604016\n",
      "iter  2549  loss  -0.34699875  grad l2 norm  0.0051393043\n",
      "iter  2550  loss  -0.34699908  grad l2 norm  0.0051487475\n",
      "iter  2551  loss  -0.3469993  grad l2 norm  0.005134258\n",
      "iter  2552  loss  -0.34699968  grad l2 norm  0.0051519\n",
      "iter  2553  loss  -0.34699976  grad l2 norm  0.00514473\n",
      "iter  2554  loss  -0.34700006  grad l2 norm  0.0051674843\n",
      "iter  2555  loss  -0.3470002  grad l2 norm  0.0051680985\n",
      "iter  2556  loss  -0.34700033  grad l2 norm  0.005197485\n",
      "iter  2557  loss  -0.34700033  grad l2 norm  0.005190168\n",
      "iter  2558  loss  -0.3470008  grad l2 norm  0.005189355\n",
      "iter  2559  loss  -0.34700128  grad l2 norm  0.005148317\n",
      "iter  2560  loss  -0.34700182  grad l2 norm  0.0051368903\n",
      "iter  2561  loss  -0.3470021  grad l2 norm  0.0051093344\n",
      "iter  2562  loss  -0.34700242  grad l2 norm  0.0051229517\n",
      "iter  2563  loss  -0.34700257  grad l2 norm  0.005120208\n",
      "iter  2564  loss  -0.34700277  grad l2 norm  0.0051505626\n",
      "iter  2565  loss  -0.34700277  grad l2 norm  0.0051534874\n",
      "iter  2566  loss  -0.3470031  grad l2 norm  0.0051784297\n",
      "iter  2567  loss  -0.34700322  grad l2 norm  0.005169115\n",
      "iter  2568  loss  -0.34700364  grad l2 norm  0.005179789\n",
      "iter  2569  loss  -0.3470038  grad l2 norm  0.005159077\n",
      "iter  2570  loss  -0.34700418  grad l2 norm  0.0051633394\n",
      "iter  2571  loss  -0.34700447  grad l2 norm  0.005141978\n",
      "iter  2572  loss  -0.34700483  grad l2 norm  0.0051502287\n",
      "iter  2573  loss  -0.34700504  grad l2 norm  0.005137825\n",
      "iter  2574  loss  -0.34700534  grad l2 norm  0.0051572737\n",
      "iter  2575  loss  -0.34700546  grad l2 norm  0.0051552705\n",
      "iter  2576  loss  -0.3470056  grad l2 norm  0.0051819286\n",
      "iter  2577  loss  -0.34700564  grad l2 norm  0.0051825996\n",
      "iter  2578  loss  -0.3470059  grad l2 norm  0.0052060033\n",
      "iter  2579  loss  -0.34700602  grad l2 norm  0.0051926663\n",
      "iter  2580  loss  -0.34700644  grad l2 norm  0.005188155\n",
      "iter  2581  loss  -0.3470069  grad l2 norm  0.0051482483\n",
      "iter  2582  loss  -0.34700733  grad l2 norm  0.0051388033\n",
      "iter  2583  loss  -0.34700763  grad l2 norm  0.0051156105\n",
      "iter  2584  loss  -0.3470079  grad l2 norm  0.005131506\n",
      "iter  2585  loss  -0.347008  grad l2 norm  0.0051313974\n",
      "iter  2586  loss  -0.34700808  grad l2 norm  0.0051619946\n",
      "iter  2587  loss  -0.3470082  grad l2 norm  0.0051644775\n",
      "iter  2588  loss  -0.3470086  grad l2 norm  0.005185532\n",
      "iter  2589  loss  -0.3470087  grad l2 norm  0.005174049\n",
      "iter  2590  loss  -0.347009  grad l2 norm  0.0051818434\n",
      "iter  2591  loss  -0.34700933  grad l2 norm  0.0051611476\n",
      "iter  2592  loss  -0.34700957  grad l2 norm  0.0051662396\n",
      "iter  2593  loss  -0.34700987  grad l2 norm  0.0051492006\n",
      "iter  2594  loss  -0.3470102  grad l2 norm  0.0051617078\n",
      "iter  2595  loss  -0.3470103  grad l2 norm  0.005153393\n",
      "iter  2596  loss  -0.34701058  grad l2 norm  0.0051723523\n",
      "iter  2597  loss  -0.34701076  grad l2 norm  0.005165521\n",
      "iter  2598  loss  -0.34701094  grad l2 norm  0.005183082\n",
      "iter  2599  loss  -0.34701103  grad l2 norm  0.005177772\n",
      "iter  2600  loss  -0.34701127  grad l2 norm  0.005199789\n",
      "iter  2601  loss  -0.34701133  grad l2 norm  0.005192082\n",
      "iter  2602  loss  -0.34701172  grad l2 norm  0.005193533\n",
      "iter  2603  loss  -0.34701204  grad l2 norm  0.0051594647\n",
      "iter  2604  loss  -0.3470125  grad l2 norm  0.005151406\n",
      "iter  2605  loss  -0.34701273  grad l2 norm  0.0051286486\n",
      "iter  2606  loss  -0.347013  grad l2 norm  0.0051418007\n",
      "iter  2607  loss  -0.34701312  grad l2 norm  0.005139406\n",
      "iter  2608  loss  -0.3470133  grad l2 norm  0.0051662982\n",
      "iter  2609  loss  -0.34701344  grad l2 norm  0.0051682885\n",
      "iter  2610  loss  -0.34701365  grad l2 norm  0.005190209\n",
      "iter  2611  loss  -0.3470138  grad l2 norm  0.0051811384\n",
      "iter  2612  loss  -0.3470141  grad l2 norm  0.00519081\n",
      "iter  2613  loss  -0.34701425  grad l2 norm  0.00517313\n",
      "iter  2614  loss  -0.34701458  grad l2 norm  0.0051793763\n",
      "iter  2615  loss  -0.34701478  grad l2 norm  0.0051626195\n",
      "iter  2616  loss  -0.34701502  grad l2 norm  0.0051704934\n",
      "iter  2617  loss  -0.34701526  grad l2 norm  0.005155237\n",
      "iter  2618  loss  -0.34701568  grad l2 norm  0.005166613\n",
      "iter  2619  loss  -0.3470158  grad l2 norm  0.005158182\n",
      "iter  2620  loss  -0.34701598  grad l2 norm  0.0051783193\n",
      "iter  2621  loss  -0.34701598  grad l2 norm  0.0051790127\n",
      "iter  2622  loss  -0.34701616  grad l2 norm  0.00520575\n",
      "iter  2623  loss  -0.34701616  grad l2 norm  0.005203989\n",
      "iter  2624  loss  -0.34701657  grad l2 norm  0.0052080704\n",
      "iter  2625  loss  -0.3470169  grad l2 norm  0.0051755193\n",
      "iter  2626  loss  -0.34701735  grad l2 norm  0.0051658433\n",
      "iter  2627  loss  -0.34701756  grad l2 norm  0.0051408904\n",
      "iter  2628  loss  -0.3470179  grad l2 norm  0.0051507503\n",
      "iter  2629  loss  -0.347018  grad l2 norm  0.005146133\n",
      "iter  2630  loss  -0.34701824  grad l2 norm  0.0051707258\n",
      "iter  2631  loss  -0.3470182  grad l2 norm  0.0051728752\n",
      "iter  2632  loss  -0.34701845  grad l2 norm  0.005195105\n",
      "iter  2633  loss  -0.3470185  grad l2 norm  0.005188778\n",
      "iter  2634  loss  -0.34701872  grad l2 norm  0.005200017\n",
      "iter  2635  loss  -0.3470189  grad l2 norm  0.005183075\n",
      "iter  2636  loss  -0.3470192  grad l2 norm  0.005184449\n",
      "iter  2637  loss  -0.34701952  grad l2 norm  0.005161683\n",
      "iter  2638  loss  -0.34701994  grad l2 norm  0.0051644794\n",
      "iter  2639  loss  -0.34702006  grad l2 norm  0.005150321\n",
      "iter  2640  loss  -0.34702036  grad l2 norm  0.0051655294\n",
      "iter  2641  loss  -0.34702048  grad l2 norm  0.0051636766\n",
      "iter  2642  loss  -0.34702066  grad l2 norm  0.0051884083\n",
      "iter  2643  loss  -0.3470207  grad l2 norm  0.0051935543\n",
      "iter  2644  loss  -0.34702075  grad l2 norm  0.0052223057\n",
      "iter  2645  loss  -0.34702083  grad l2 norm  0.0052215587\n",
      "iter  2646  loss  -0.34702107  grad l2 norm  0.005223914\n",
      "iter  2647  loss  -0.34702152  grad l2 norm  0.0051882635\n",
      "iter  2648  loss  -0.34702194  grad l2 norm  0.005172963\n",
      "iter  2649  loss  -0.34702218  grad l2 norm  0.0051441323\n",
      "iter  2650  loss  -0.34702247  grad l2 norm  0.005150845\n",
      "iter  2651  loss  -0.34702253  grad l2 norm  0.005146883\n",
      "iter  2652  loss  -0.3470227  grad l2 norm  0.0051732757\n",
      "iter  2653  loss  -0.34702268  grad l2 norm  0.005179986\n",
      "iter  2654  loss  -0.34702283  grad l2 norm  0.0052043917\n",
      "iter  2655  loss  -0.34702295  grad l2 norm  0.0051974016\n",
      "iter  2656  loss  -0.34702325  grad l2 norm  0.00520235\n",
      "iter  2657  loss  -0.34702352  grad l2 norm  0.005180284\n",
      "iter  2658  loss  -0.34702387  grad l2 norm  0.005180338\n",
      "iter  2659  loss  -0.34702405  grad l2 norm  0.0051616835\n",
      "iter  2660  loss  -0.34702432  grad l2 norm  0.0051706727\n",
      "iter  2661  loss  -0.3470244  grad l2 norm  0.00516309\n",
      "iter  2662  loss  -0.34702474  grad l2 norm  0.0051816106\n",
      "iter  2663  loss  -0.3470248  grad l2 norm  0.005181142\n",
      "iter  2664  loss  -0.34702498  grad l2 norm  0.005203517\n",
      "iter  2665  loss  -0.34702513  grad l2 norm  0.0052051786\n",
      "iter  2666  loss  -0.34702516  grad l2 norm  0.005228629\n",
      "iter  2667  loss  -0.3470251  grad l2 norm  0.005225713\n",
      "iter  2668  loss  -0.34702548  grad l2 norm  0.00522707\n",
      "iter  2669  loss  -0.3470258  grad l2 norm  0.0051917094\n",
      "iter  2670  loss  -0.34702632  grad l2 norm  0.005176308\n",
      "iter  2671  loss  -0.3470265  grad l2 norm  0.0051491843\n",
      "iter  2672  loss  -0.3470267  grad l2 norm  0.0051571564\n",
      "iter  2673  loss  -0.34702674  grad l2 norm  0.005156216\n",
      "iter  2674  loss  -0.34702682  grad l2 norm  0.0051823184\n",
      "iter  2675  loss  -0.3470269  grad l2 norm  0.0051856725\n",
      "iter  2676  loss  -0.34702718  grad l2 norm  0.0052045723\n",
      "iter  2677  loss  -0.34702736  grad l2 norm  0.005195938\n",
      "iter  2678  loss  -0.34702763  grad l2 norm  0.005203527\n",
      "iter  2679  loss  -0.34702778  grad l2 norm  0.0051870863\n",
      "iter  2680  loss  -0.34702814  grad l2 norm  0.0051914374\n",
      "iter  2681  loss  -0.3470282  grad l2 norm  0.0051764995\n",
      "iter  2682  loss  -0.3470285  grad l2 norm  0.005185488\n",
      "iter  2683  loss  -0.34702864  grad l2 norm  0.0051767076\n",
      "iter  2684  loss  -0.34702882  grad l2 norm  0.005191357\n",
      "iter  2685  loss  -0.34702897  grad l2 norm  0.0051874556\n",
      "iter  2686  loss  -0.34702924  grad l2 norm  0.005205606\n",
      "iter  2687  loss  -0.34702924  grad l2 norm  0.005204561\n",
      "iter  2688  loss  -0.34702936  grad l2 norm  0.005226252\n",
      "iter  2689  loss  -0.3470293  grad l2 norm  0.005226676\n",
      "iter  2690  loss  -0.3470296  grad l2 norm  0.0052333493\n",
      "iter  2691  loss  -0.3470299  grad l2 norm  0.0052042766\n",
      "iter  2692  loss  -0.34703016  grad l2 norm  0.0051923874\n",
      "iter  2693  loss  -0.34703046  grad l2 norm  0.005166984\n",
      "iter  2694  loss  -0.34703073  grad l2 norm  0.005170828\n",
      "iter  2695  loss  -0.34703082  grad l2 norm  0.0051615057\n",
      "iter  2696  loss  -0.34703115  grad l2 norm  0.005179024\n",
      "iter  2697  loss  -0.34703112  grad l2 norm  0.005180173\n",
      "iter  2698  loss  -0.34703135  grad l2 norm  0.0052017653\n",
      "iter  2699  loss  -0.3470314  grad l2 norm  0.0051996075\n",
      "iter  2700  loss  -0.3470316  grad l2 norm  0.005212372\n",
      "iter  2701  loss  -0.34703168  grad l2 norm  0.005200329\n",
      "iter  2702  loss  -0.34703198  grad l2 norm  0.005205546\n",
      "iter  2703  loss  -0.3470322  grad l2 norm  0.0051899543\n",
      "iter  2704  loss  -0.3470325  grad l2 norm  0.005195284\n",
      "iter  2705  loss  -0.34703267  grad l2 norm  0.0051833093\n",
      "iter  2706  loss  -0.347033  grad l2 norm  0.0051942067\n",
      "iter  2707  loss  -0.34703296  grad l2 norm  0.005188743\n",
      "iter  2708  loss  -0.34703332  grad l2 norm  0.0052061514\n",
      "iter  2709  loss  -0.3470333  grad l2 norm  0.005207665\n",
      "iter  2710  loss  -0.34703326  grad l2 norm  0.005232667\n",
      "iter  2711  loss  -0.3470332  grad l2 norm  0.0052389186\n",
      "iter  2712  loss  -0.3470333  grad l2 norm  0.0052517946\n",
      "iter  2713  loss  -0.34703365  grad l2 norm  0.0052248673\n",
      "iter  2714  loss  -0.3470341  grad l2 norm  0.0052056136\n",
      "iter  2715  loss  -0.34703434  grad l2 norm  0.0051689246\n",
      "iter  2716  loss  -0.34703478  grad l2 norm  0.0051639974\n",
      "iter  2717  loss  -0.34703484  grad l2 norm  0.0051534674\n",
      "iter  2718  loss  -0.34703502  grad l2 norm  0.0051744683\n",
      "iter  2719  loss  -0.347035  grad l2 norm  0.0051822765\n",
      "iter  2720  loss  -0.34703514  grad l2 norm  0.005209504\n",
      "iter  2721  loss  -0.34703514  grad l2 norm  0.005212361\n",
      "iter  2722  loss  -0.34703547  grad l2 norm  0.005226995\n",
      "iter  2723  loss  -0.3470356  grad l2 norm  0.0052152867\n",
      "iter  2724  loss  -0.34703586  grad l2 norm  0.005217899\n",
      "iter  2725  loss  -0.347036  grad l2 norm  0.005199688\n",
      "iter  2726  loss  -0.34703627  grad l2 norm  0.0052016\n",
      "iter  2727  loss  -0.34703636  grad l2 norm  0.005187987\n",
      "iter  2728  loss  -0.34703672  grad l2 norm  0.005197759\n",
      "iter  2729  loss  -0.34703678  grad l2 norm  0.0051935734\n",
      "iter  2730  loss  -0.34703687  grad l2 norm  0.005212741\n",
      "iter  2731  loss  -0.34703687  grad l2 norm  0.0052173682\n",
      "iter  2732  loss  -0.34703693  grad l2 norm  0.0052430844\n",
      "iter  2733  loss  -0.34703687  grad l2 norm  0.00524756\n",
      "iter  2734  loss  -0.34703708  grad l2 norm  0.0052560926\n",
      "iter  2735  loss  -0.34703735  grad l2 norm  0.0052254885\n",
      "iter  2736  loss  -0.34703785  grad l2 norm  0.0052038883\n",
      "iter  2737  loss  -0.34703815  grad l2 norm  0.0051687066\n",
      "iter  2738  loss  -0.34703845  grad l2 norm  0.005166402\n",
      "iter  2739  loss  -0.34703842  grad l2 norm  0.005160587\n",
      "iter  2740  loss  -0.34703866  grad l2 norm  0.0051845256\n",
      "iter  2741  loss  -0.34703857  grad l2 norm  0.0051954705\n",
      "iter  2742  loss  -0.34703878  grad l2 norm  0.00522369\n",
      "iter  2743  loss  -0.34703872  grad l2 norm  0.0052264915\n",
      "iter  2744  loss  -0.34703892  grad l2 norm  0.0052386606\n",
      "iter  2745  loss  -0.34703907  grad l2 norm  0.0052242577\n",
      "iter  2746  loss  -0.3470394  grad l2 norm  0.005223277\n",
      "iter  2747  loss  -0.34703958  grad l2 norm  0.0052034426\n",
      "iter  2748  loss  -0.34703985  grad l2 norm  0.0052046264\n",
      "iter  2749  loss  -0.3470399  grad l2 norm  0.005192928\n",
      "iter  2750  loss  -0.34704018  grad l2 norm  0.005204893\n",
      "iter  2751  loss  -0.34704018  grad l2 norm  0.005204173\n",
      "iter  2752  loss  -0.3470404  grad l2 norm  0.005222895\n",
      "iter  2753  loss  -0.34704044  grad l2 norm  0.0052221394\n",
      "iter  2754  loss  -0.34704065  grad l2 norm  0.0052383323\n",
      "iter  2755  loss  -0.3470405  grad l2 norm  0.005238722\n",
      "iter  2756  loss  -0.34704068  grad l2 norm  0.0052524484\n",
      "iter  2757  loss  -0.34704086  grad l2 norm  0.00523256\n",
      "iter  2758  loss  -0.3470413  grad l2 norm  0.005218711\n",
      "iter  2759  loss  -0.34704143  grad l2 norm  0.0051876074\n",
      "iter  2760  loss  -0.34704182  grad l2 norm  0.005184877\n",
      "iter  2761  loss  -0.3470419  grad l2 norm  0.005176921\n",
      "iter  2762  loss  -0.34704208  grad l2 norm  0.005196275\n",
      "iter  2763  loss  -0.3470421  grad l2 norm  0.0052029467\n",
      "iter  2764  loss  -0.34704226  grad l2 norm  0.0052267974\n",
      "iter  2765  loss  -0.3470423  grad l2 norm  0.0052279127\n",
      "iter  2766  loss  -0.34704247  grad l2 norm  0.0052392515\n",
      "iter  2767  loss  -0.34704256  grad l2 norm  0.0052265\n",
      "iter  2768  loss  -0.34704283  grad l2 norm  0.00522768\n",
      "iter  2769  loss  -0.3470429  grad l2 norm  0.005211831\n",
      "iter  2770  loss  -0.34704316  grad l2 norm  0.005216578\n",
      "iter  2771  loss  -0.3470432  grad l2 norm  0.005207766\n",
      "iter  2772  loss  -0.34704348  grad l2 norm  0.0052173077\n",
      "iter  2773  loss  -0.3470436  grad l2 norm  0.005209115\n",
      "iter  2774  loss  -0.347044  grad l2 norm  0.0052189175\n",
      "iter  2775  loss  -0.34704396  grad l2 norm  0.0052147703\n",
      "iter  2776  loss  -0.3470441  grad l2 norm  0.0052328957\n",
      "iter  2777  loss  -0.34704405  grad l2 norm  0.005239169\n",
      "iter  2778  loss  -0.34704408  grad l2 norm  0.005261324\n",
      "iter  2779  loss  -0.3470442  grad l2 norm  0.0052511124\n",
      "iter  2780  loss  -0.3470446  grad l2 norm  0.005240468\n",
      "iter  2781  loss  -0.34704483  grad l2 norm  0.005207863\n",
      "iter  2782  loss  -0.34704518  grad l2 norm  0.005199301\n",
      "iter  2783  loss  -0.3470453  grad l2 norm  0.0051851952\n",
      "iter  2784  loss  -0.34704548  grad l2 norm  0.005198281\n",
      "iter  2785  loss  -0.34704542  grad l2 norm  0.005201558\n",
      "iter  2786  loss  -0.34704566  grad l2 norm  0.0052241352\n",
      "iter  2787  loss  -0.34704563  grad l2 norm  0.0052282847\n",
      "iter  2788  loss  -0.34704578  grad l2 norm  0.0052440288\n",
      "iter  2789  loss  -0.34704584  grad l2 norm  0.0052375603\n",
      "iter  2790  loss  -0.34704605  grad l2 norm  0.0052436227\n",
      "iter  2791  loss  -0.3470461  grad l2 norm  0.0052300603\n",
      "iter  2792  loss  -0.34704632  grad l2 norm  0.005229212\n",
      "iter  2793  loss  -0.3470466  grad l2 norm  0.005210583\n",
      "iter  2794  loss  -0.3470469  grad l2 norm  0.0052116974\n",
      "iter  2795  loss  -0.34704697  grad l2 norm  0.005201827\n",
      "iter  2796  loss  -0.34704727  grad l2 norm  0.0052148323\n",
      "iter  2797  loss  -0.34704724  grad l2 norm  0.005216652\n",
      "iter  2798  loss  -0.34704742  grad l2 norm  0.0052391826\n",
      "iter  2799  loss  -0.3470473  grad l2 norm  0.005248948\n",
      "iter  2800  loss  -0.34704733  grad l2 norm  0.005274324\n",
      "iter  2801  loss  -0.34704727  grad l2 norm  0.005269882\n",
      "iter  2802  loss  -0.34704775  grad l2 norm  0.005260584\n",
      "iter  2803  loss  -0.347048  grad l2 norm  0.0052246824\n",
      "iter  2804  loss  -0.3470484  grad l2 norm  0.005208871\n",
      "iter  2805  loss  -0.34704858  grad l2 norm  0.0051885378\n",
      "iter  2806  loss  -0.3470488  grad l2 norm  0.005197078\n",
      "iter  2807  loss  -0.34704876  grad l2 norm  0.0051999716\n",
      "iter  2808  loss  -0.34704882  grad l2 norm  0.005225294\n",
      "iter  2809  loss  -0.34704876  grad l2 norm  0.0052353297\n",
      "iter  2810  loss  -0.34704888  grad l2 norm  0.0052569183\n",
      "iter  2811  loss  -0.34704888  grad l2 norm  0.0052530994\n",
      "iter  2812  loss  -0.34704915  grad l2 norm  0.005253609\n",
      "iter  2813  loss  -0.3470493  grad l2 norm  0.005231276\n",
      "iter  2814  loss  -0.3470497  grad l2 norm  0.005224984\n",
      "iter  2815  loss  -0.3470498  grad l2 norm  0.0052070664\n",
      "iter  2816  loss  -0.3470501  grad l2 norm  0.0052118334\n",
      "iter  2817  loss  -0.34705013  grad l2 norm  0.0052074376\n",
      "iter  2818  loss  -0.34705034  grad l2 norm  0.005224287\n",
      "iter  2819  loss  -0.34705037  grad l2 norm  0.0052289795\n",
      "iter  2820  loss  -0.34705052  grad l2 norm  0.005251076\n",
      "iter  2821  loss  -0.34705046  grad l2 norm  0.005258873\n",
      "iter  2822  loss  -0.34705046  grad l2 norm  0.005282121\n",
      "iter  2823  loss  -0.34705037  grad l2 norm  0.0052802367\n",
      "iter  2824  loss  -0.34705082  grad l2 norm  0.0052726595\n",
      "iter  2825  loss  -0.34705108  grad l2 norm  0.0052357935\n",
      "iter  2826  loss  -0.3470515  grad l2 norm  0.005216379\n",
      "iter  2827  loss  -0.34705168  grad l2 norm  0.0051938184\n",
      "iter  2828  loss  -0.34705183  grad l2 norm  0.0052021407\n",
      "iter  2829  loss  -0.34705183  grad l2 norm  0.0052075265\n",
      "iter  2830  loss  -0.3470518  grad l2 norm  0.0052349116\n",
      "iter  2831  loss  -0.34705174  grad l2 norm  0.005244394\n",
      "iter  2832  loss  -0.34705195  grad l2 norm  0.00525943\n",
      "iter  2833  loss  -0.347052  grad l2 norm  0.0052494826\n",
      "iter  2834  loss  -0.34705234  grad l2 norm  0.0052489997\n",
      "iter  2835  loss  -0.3470525  grad l2 norm  0.005231041\n",
      "iter  2836  loss  -0.34705278  grad l2 norm  0.0052302973\n",
      "iter  2837  loss  -0.34705284  grad l2 norm  0.005218\n",
      "iter  2838  loss  -0.34705308  grad l2 norm  0.0052257166\n",
      "iter  2839  loss  -0.34705314  grad l2 norm  0.00522279\n",
      "iter  2840  loss  -0.34705338  grad l2 norm  0.0052379593\n",
      "iter  2841  loss  -0.34705335  grad l2 norm  0.005239467\n",
      "iter  2842  loss  -0.3470536  grad l2 norm  0.00525655\n",
      "iter  2843  loss  -0.3470535  grad l2 norm  0.00525977\n",
      "iter  2844  loss  -0.34705365  grad l2 norm  0.0052793445\n",
      "iter  2845  loss  -0.3470535  grad l2 norm  0.005281559\n",
      "iter  2846  loss  -0.3470537  grad l2 norm  0.0052822023\n",
      "iter  2847  loss  -0.34705403  grad l2 norm  0.0052517266\n",
      "iter  2848  loss  -0.34705442  grad l2 norm  0.005234126\n",
      "iter  2849  loss  -0.34705448  grad l2 norm  0.0052116513\n",
      "iter  2850  loss  -0.34705466  grad l2 norm  0.0052168164\n",
      "iter  2851  loss  -0.3470547  grad l2 norm  0.0052149054\n",
      "iter  2852  loss  -0.34705487  grad l2 norm  0.005230513\n",
      "iter  2853  loss  -0.3470549  grad l2 norm  0.005232764\n",
      "iter  2854  loss  -0.34705508  grad l2 norm  0.005249126\n",
      "iter  2855  loss  -0.34705514  grad l2 norm  0.0052470663\n",
      "iter  2856  loss  -0.34705535  grad l2 norm  0.005255576\n",
      "iter  2857  loss  -0.34705535  grad l2 norm  0.005245586\n",
      "iter  2858  loss  -0.34705558  grad l2 norm  0.005248385\n",
      "iter  2859  loss  -0.34705567  grad l2 norm  0.005236953\n",
      "iter  2860  loss  -0.347056  grad l2 norm  0.0052418564\n",
      "iter  2861  loss  -0.347056  grad l2 norm  0.005234765\n",
      "iter  2862  loss  -0.3470563  grad l2 norm  0.005244064\n",
      "iter  2863  loss  -0.3470564  grad l2 norm  0.005240979\n",
      "iter  2864  loss  -0.3470566  grad l2 norm  0.005254192\n",
      "iter  2865  loss  -0.3470566  grad l2 norm  0.005255897\n",
      "iter  2866  loss  -0.34705663  grad l2 norm  0.0052758534\n",
      "iter  2867  loss  -0.34705642  grad l2 norm  0.0052847783\n",
      "iter  2868  loss  -0.34705654  grad l2 norm  0.0052982545\n",
      "iter  2869  loss  -0.34705663  grad l2 norm  0.005278444\n",
      "iter  2870  loss  -0.34705696  grad l2 norm  0.005261395\n",
      "iter  2871  loss  -0.34705728  grad l2 norm  0.0052284603\n",
      "iter  2872  loss  -0.3470577  grad l2 norm  0.005216946\n",
      "iter  2873  loss  -0.34705776  grad l2 norm  0.005203435\n",
      "iter  2874  loss  -0.347058  grad l2 norm  0.00521663\n",
      "iter  2875  loss  -0.34705788  grad l2 norm  0.0052244067\n",
      "iter  2876  loss  -0.34705797  grad l2 norm  0.0052494556\n",
      "iter  2877  loss  -0.34705788  grad l2 norm  0.0052572675\n",
      "iter  2878  loss  -0.34705812  grad l2 norm  0.005272152\n",
      "iter  2879  loss  -0.3470581  grad l2 norm  0.0052655954\n",
      "iter  2880  loss  -0.34705845  grad l2 norm  0.005267172\n",
      "iter  2881  loss  -0.3470585  grad l2 norm  0.005252151\n",
      "iter  2882  loss  -0.34705883  grad l2 norm  0.005251215\n",
      "iter  2883  loss  -0.3470589  grad l2 norm  0.0052386904\n",
      "iter  2884  loss  -0.34705922  grad l2 norm  0.00524362\n",
      "iter  2885  loss  -0.34705925  grad l2 norm  0.0052395207\n",
      "iter  2886  loss  -0.3470594  grad l2 norm  0.0052534277\n",
      "iter  2887  loss  -0.34705946  grad l2 norm  0.0052581755\n",
      "iter  2888  loss  -0.3470594  grad l2 norm  0.0052813683\n",
      "iter  2889  loss  -0.3470592  grad l2 norm  0.0052949274\n",
      "iter  2890  loss  -0.34705913  grad l2 norm  0.0053148638\n",
      "iter  2891  loss  -0.34705934  grad l2 norm  0.005296634\n",
      "iter  2892  loss  -0.34705997  grad l2 norm  0.0052711903\n",
      "iter  2893  loss  -0.3470602  grad l2 norm  0.0052295807\n",
      "iter  2894  loss  -0.34706062  grad l2 norm  0.005214086\n",
      "iter  2895  loss  -0.34706062  grad l2 norm  0.005201814\n",
      "iter  2896  loss  -0.34706077  grad l2 norm  0.0052178986\n",
      "iter  2897  loss  -0.34706068  grad l2 norm  0.0052298424\n",
      "iter  2898  loss  -0.34706083  grad l2 norm  0.0052585253\n",
      "iter  2899  loss  -0.3470607  grad l2 norm  0.0052699787\n",
      "iter  2900  loss  -0.34706092  grad l2 norm  0.005286353\n",
      "iter  2901  loss  -0.34706098  grad l2 norm  0.005279437\n",
      "iter  2902  loss  -0.34706125  grad l2 norm  0.0052782428\n",
      "iter  2903  loss  -0.34706137  grad l2 norm  0.00525992\n",
      "iter  2904  loss  -0.34706166  grad l2 norm  0.005255211\n",
      "iter  2905  loss  -0.34706178  grad l2 norm  0.005241351\n",
      "iter  2906  loss  -0.347062  grad l2 norm  0.005246412\n",
      "iter  2907  loss  -0.34706205  grad l2 norm  0.0052446965\n",
      "iter  2908  loss  -0.34706208  grad l2 norm  0.0052617406\n",
      "iter  2909  loss  -0.34706205  grad l2 norm  0.0052695693\n",
      "iter  2910  loss  -0.3470621  grad l2 norm  0.005290114\n",
      "iter  2911  loss  -0.34706205  grad l2 norm  0.005294296\n",
      "iter  2912  loss  -0.3470622  grad l2 norm  0.005307972\n",
      "iter  2913  loss  -0.34706217  grad l2 norm  0.005296408\n",
      "iter  2914  loss  -0.34706265  grad l2 norm  0.005279715\n",
      "iter  2915  loss  -0.3470629  grad l2 norm  0.0052436856\n",
      "iter  2916  loss  -0.3470633  grad l2 norm  0.0052287015\n",
      "iter  2917  loss  -0.34706336  grad l2 norm  0.0052154036\n",
      "iter  2918  loss  -0.3470635  grad l2 norm  0.0052288\n",
      "iter  2919  loss  -0.34706345  grad l2 norm  0.005238532\n",
      "iter  2920  loss  -0.34706357  grad l2 norm  0.0052646566\n",
      "iter  2921  loss  -0.34706348  grad l2 norm  0.005275136\n",
      "iter  2922  loss  -0.3470637  grad l2 norm  0.0052908086\n",
      "iter  2923  loss  -0.34706375  grad l2 norm  0.005284318\n",
      "iter  2924  loss  -0.347064  grad l2 norm  0.0052832942\n",
      "iter  2925  loss  -0.34706408  grad l2 norm  0.005266283\n",
      "iter  2926  loss  -0.3470644  grad l2 norm  0.0052631693\n",
      "iter  2927  loss  -0.34706444  grad l2 norm  0.005251799\n",
      "iter  2928  loss  -0.34706467  grad l2 norm  0.0052590272\n",
      "iter  2929  loss  -0.34706458  grad l2 norm  0.005259292\n",
      "iter  2930  loss  -0.34706482  grad l2 norm  0.0052724476\n",
      "iter  2931  loss  -0.34706485  grad l2 norm  0.005269957\n",
      "iter  2932  loss  -0.3470651  grad l2 norm  0.0052789715\n",
      "iter  2933  loss  -0.34706506  grad l2 norm  0.005279219\n",
      "iter  2934  loss  -0.34706506  grad l2 norm  0.0052971304\n",
      "iter  2935  loss  -0.34706494  grad l2 norm  0.0053011766\n",
      "iter  2936  loss  -0.3470653  grad l2 norm  0.005300686\n",
      "iter  2937  loss  -0.34706542  grad l2 norm  0.0052722166\n",
      "iter  2938  loss  -0.34706587  grad l2 norm  0.005255193\n",
      "iter  2939  loss  -0.347066  grad l2 norm  0.005235252\n",
      "iter  2940  loss  -0.34706622  grad l2 norm  0.005239983\n",
      "iter  2941  loss  -0.34706625  grad l2 norm  0.005242156\n",
      "iter  2942  loss  -0.3470664  grad l2 norm  0.0052623306\n",
      "iter  2943  loss  -0.34706634  grad l2 norm  0.005270866\n",
      "iter  2944  loss  -0.3470664  grad l2 norm  0.0052878717\n",
      "iter  2945  loss  -0.34706652  grad l2 norm  0.00528617\n",
      "iter  2946  loss  -0.34706673  grad l2 norm  0.005290693\n",
      "iter  2947  loss  -0.3470667  grad l2 norm  0.0052797752\n",
      "iter  2948  loss  -0.34706688  grad l2 norm  0.005281041\n",
      "iter  2949  loss  -0.3470669  grad l2 norm  0.0052716965\n",
      "iter  2950  loss  -0.34706715  grad l2 norm  0.0052735563\n",
      "iter  2951  loss  -0.3470673  grad l2 norm  0.005261448\n",
      "iter  2952  loss  -0.34706765  grad l2 norm  0.00526303\n",
      "iter  2953  loss  -0.34706768  grad l2 norm  0.0052571213\n",
      "iter  2954  loss  -0.34706795  grad l2 norm  0.0052692364\n",
      "iter  2955  loss  -0.3470679  grad l2 norm  0.0052754907\n",
      "iter  2956  loss  -0.34706783  grad l2 norm  0.005299319\n",
      "iter  2957  loss  -0.34706774  grad l2 norm  0.0053131757\n",
      "iter  2958  loss  -0.34706783  grad l2 norm  0.005325784\n",
      "iter  2959  loss  -0.34706804  grad l2 norm  0.0053039286\n",
      "iter  2960  loss  -0.34706843  grad l2 norm  0.005282576\n",
      "iter  2961  loss  -0.3470687  grad l2 norm  0.0052522854\n",
      "iter  2962  loss  -0.34706897  grad l2 norm  0.0052453736\n",
      "iter  2963  loss  -0.34706905  grad l2 norm  0.0052386234\n",
      "iter  2964  loss  -0.34706917  grad l2 norm  0.0052541625\n",
      "iter  2965  loss  -0.34706908  grad l2 norm  0.0052639893\n",
      "iter  2966  loss  -0.34706926  grad l2 norm  0.0052865413\n",
      "iter  2967  loss  -0.34706914  grad l2 norm  0.005293812\n",
      "iter  2968  loss  -0.3470692  grad l2 norm  0.005307053\n",
      "iter  2969  loss  -0.3470692  grad l2 norm  0.005301542\n",
      "iter  2970  loss  -0.34706938  grad l2 norm  0.0052987477\n",
      "iter  2971  loss  -0.34706962  grad l2 norm  0.005277869\n",
      "iter  2972  loss  -0.34706998  grad l2 norm  0.005268679\n",
      "iter  2973  loss  -0.34707016  grad l2 norm  0.005253342\n",
      "iter  2974  loss  -0.34707043  grad l2 norm  0.005257584\n",
      "iter  2975  loss  -0.34707037  grad l2 norm  0.005257526\n",
      "iter  2976  loss  -0.3470706  grad l2 norm  0.0052745757\n",
      "iter  2977  loss  -0.34707054  grad l2 norm  0.005283958\n",
      "iter  2978  loss  -0.3470706  grad l2 norm  0.005307334\n",
      "iter  2979  loss  -0.34707046  grad l2 norm  0.0053211236\n",
      "iter  2980  loss  -0.34707052  grad l2 norm  0.005340268\n",
      "iter  2981  loss  -0.3470706  grad l2 norm  0.005326673\n",
      "iter  2982  loss  -0.3470711  grad l2 norm  0.005304553\n",
      "iter  2983  loss  -0.34707138  grad l2 norm  0.005267125\n",
      "iter  2984  loss  -0.34707174  grad l2 norm  0.0052509713\n",
      "iter  2985  loss  -0.34707183  grad l2 norm  0.005238338\n",
      "iter  2986  loss  -0.34707198  grad l2 norm  0.005251415\n",
      "iter  2987  loss  -0.3470718  grad l2 norm  0.005263587\n",
      "iter  2988  loss  -0.34707186  grad l2 norm  0.005291987\n",
      "iter  2989  loss  -0.34707174  grad l2 norm  0.0053060665\n",
      "iter  2990  loss  -0.34707192  grad l2 norm  0.005318736\n",
      "iter  2991  loss  -0.34707204  grad l2 norm  0.0053060357\n",
      "iter  2992  loss  -0.34707236  grad l2 norm  0.005296792\n",
      "iter  2993  loss  -0.34707257  grad l2 norm  0.0052754087\n",
      "iter  2994  loss  -0.34707287  grad l2 norm  0.0052697365\n",
      "iter  2995  loss  -0.34707287  grad l2 norm  0.0052594612\n",
      "iter  2996  loss  -0.3470731  grad l2 norm  0.005266995\n",
      "iter  2997  loss  -0.3470731  grad l2 norm  0.0052689756\n",
      "iter  2998  loss  -0.3470733  grad l2 norm  0.0052856226\n",
      "iter  2999  loss  -0.34707332  grad l2 norm  0.005292964\n",
      "iter  3000  loss  -0.3470735  grad l2 norm  0.0053119455\n",
      "iter  3001  loss  -0.3470733  grad l2 norm  0.0053207544\n",
      "iter  3002  loss  -0.34707338  grad l2 norm  0.0053400453\n",
      "iter  3003  loss  -0.34707338  grad l2 norm  0.0053373645\n",
      "iter  3004  loss  -0.34707373  grad l2 norm  0.005323407\n",
      "iter  3005  loss  -0.34707403  grad l2 norm  0.0052867336\n",
      "iter  3006  loss  -0.3470744  grad l2 norm  0.005266329\n",
      "iter  3007  loss  -0.3470745  grad l2 norm  0.0052500768\n",
      "iter  3008  loss  -0.34707466  grad l2 norm  0.0052609597\n",
      "iter  3009  loss  -0.34707448  grad l2 norm  0.0052713677\n",
      "iter  3010  loss  -0.34707463  grad l2 norm  0.0052931216\n",
      "iter  3011  loss  -0.3470746  grad l2 norm  0.0052979863\n",
      "iter  3012  loss  -0.34707484  grad l2 norm  0.005307408\n",
      "iter  3013  loss  -0.34707493  grad l2 norm  0.0052995677\n",
      "iter  3014  loss  -0.34707516  grad l2 norm  0.005299191\n",
      "iter  3015  loss  -0.34707525  grad l2 norm  0.005286805\n",
      "iter  3016  loss  -0.3470755  grad l2 norm  0.0052863597\n",
      "iter  3017  loss  -0.34707558  grad l2 norm  0.0052779703\n",
      "iter  3018  loss  -0.3470759  grad l2 norm  0.0052839387\n",
      "iter  3019  loss  -0.34707594  grad l2 norm  0.0052821864\n",
      "iter  3020  loss  -0.34707618  grad l2 norm  0.005293507\n",
      "iter  3021  loss  -0.34707618  grad l2 norm  0.0052953167\n",
      "iter  3022  loss  -0.34707627  grad l2 norm  0.0053086355\n",
      "iter  3023  loss  -0.34707633  grad l2 norm  0.005313075\n",
      "iter  3024  loss  -0.34707636  grad l2 norm  0.0053312383\n",
      "iter  3025  loss  -0.3470762  grad l2 norm  0.005339114\n",
      "iter  3026  loss  -0.34707648  grad l2 norm  0.005342862\n",
      "iter  3027  loss  -0.34707665  grad l2 norm  0.00531744\n",
      "iter  3028  loss  -0.347077  grad l2 norm  0.005298116\n",
      "iter  3029  loss  -0.3470771  grad l2 norm  0.005275242\n",
      "iter  3030  loss  -0.34707734  grad l2 norm  0.005270769\n",
      "iter  3031  loss  -0.3470775  grad l2 norm  0.005262295\n",
      "iter  3032  loss  -0.34707773  grad l2 norm  0.00527235\n",
      "iter  3033  loss  -0.34707767  grad l2 norm  0.0052781706\n",
      "iter  3034  loss  -0.34707788  grad l2 norm  0.005297464\n",
      "iter  3035  loss  -0.34707776  grad l2 norm  0.005303487\n",
      "iter  3036  loss  -0.34707797  grad l2 norm  0.005314525\n",
      "iter  3037  loss  -0.347078  grad l2 norm  0.005308828\n",
      "iter  3038  loss  -0.34707817  grad l2 norm  0.005309659\n",
      "iter  3039  loss  -0.34707835  grad l2 norm  0.005298459\n",
      "iter  3040  loss  -0.34707862  grad l2 norm  0.005298558\n",
      "iter  3041  loss  -0.3470787  grad l2 norm  0.0052901576\n",
      "iter  3042  loss  -0.34707895  grad l2 norm  0.005294682\n",
      "iter  3043  loss  -0.34707907  grad l2 norm  0.0052916594\n",
      "iter  3044  loss  -0.3470793  grad l2 norm  0.005302432\n",
      "iter  3045  loss  -0.3470792  grad l2 norm  0.0053065824\n",
      "iter  3046  loss  -0.34707937  grad l2 norm  0.0053262874\n",
      "iter  3047  loss  -0.34707904  grad l2 norm  0.0053412938\n",
      "iter  3048  loss  -0.3470791  grad l2 norm  0.005363268\n",
      "iter  3049  loss  -0.3470791  grad l2 norm  0.005353362\n",
      "iter  3050  loss  -0.34707955  grad l2 norm  0.005330157\n",
      "iter  3051  loss  -0.34707993  grad l2 norm  0.00528799\n",
      "iter  3052  loss  -0.34708038  grad l2 norm  0.0052655847\n",
      "iter  3053  loss  -0.3470805  grad l2 norm  0.0052490057\n",
      "iter  3054  loss  -0.3470807  grad l2 norm  0.0052592377\n",
      "iter  3055  loss  -0.34708065  grad l2 norm  0.0052708914\n",
      "iter  3056  loss  -0.34708074  grad l2 norm  0.005298491\n",
      "iter  3057  loss  -0.34708062  grad l2 norm  0.005313713\n",
      "iter  3058  loss  -0.3470808  grad l2 norm  0.00533164\n",
      "iter  3059  loss  -0.34708074  grad l2 norm  0.0053294534\n",
      "iter  3060  loss  -0.34708107  grad l2 norm  0.0053290105\n",
      "iter  3061  loss  -0.34708112  grad l2 norm  0.005313288\n",
      "iter  3062  loss  -0.34708154  grad l2 norm  0.005306988\n",
      "iter  3063  loss  -0.34708157  grad l2 norm  0.0052926852\n",
      "iter  3064  loss  -0.34708193  grad l2 norm  0.0052933698\n",
      "iter  3065  loss  -0.34708196  grad l2 norm  0.0052897227\n",
      "iter  3066  loss  -0.34708217  grad l2 norm  0.005302046\n",
      "iter  3067  loss  -0.3470821  grad l2 norm  0.005309785\n",
      "iter  3068  loss  -0.34708208  grad l2 norm  0.00533227\n",
      "iter  3069  loss  -0.3470819  grad l2 norm  0.0053472845\n",
      "iter  3070  loss  -0.3470819  grad l2 norm  0.00536816\n",
      "iter  3071  loss  -0.34708193  grad l2 norm  0.0053623696\n",
      "iter  3072  loss  -0.3470824  grad l2 norm  0.005342098\n",
      "iter  3073  loss  -0.3470828  grad l2 norm  0.005299777\n",
      "iter  3074  loss  -0.34708315  grad l2 norm  0.0052749\n",
      "iter  3075  loss  -0.34708333  grad l2 norm  0.005255838\n",
      "iter  3076  loss  -0.3470835  grad l2 norm  0.005264064\n",
      "iter  3077  loss  -0.34708348  grad l2 norm  0.005274673\n",
      "iter  3078  loss  -0.34708348  grad l2 norm  0.0053024148\n",
      "iter  3079  loss  -0.34708342  grad l2 norm  0.005319481\n",
      "iter  3080  loss  -0.3470836  grad l2 norm  0.0053399308\n",
      "iter  3081  loss  -0.3470836  grad l2 norm  0.005339602\n",
      "iter  3082  loss  -0.34708393  grad l2 norm  0.005339286\n",
      "iter  3083  loss  -0.3470839  grad l2 norm  0.0053222096\n",
      "iter  3084  loss  -0.34708425  grad l2 norm  0.005314063\n",
      "iter  3085  loss  -0.34708434  grad l2 norm  0.005298793\n",
      "iter  3086  loss  -0.34708458  grad l2 norm  0.0052998797\n",
      "iter  3087  loss  -0.3470846  grad l2 norm  0.0052981055\n",
      "iter  3088  loss  -0.3470848  grad l2 norm  0.00531293\n",
      "iter  3089  loss  -0.34708476  grad l2 norm  0.005320613\n",
      "iter  3090  loss  -0.34708485  grad l2 norm  0.0053347843\n",
      "iter  3091  loss  -0.34708488  grad l2 norm  0.0053349044\n",
      "iter  3092  loss  -0.34708503  grad l2 norm  0.0053467653\n",
      "iter  3093  loss  -0.34708485  grad l2 norm  0.0053510675\n",
      "iter  3094  loss  -0.34708512  grad l2 norm  0.005354342\n",
      "iter  3095  loss  -0.34708527  grad l2 norm  0.005328494\n",
      "iter  3096  loss  -0.3470857  grad l2 norm  0.005306517\n",
      "iter  3097  loss  -0.3470859  grad l2 norm  0.005281276\n",
      "iter  3098  loss  -0.3470862  grad l2 norm  0.0052802875\n",
      "iter  3099  loss  -0.34708622  grad l2 norm  0.005281836\n",
      "iter  3100  loss  -0.3470863  grad l2 norm  0.005301995\n",
      "iter  3101  loss  -0.34708625  grad l2 norm  0.0053149113\n",
      "iter  3102  loss  -0.34708643  grad l2 norm  0.0053351778\n",
      "iter  3103  loss  -0.3470863  grad l2 norm  0.0053380076\n",
      "iter  3104  loss  -0.34708652  grad l2 norm  0.005342478\n",
      "iter  3105  loss  -0.34708664  grad l2 norm  0.0053309863\n",
      "iter  3106  loss  -0.34708688  grad l2 norm  0.0053275595\n",
      "iter  3107  loss  -0.34708688  grad l2 norm  0.005315987\n",
      "iter  3108  loss  -0.34708712  grad l2 norm  0.005318943\n",
      "iter  3109  loss  -0.34708712  grad l2 norm  0.0053157727\n",
      "iter  3110  loss  -0.34708732  grad l2 norm  0.0053208843\n",
      "iter  3111  loss  -0.3470875  grad l2 norm  0.0053133764\n",
      "iter  3112  loss  -0.34708774  grad l2 norm  0.0053177634\n",
      "iter  3113  loss  -0.34708774  grad l2 norm  0.005316763\n",
      "iter  3114  loss  -0.3470879  grad l2 norm  0.0053326446\n",
      "iter  3115  loss  -0.34708765  grad l2 norm  0.005345523\n",
      "iter  3116  loss  -0.34708768  grad l2 norm  0.0053686346\n",
      "iter  3117  loss  -0.34708768  grad l2 norm  0.005363299\n",
      "iter  3118  loss  -0.34708807  grad l2 norm  0.0053472584\n",
      "iter  3119  loss  -0.34708834  grad l2 norm  0.005314796\n",
      "iter  3120  loss  -0.34708875  grad l2 norm  0.0052998806\n",
      "iter  3121  loss  -0.34708887  grad l2 norm  0.0052871034\n",
      "iter  3122  loss  -0.34708902  grad l2 norm  0.00529572\n",
      "iter  3123  loss  -0.34708902  grad l2 norm  0.0053022704\n",
      "iter  3124  loss  -0.34708908  grad l2 norm  0.0053231115\n",
      "iter  3125  loss  -0.34708905  grad l2 norm  0.005332886\n",
      "iter  3126  loss  -0.3470891  grad l2 norm  0.0053477287\n",
      "iter  3127  loss  -0.34708917  grad l2 norm  0.00534662\n",
      "iter  3128  loss  -0.3470893  grad l2 norm  0.005350891\n",
      "iter  3129  loss  -0.34708932  grad l2 norm  0.0053411825\n",
      "iter  3130  loss  -0.3470895  grad l2 norm  0.0053353207\n",
      "iter  3131  loss  -0.34708968  grad l2 norm  0.005315805\n",
      "iter  3132  loss  -0.3470901  grad l2 norm  0.0053097266\n",
      "iter  3133  loss  -0.34709018  grad l2 norm  0.005300358\n",
      "iter  3134  loss  -0.3470904  grad l2 norm  0.005309134\n",
      "iter  3135  loss  -0.3470904  grad l2 norm  0.005314308\n",
      "iter  3136  loss  -0.34709048  grad l2 norm  0.005334809\n",
      "iter  3137  loss  -0.34709036  grad l2 norm  0.005348968\n",
      "iter  3138  loss  -0.34709033  grad l2 norm  0.005376036\n",
      "iter  3139  loss  -0.34709013  grad l2 norm  0.0053863013\n",
      "iter  3140  loss  -0.34709042  grad l2 norm  0.005382183\n",
      "iter  3141  loss  -0.34709072  grad l2 norm  0.0053473534\n",
      "iter  3142  loss  -0.3470912  grad l2 norm  0.0053198272\n",
      "iter  3143  loss  -0.34709132  grad l2 norm  0.005292456\n",
      "iter  3144  loss  -0.34709165  grad l2 norm  0.005290429\n",
      "iter  3145  loss  -0.34709162  grad l2 norm  0.0052919234\n",
      "iter  3146  loss  -0.3470917  grad l2 norm  0.0053142696\n",
      "iter  3147  loss  -0.3470915  grad l2 norm  0.0053315745\n",
      "iter  3148  loss  -0.34709153  grad l2 norm  0.0053578485\n",
      "iter  3149  loss  -0.34709135  grad l2 norm  0.0053653833\n",
      "iter  3150  loss  -0.34709167  grad l2 norm  0.0053675324\n",
      "iter  3151  loss  -0.3470918  grad l2 norm  0.0053470028\n",
      "iter  3152  loss  -0.34709212  grad l2 norm  0.0053330837\n",
      "iter  3153  loss  -0.34709233  grad l2 norm  0.0053123687\n",
      "iter  3154  loss  -0.3470925  grad l2 norm  0.005309616\n",
      "iter  3155  loss  -0.34709263  grad l2 norm  0.005304661\n",
      "iter  3156  loss  -0.3470928  grad l2 norm  0.0053166626\n",
      "iter  3157  loss  -0.34709275  grad l2 norm  0.0053231874\n",
      "iter  3158  loss  -0.34709293  grad l2 norm  0.0053426125\n",
      "iter  3159  loss  -0.3470928  grad l2 norm  0.005353128\n",
      "iter  3160  loss  -0.34709296  grad l2 norm  0.0053743157\n",
      "iter  3161  loss  -0.3470927  grad l2 norm  0.005386004\n",
      "iter  3162  loss  -0.34709278  grad l2 norm  0.005397158\n",
      "iter  3163  loss  -0.34709293  grad l2 norm  0.0053739096\n",
      "iter  3164  loss  -0.3470934  grad l2 norm  0.0053459364\n",
      "iter  3165  loss  -0.34709364  grad l2 norm  0.0053101615\n",
      "iter  3166  loss  -0.34709397  grad l2 norm  0.005299524\n",
      "iter  3167  loss  -0.34709385  grad l2 norm  0.0052961\n",
      "iter  3168  loss  -0.347094  grad l2 norm  0.005317202\n",
      "iter  3169  loss  -0.34709376  grad l2 norm  0.0053344904\n",
      "iter  3170  loss  -0.34709385  grad l2 norm  0.0053561013\n",
      "iter  3171  loss  -0.3470939  grad l2 norm  0.005356838\n",
      "iter  3172  loss  -0.3470942  grad l2 norm  0.005359193\n",
      "iter  3173  loss  -0.3470942  grad l2 norm  0.0053453986\n",
      "iter  3174  loss  -0.3470945  grad l2 norm  0.005340344\n",
      "iter  3175  loss  -0.34709457  grad l2 norm  0.005326585\n",
      "iter  3176  loss  -0.34709474  grad l2 norm  0.0053270073\n",
      "iter  3177  loss  -0.34709483  grad l2 norm  0.005321852\n",
      "iter  3178  loss  -0.3470951  grad l2 norm  0.0053312317\n",
      "iter  3179  loss  -0.34709498  grad l2 norm  0.005333548\n",
      "iter  3180  loss  -0.34709525  grad l2 norm  0.0053471304\n",
      "iter  3181  loss  -0.34709522  grad l2 norm  0.0053511728\n",
      "iter  3182  loss  -0.34709537  grad l2 norm  0.0053656795\n",
      "iter  3183  loss  -0.34709522  grad l2 norm  0.0053726584\n",
      "iter  3184  loss  -0.34709525  grad l2 norm  0.00539212\n",
      "iter  3185  loss  -0.34709516  grad l2 norm  0.0053921505\n",
      "iter  3186  loss  -0.34709543  grad l2 norm  0.0053800545\n",
      "iter  3187  loss  -0.3470957  grad l2 norm  0.005347101\n",
      "iter  3188  loss  -0.347096  grad l2 norm  0.0053303367\n",
      "iter  3189  loss  -0.34709597  grad l2 norm  0.0053157504\n",
      "iter  3190  loss  -0.34709615  grad l2 norm  0.0053183716\n",
      "iter  3191  loss  -0.34709632  grad l2 norm  0.005315412\n",
      "iter  3192  loss  -0.3470965  grad l2 norm  0.0053288257\n",
      "iter  3193  loss  -0.34709638  grad l2 norm  0.0053357966\n",
      "iter  3194  loss  -0.3470965  grad l2 norm  0.0053528273\n",
      "iter  3195  loss  -0.34709647  grad l2 norm  0.0053550624\n",
      "iter  3196  loss  -0.34709665  grad l2 norm  0.0053614867\n",
      "iter  3197  loss  -0.34709665  grad l2 norm  0.0053533483\n",
      "iter  3198  loss  -0.34709692  grad l2 norm  0.0053531188\n",
      "iter  3199  loss  -0.3470969  grad l2 norm  0.0053431266\n",
      "iter  3200  loss  -0.34709722  grad l2 norm  0.005344518\n",
      "iter  3201  loss  -0.3470973  grad l2 norm  0.0053381375\n",
      "iter  3202  loss  -0.34709755  grad l2 norm  0.005344402\n",
      "iter  3203  loss  -0.34709758  grad l2 norm  0.005343344\n",
      "iter  3204  loss  -0.3470977  grad l2 norm  0.0053553754\n",
      "iter  3205  loss  -0.3470976  grad l2 norm  0.0053616287\n",
      "iter  3206  loss  -0.34709767  grad l2 norm  0.0053838477\n",
      "iter  3207  loss  -0.34709734  grad l2 norm  0.0054004192\n",
      "iter  3208  loss  -0.34709728  grad l2 norm  0.0054153358\n",
      "iter  3209  loss  -0.34709746  grad l2 norm  0.0053945906\n",
      "iter  3210  loss  -0.3470979  grad l2 norm  0.0053652674\n",
      "iter  3211  loss  -0.3470983  grad l2 norm  0.0053239884\n",
      "iter  3212  loss  -0.34709862  grad l2 norm  0.0053065214\n",
      "iter  3213  loss  -0.34709874  grad l2 norm  0.005295858\n",
      "iter  3214  loss  -0.34709877  grad l2 norm  0.005311059\n",
      "iter  3215  loss  -0.34709865  grad l2 norm  0.0053263223\n",
      "iter  3216  loss  -0.34709874  grad l2 norm  0.0053548156\n",
      "iter  3217  loss  -0.34709856  grad l2 norm  0.0053686434\n",
      "iter  3218  loss  -0.3470988  grad l2 norm  0.0053834924\n",
      "iter  3219  loss  -0.34709877  grad l2 norm  0.0053781816\n",
      "iter  3220  loss  -0.34709898  grad l2 norm  0.005375008\n",
      "iter  3221  loss  -0.34709904  grad l2 norm  0.005358456\n",
      "iter  3222  loss  -0.34709936  grad l2 norm  0.005352082\n",
      "iter  3223  loss  -0.34709948  grad l2 norm  0.00533902\n",
      "iter  3224  loss  -0.34709975  grad l2 norm  0.005341256\n",
      "iter  3225  loss  -0.34709972  grad l2 norm  0.00533957\n",
      "iter  3226  loss  -0.3470998  grad l2 norm  0.005353716\n",
      "iter  3227  loss  -0.34709972  grad l2 norm  0.0053632804\n",
      "iter  3228  loss  -0.34709975  grad l2 norm  0.005387344\n",
      "iter  3229  loss  -0.34709942  grad l2 norm  0.005403475\n",
      "iter  3230  loss  -0.34709945  grad l2 norm  0.005422631\n",
      "iter  3231  loss  -0.34709954  grad l2 norm  0.005410789\n",
      "iter  3232  loss  -0.34710002  grad l2 norm  0.005384191\n",
      "iter  3233  loss  -0.34710032  grad l2 norm  0.005339776\n",
      "iter  3234  loss  -0.3471007  grad l2 norm  0.0053164386\n",
      "iter  3235  loss  -0.3471008  grad l2 norm  0.005301221\n",
      "iter  3236  loss  -0.3471009  grad l2 norm  0.0053133005\n",
      "iter  3237  loss  -0.34710076  grad l2 norm  0.0053270017\n",
      "iter  3238  loss  -0.34710088  grad l2 norm  0.005356357\n",
      "iter  3239  loss  -0.34710068  grad l2 norm  0.005373982\n",
      "iter  3240  loss  -0.34710085  grad l2 norm  0.0053932923\n",
      "iter  3241  loss  -0.34710085  grad l2 norm  0.0053905193\n",
      "iter  3242  loss  -0.3471011  grad l2 norm  0.005387508\n",
      "iter  3243  loss  -0.34710118  grad l2 norm  0.005368931\n",
      "iter  3244  loss  -0.34710145  grad l2 norm  0.0053603672\n",
      "iter  3245  loss  -0.34710145  grad l2 norm  0.005345993\n",
      "iter  3246  loss  -0.3471017  grad l2 norm  0.005348214\n",
      "iter  3247  loss  -0.3471017  grad l2 norm  0.005348096\n",
      "iter  3248  loss  -0.34710175  grad l2 norm  0.005364293\n",
      "iter  3249  loss  -0.34710172  grad l2 norm  0.0053723427\n",
      "iter  3250  loss  -0.3471019  grad l2 norm  0.005384547\n",
      "iter  3251  loss  -0.34710187  grad l2 norm  0.00538341\n",
      "iter  3252  loss  -0.34710193  grad l2 norm  0.0053956746\n",
      "iter  3253  loss  -0.34710184  grad l2 norm  0.0054020616\n",
      "iter  3254  loss  -0.34710193  grad l2 norm  0.005406117\n",
      "iter  3255  loss  -0.34710208  grad l2 norm  0.005379296\n",
      "iter  3256  loss  -0.34710255  grad l2 norm  0.005355894\n",
      "iter  3257  loss  -0.3471027  grad l2 norm  0.005330141\n",
      "iter  3258  loss  -0.3471029  grad l2 norm  0.0053291996\n",
      "iter  3259  loss  -0.34710288  grad l2 norm  0.0053309733\n",
      "iter  3260  loss  -0.34710303  grad l2 norm  0.0053509735\n",
      "iter  3261  loss  -0.3471029  grad l2 norm  0.005364368\n",
      "iter  3262  loss  -0.34710297  grad l2 norm  0.0053848964\n",
      "iter  3263  loss  -0.347103  grad l2 norm  0.005388284\n",
      "iter  3264  loss  -0.34710318  grad l2 norm  0.0053932\n",
      "iter  3265  loss  -0.34710315  grad l2 norm  0.0053819693\n",
      "iter  3266  loss  -0.34710333  grad l2 norm  0.0053783604\n",
      "iter  3267  loss  -0.34710333  grad l2 norm  0.005367153\n",
      "iter  3268  loss  -0.34710348  grad l2 norm  0.0053703287\n",
      "iter  3269  loss  -0.34710348  grad l2 norm  0.005365655\n",
      "iter  3270  loss  -0.34710374  grad l2 norm  0.00536734\n",
      "iter  3271  loss  -0.34710386  grad l2 norm  0.005358461\n",
      "iter  3272  loss  -0.34710416  grad l2 norm  0.005363088\n",
      "iter  3273  loss  -0.3471041  grad l2 norm  0.0053633326\n",
      "iter  3274  loss  -0.34710425  grad l2 norm  0.0053804372\n",
      "iter  3275  loss  -0.34710392  grad l2 norm  0.0053951377\n",
      "iter  3276  loss  -0.34710386  grad l2 norm  0.005421688\n",
      "iter  3277  loss  -0.3471038  grad l2 norm  0.0054217186\n",
      "iter  3278  loss  -0.3471042  grad l2 norm  0.005406596\n",
      "iter  3279  loss  -0.34710446  grad l2 norm  0.0053711934\n",
      "iter  3280  loss  -0.34710485  grad l2 norm  0.0053516333\n",
      "iter  3281  loss  -0.3471049  grad l2 norm  0.0053351065\n",
      "iter  3282  loss  -0.347105  grad l2 norm  0.00534081\n",
      "iter  3283  loss  -0.34710503  grad l2 norm  0.0053464146\n",
      "iter  3284  loss  -0.34710506  grad l2 norm  0.0053679314\n",
      "iter  3285  loss  -0.34710497  grad l2 norm  0.005380394\n",
      "iter  3286  loss  -0.34710506  grad l2 norm  0.005398795\n",
      "iter  3287  loss  -0.3471049  grad l2 norm  0.005401073\n",
      "iter  3288  loss  -0.3471051  grad l2 norm  0.0054066936\n",
      "iter  3289  loss  -0.34710506  grad l2 norm  0.005394985\n",
      "iter  3290  loss  -0.34710538  grad l2 norm  0.005384358\n",
      "iter  3291  loss  -0.3471056  grad l2 norm  0.005362579\n",
      "iter  3292  loss  -0.3471058  grad l2 norm  0.005355655\n",
      "iter  3293  loss  -0.34710598  grad l2 norm  0.005346799\n",
      "iter  3294  loss  -0.34710607  grad l2 norm  0.00535645\n",
      "iter  3295  loss  -0.34710604  grad l2 norm  0.0053628013\n",
      "iter  3296  loss  -0.34710616  grad l2 norm  0.0053830794\n",
      "iter  3297  loss  -0.34710598  grad l2 norm  0.005396899\n",
      "iter  3298  loss  -0.34710598  grad l2 norm  0.00542334\n",
      "iter  3299  loss  -0.3471057  grad l2 norm  0.005438935\n",
      "iter  3300  loss  -0.34710586  grad l2 norm  0.005443331\n",
      "iter  3301  loss  -0.34710616  grad l2 norm  0.00541147\n",
      "iter  3302  loss  -0.34710664  grad l2 norm  0.0053799627\n",
      "iter  3303  loss  -0.3471068  grad l2 norm  0.00534616\n",
      "iter  3304  loss  -0.34710705  grad l2 norm  0.005338092\n",
      "iter  3305  loss  -0.34710705  grad l2 norm  0.005336389\n",
      "iter  3306  loss  -0.34710708  grad l2 norm  0.005357401\n",
      "iter  3307  loss  -0.34710693  grad l2 norm  0.0053770784\n",
      "iter  3308  loss  -0.34710684  grad l2 norm  0.005406461\n",
      "iter  3309  loss  -0.34710678  grad l2 norm  0.005415459\n",
      "iter  3310  loss  -0.34710693  grad l2 norm  0.0054168086\n",
      "iter  3311  loss  -0.3471071  grad l2 norm  0.0053972793\n",
      "iter  3312  loss  -0.34710744  grad l2 norm  0.0053843535\n",
      "iter  3313  loss  -0.3471075  grad l2 norm  0.0053647743\n",
      "iter  3314  loss  -0.34710774  grad l2 norm  0.0053617973\n",
      "iter  3315  loss  -0.3471077  grad l2 norm  0.005356559\n",
      "iter  3316  loss  -0.34710795  grad l2 norm  0.0053672194\n",
      "iter  3317  loss  -0.34710783  grad l2 norm  0.005372751\n",
      "iter  3318  loss  -0.34710798  grad l2 norm  0.005390255\n",
      "iter  3319  loss  -0.34710795  grad l2 norm  0.0053990884\n",
      "iter  3320  loss  -0.3471081  grad l2 norm  0.005417345\n",
      "iter  3321  loss  -0.3471079  grad l2 norm  0.005427971\n",
      "iter  3322  loss  -0.34710786  grad l2 norm  0.005446941\n",
      "iter  3323  loss  -0.34710777  grad l2 norm  0.005439022\n",
      "iter  3324  loss  -0.34710822  grad l2 norm  0.0054165893\n",
      "iter  3325  loss  -0.34710848  grad l2 norm  0.0053774337\n",
      "iter  3326  loss  -0.3471088  grad l2 norm  0.0053583407\n",
      "iter  3327  loss  -0.34710875  grad l2 norm  0.0053479644\n",
      "iter  3328  loss  -0.34710887  grad l2 norm  0.0053622993\n",
      "iter  3329  loss  -0.34710866  grad l2 norm  0.0053728158\n",
      "iter  3330  loss  -0.3471089  grad l2 norm  0.0053900303\n",
      "iter  3331  loss  -0.34710887  grad l2 norm  0.0053943912\n",
      "iter  3332  loss  -0.347109  grad l2 norm  0.0054040854\n",
      "iter  3333  loss  -0.347109  grad l2 norm  0.0053986283\n",
      "iter  3334  loss  -0.34710926  grad l2 norm  0.0053989175\n",
      "iter  3335  loss  -0.34710923  grad l2 norm  0.005387719\n",
      "iter  3336  loss  -0.34710947  grad l2 norm  0.005386737\n",
      "iter  3337  loss  -0.34710947  grad l2 norm  0.005379042\n",
      "iter  3338  loss  -0.3471097  grad l2 norm  0.005383975\n",
      "iter  3339  loss  -0.34710968  grad l2 norm  0.005382416\n",
      "iter  3340  loss  -0.34710997  grad l2 norm  0.005392033\n",
      "iter  3341  loss  -0.3471099  grad l2 norm  0.005393433\n",
      "iter  3342  loss  -0.3471101  grad l2 norm  0.005405512\n",
      "iter  3343  loss  -0.34710997  grad l2 norm  0.0054110624\n",
      "iter  3344  loss  -0.34710997  grad l2 norm  0.005430563\n",
      "iter  3345  loss  -0.34710968  grad l2 norm  0.005444597\n",
      "iter  3346  loss  -0.3471098  grad l2 norm  0.0054538175\n",
      "iter  3347  loss  -0.34710988  grad l2 norm  0.005430619\n",
      "iter  3348  loss  -0.34711033  grad l2 norm  0.005407071\n",
      "iter  3349  loss  -0.3471104  grad l2 norm  0.0053748903\n",
      "iter  3350  loss  -0.34711075  grad l2 norm  0.0053594257\n",
      "iter  3351  loss  -0.3471108  grad l2 norm  0.00534732\n",
      "iter  3352  loss  -0.34711105  grad l2 norm  0.005358139\n",
      "iter  3353  loss  -0.34711087  grad l2 norm  0.005369443\n",
      "iter  3354  loss  -0.34711096  grad l2 norm  0.0053948136\n",
      "iter  3355  loss  -0.34711084  grad l2 norm  0.0054078377\n",
      "iter  3356  loss  -0.34711093  grad l2 norm  0.005422109\n",
      "iter  3357  loss  -0.34711087  grad l2 norm  0.0054184985\n",
      "iter  3358  loss  -0.3471111  grad l2 norm  0.005417431\n",
      "iter  3359  loss  -0.34711117  grad l2 norm  0.0054037296\n",
      "iter  3360  loss  -0.34711146  grad l2 norm  0.005398911\n",
      "iter  3361  loss  -0.34711146  grad l2 norm  0.0053871074\n",
      "iter  3362  loss  -0.3471118  grad l2 norm  0.0053882557\n",
      "iter  3363  loss  -0.34711176  grad l2 norm  0.005384997\n",
      "iter  3364  loss  -0.34711194  grad l2 norm  0.0053957375\n",
      "iter  3365  loss  -0.34711188  grad l2 norm  0.005402427\n",
      "iter  3366  loss  -0.34711185  grad l2 norm  0.0054236343\n",
      "iter  3367  loss  -0.3471116  grad l2 norm  0.005442285\n",
      "iter  3368  loss  -0.34711143  grad l2 norm  0.005470972\n",
      "iter  3369  loss  -0.3471113  grad l2 norm  0.005469391\n",
      "iter  3370  loss  -0.34711188  grad l2 norm  0.0054430147\n",
      "iter  3371  loss  -0.34711218  grad l2 norm  0.005393938\n",
      "iter  3372  loss  -0.34711263  grad l2 norm  0.0053632585\n",
      "iter  3373  loss  -0.34711272  grad l2 norm  0.0053422484\n",
      "iter  3374  loss  -0.34711286  grad l2 norm  0.0053495583\n",
      "iter  3375  loss  -0.34711275  grad l2 norm  0.005362142\n",
      "iter  3376  loss  -0.34711275  grad l2 norm  0.005391898\n",
      "iter  3377  loss  -0.34711266  grad l2 norm  0.005412973\n",
      "iter  3378  loss  -0.34711272  grad l2 norm  0.005435935\n",
      "iter  3379  loss  -0.34711263  grad l2 norm  0.0054381127\n",
      "iter  3380  loss  -0.34711283  grad l2 norm  0.0054376423\n",
      "iter  3381  loss  -0.3471129  grad l2 norm  0.005420451\n",
      "iter  3382  loss  -0.34711316  grad l2 norm  0.005409751\n",
      "iter  3383  loss  -0.34711325  grad l2 norm  0.005392666\n",
      "iter  3384  loss  -0.34711346  grad l2 norm  0.0053904452\n",
      "iter  3385  loss  -0.34711355  grad l2 norm  0.005386757\n",
      "iter  3386  loss  -0.34711364  grad l2 norm  0.005399567\n",
      "iter  3387  loss  -0.3471135  grad l2 norm  0.00540936\n",
      "iter  3388  loss  -0.34711352  grad l2 norm  0.00542902\n",
      "iter  3389  loss  -0.34711343  grad l2 norm  0.005434772\n",
      "iter  3390  loss  -0.34711367  grad l2 norm  0.0054469192\n",
      "iter  3391  loss  -0.3471134  grad l2 norm  0.005453049\n",
      "iter  3392  loss  -0.34711352  grad l2 norm  0.0054583545\n",
      "iter  3393  loss  -0.34711367  grad l2 norm  0.00543141\n",
      "iter  3394  loss  -0.34711415  grad l2 norm  0.005403689\n",
      "iter  3395  loss  -0.3471143  grad l2 norm  0.00537292\n",
      "iter  3396  loss  -0.34711456  grad l2 norm  0.0053674476\n",
      "iter  3397  loss  -0.3471145  grad l2 norm  0.0053681205\n",
      "iter  3398  loss  -0.34711456  grad l2 norm  0.0053883623\n",
      "iter  3399  loss  -0.3471145  grad l2 norm  0.0054047713\n",
      "iter  3400  loss  -0.34711456  grad l2 norm  0.0054285517\n",
      "iter  3401  loss  -0.34711444  grad l2 norm  0.005436041\n",
      "iter  3402  loss  -0.34711462  grad l2 norm  0.00544262\n",
      "iter  3403  loss  -0.3471146  grad l2 norm  0.0054318216\n",
      "iter  3404  loss  -0.34711483  grad l2 norm  0.0054254904\n",
      "iter  3405  loss  -0.34711486  grad l2 norm  0.0054108575\n",
      "iter  3406  loss  -0.34711507  grad l2 norm  0.0054098824\n",
      "iter  3407  loss  -0.34711495  grad l2 norm  0.005406367\n",
      "iter  3408  loss  -0.3471151  grad l2 norm  0.005413693\n",
      "iter  3409  loss  -0.3471152  grad l2 norm  0.0054090363\n",
      "iter  3410  loss  -0.3471155  grad l2 norm  0.005412016\n",
      "iter  3411  loss  -0.3471155  grad l2 norm  0.005409289\n",
      "iter  3412  loss  -0.3471156  grad l2 norm  0.005421457\n",
      "iter  3413  loss  -0.3471155  grad l2 norm  0.0054322593\n",
      "iter  3414  loss  -0.3471153  grad l2 norm  0.0054587703\n",
      "iter  3415  loss  -0.34711513  grad l2 norm  0.005468856\n",
      "iter  3416  loss  -0.34711543  grad l2 norm  0.005460944\n",
      "iter  3417  loss  -0.34711576  grad l2 norm  0.005426762\n",
      "iter  3418  loss  -0.34711605  grad l2 norm  0.0054028323\n",
      "iter  3419  loss  -0.34711614  grad l2 norm  0.00538202\n",
      "iter  3420  loss  -0.34711638  grad l2 norm  0.0053836387\n",
      "iter  3421  loss  -0.34711635  grad l2 norm  0.005387006\n",
      "iter  3422  loss  -0.34711644  grad l2 norm  0.005406512\n",
      "iter  3423  loss  -0.34711632  grad l2 norm  0.005419811\n",
      "iter  3424  loss  -0.34711638  grad l2 norm  0.005439167\n",
      "iter  3425  loss  -0.34711626  grad l2 norm  0.005443714\n",
      "iter  3426  loss  -0.34711635  grad l2 norm  0.0054504117\n",
      "iter  3427  loss  -0.34711635  grad l2 norm  0.0054432093\n",
      "iter  3428  loss  -0.3471165  grad l2 norm  0.00543776\n",
      "iter  3429  loss  -0.34711668  grad l2 norm  0.0054176375\n",
      "iter  3430  loss  -0.34711704  grad l2 norm  0.005406817\n",
      "iter  3431  loss  -0.34711707  grad l2 norm  0.0053936327\n",
      "iter  3432  loss  -0.34711727  grad l2 norm  0.005397675\n",
      "iter  3433  loss  -0.34711725  grad l2 norm  0.0054006535\n",
      "iter  3434  loss  -0.3471173  grad l2 norm  0.0054183127\n",
      "iter  3435  loss  -0.34711722  grad l2 norm  0.0054315105\n",
      "iter  3436  loss  -0.34711716  grad l2 norm  0.005456805\n",
      "iter  3437  loss  -0.34711692  grad l2 norm  0.005476767\n",
      "iter  3438  loss  -0.34711683  grad l2 norm  0.0054956186\n",
      "iter  3439  loss  -0.34711704  grad l2 norm  0.0054772366\n",
      "iter  3440  loss  -0.3471175  grad l2 norm  0.0054462356\n",
      "iter  3441  loss  -0.34711778  grad l2 norm  0.0054050307\n",
      "iter  3442  loss  -0.34711814  grad l2 norm  0.005386264\n",
      "iter  3443  loss  -0.3471181  grad l2 norm  0.0053762365\n",
      "iter  3444  loss  -0.34711826  grad l2 norm  0.005389958\n",
      "iter  3445  loss  -0.34711808  grad l2 norm  0.0054064007\n",
      "iter  3446  loss  -0.347118  grad l2 norm  0.0054369434\n",
      "iter  3447  loss  -0.3471178  grad l2 norm  0.0054555573\n",
      "iter  3448  loss  -0.34711787  grad l2 norm  0.005468139\n",
      "iter  3449  loss  -0.347118  grad l2 norm  0.0054551833\n",
      "iter  3450  loss  -0.3471183  grad l2 norm  0.005441863\n",
      "iter  3451  loss  -0.34711838  grad l2 norm  0.005419051\n",
      "iter  3452  loss  -0.34711865  grad l2 norm  0.005410096\n",
      "iter  3453  loss  -0.34711868  grad l2 norm  0.0054001915\n",
      "iter  3454  loss  -0.3471189  grad l2 norm  0.005406369\n",
      "iter  3455  loss  -0.34711882  grad l2 norm  0.00541022\n",
      "iter  3456  loss  -0.34711894  grad l2 norm  0.0054265247\n",
      "iter  3457  loss  -0.3471189  grad l2 norm  0.005436226\n",
      "iter  3458  loss  -0.347119  grad l2 norm  0.005454222\n",
      "iter  3459  loss  -0.34711885  grad l2 norm  0.0054644495\n",
      "iter  3460  loss  -0.3471188  grad l2 norm  0.0054855095\n",
      "iter  3461  loss  -0.34711856  grad l2 norm  0.005493836\n",
      "iter  3462  loss  -0.3471189  grad l2 norm  0.00548553\n",
      "iter  3463  loss  -0.34711915  grad l2 norm  0.005447786\n",
      "iter  3464  loss  -0.34711957  grad l2 norm  0.005417702\n",
      "iter  3465  loss  -0.34711963  grad l2 norm  0.0053937957\n",
      "iter  3466  loss  -0.3471197  grad l2 norm  0.0053974586\n",
      "iter  3467  loss  -0.3471196  grad l2 norm  0.0054057203\n",
      "iter  3468  loss  -0.34711966  grad l2 norm  0.005424759\n",
      "iter  3469  loss  -0.34711963  grad l2 norm  0.005432907\n",
      "iter  3470  loss  -0.3471198  grad l2 norm  0.0054462245\n",
      "iter  3471  loss  -0.34711972  grad l2 norm  0.0054460377\n",
      "iter  3472  loss  -0.34711993  grad l2 norm  0.0054491693\n",
      "iter  3473  loss  -0.34711993  grad l2 norm  0.005439731\n",
      "iter  3474  loss  -0.3471201  grad l2 norm  0.0054372516\n",
      "iter  3475  loss  -0.34712023  grad l2 norm  0.005427726\n",
      "iter  3476  loss  -0.3471204  grad l2 norm  0.0054295477\n",
      "iter  3477  loss  -0.34712037  grad l2 norm  0.0054261982\n",
      "iter  3478  loss  -0.3471206  grad l2 norm  0.0054335953\n",
      "iter  3479  loss  -0.34712052  grad l2 norm  0.0054344353\n",
      "iter  3480  loss  -0.34712076  grad l2 norm  0.00544431\n",
      "iter  3481  loss  -0.34712064  grad l2 norm  0.005448028\n",
      "iter  3482  loss  -0.34712073  grad l2 norm  0.0054632756\n",
      "iter  3483  loss  -0.3471205  grad l2 norm  0.0054768883\n",
      "iter  3484  loss  -0.34712043  grad l2 norm  0.005500571\n",
      "iter  3485  loss  -0.34712037  grad l2 norm  0.005497154\n",
      "iter  3486  loss  -0.34712073  grad l2 norm  0.0054785395\n",
      "iter  3487  loss  -0.34712085  grad l2 norm  0.0054420847\n",
      "iter  3488  loss  -0.34712127  grad l2 norm  0.0054162336\n",
      "iter  3489  loss  -0.34712145  grad l2 norm  0.005393665\n",
      "iter  3490  loss  -0.3471217  grad l2 norm  0.0053942064\n",
      "iter  3491  loss  -0.34712157  grad l2 norm  0.005399855\n",
      "iter  3492  loss  -0.3471217  grad l2 norm  0.0054237857\n",
      "iter  3493  loss  -0.34712142  grad l2 norm  0.005442062\n",
      "iter  3494  loss  -0.3471215  grad l2 norm  0.0054638917\n",
      "iter  3495  loss  -0.3471214  grad l2 norm  0.005468\n",
      "iter  3496  loss  -0.3471216  grad l2 norm  0.0054707397\n",
      "iter  3497  loss  -0.34712163  grad l2 norm  0.00545882\n",
      "iter  3498  loss  -0.34712186  grad l2 norm  0.0054519307\n",
      "iter  3499  loss  -0.34712195  grad l2 norm  0.0054377625\n",
      "iter  3500  loss  -0.34712222  grad l2 norm  0.0054345583\n",
      "iter  3501  loss  -0.34712222  grad l2 norm  0.005427724\n",
      "iter  3502  loss  -0.3471224  grad l2 norm  0.005434392\n",
      "iter  3503  loss  -0.34712237  grad l2 norm  0.005438421\n",
      "iter  3504  loss  -0.3471225  grad l2 norm  0.0054554422\n",
      "iter  3505  loss  -0.34712234  grad l2 norm  0.005469939\n",
      "iter  3506  loss  -0.34712216  grad l2 norm  0.00549773\n",
      "iter  3507  loss  -0.34712183  grad l2 norm  0.0055162976\n",
      "iter  3508  loss  -0.34712195  grad l2 norm  0.0055171065\n",
      "iter  3509  loss  -0.34712228  grad l2 norm  0.005478555\n",
      "iter  3510  loss  -0.34712285  grad l2 norm  0.005438525\n",
      "iter  3511  loss  -0.3471231  grad l2 norm  0.005400545\n",
      "iter  3512  loss  -0.34712324  grad l2 norm  0.00539154\n",
      "iter  3513  loss  -0.3471232  grad l2 norm  0.005392951\n",
      "iter  3514  loss  -0.3471233  grad l2 norm  0.005415993\n",
      "iter  3515  loss  -0.34712315  grad l2 norm  0.005438212\n",
      "iter  3516  loss  -0.34712315  grad l2 norm  0.005467678\n",
      "iter  3517  loss  -0.347123  grad l2 norm  0.005480766\n",
      "iter  3518  loss  -0.3471232  grad l2 norm  0.005489342\n",
      "iter  3519  loss  -0.3471232  grad l2 norm  0.0054783192\n",
      "iter  3520  loss  -0.34712347  grad l2 norm  0.0054682214\n",
      "iter  3521  loss  -0.34712356  grad l2 norm  0.0054491875\n",
      "iter  3522  loss  -0.34712374  grad l2 norm  0.0054415516\n",
      "iter  3523  loss  -0.34712383  grad l2 norm  0.0054325857\n",
      "iter  3524  loss  -0.34712398  grad l2 norm  0.005438873\n",
      "iter  3525  loss  -0.3471238  grad l2 norm  0.0054446748\n",
      "iter  3526  loss  -0.3471239  grad l2 norm  0.0054621473\n",
      "iter  3527  loss  -0.3471238  grad l2 norm  0.0054689148\n",
      "iter  3528  loss  -0.34712398  grad l2 norm  0.005478385\n",
      "iter  3529  loss  -0.34712392  grad l2 norm  0.0054821465\n",
      "iter  3530  loss  -0.3471238  grad l2 norm  0.0054999664\n",
      "iter  3531  loss  -0.3471237  grad l2 norm  0.005502745\n",
      "iter  3532  loss  -0.347124  grad l2 norm  0.0054882467\n",
      "iter  3533  loss  -0.34712425  grad l2 norm  0.005451804\n",
      "iter  3534  loss  -0.34712467  grad l2 norm  0.0054288013\n",
      "iter  3535  loss  -0.3471247  grad l2 norm  0.0054129357\n",
      "iter  3536  loss  -0.34712493  grad l2 norm  0.005419809\n",
      "iter  3537  loss  -0.34712484  grad l2 norm  0.005429413\n",
      "iter  3538  loss  -0.3471249  grad l2 norm  0.005452422\n",
      "iter  3539  loss  -0.3471247  grad l2 norm  0.0054673767\n",
      "iter  3540  loss  -0.34712484  grad l2 norm  0.005483991\n",
      "iter  3541  loss  -0.3471248  grad l2 norm  0.0054838373\n",
      "iter  3542  loss  -0.34712505  grad l2 norm  0.005482939\n",
      "iter  3543  loss  -0.34712502  grad l2 norm  0.005469811\n",
      "iter  3544  loss  -0.34712517  grad l2 norm  0.00546505\n",
      "iter  3545  loss  -0.34712505  grad l2 norm  0.005456625\n",
      "iter  3546  loss  -0.3471253  grad l2 norm  0.005458905\n",
      "iter  3547  loss  -0.3471253  grad l2 norm  0.0054517635\n",
      "iter  3548  loss  -0.34712556  grad l2 norm  0.005450917\n",
      "iter  3549  loss  -0.34712565  grad l2 norm  0.005445532\n",
      "iter  3550  loss  -0.3471258  grad l2 norm  0.005454031\n",
      "iter  3551  loss  -0.34712577  grad l2 norm  0.005461074\n",
      "iter  3552  loss  -0.3471257  grad l2 norm  0.0054831826\n",
      "iter  3553  loss  -0.34712535  grad l2 norm  0.005504918\n",
      "iter  3554  loss  -0.3471253  grad l2 norm  0.005528148\n",
      "iter  3555  loss  -0.34712547  grad l2 norm  0.0055152923\n",
      "iter  3556  loss  -0.3471259  grad l2 norm  0.005487937\n",
      "iter  3557  loss  -0.34712616  grad l2 norm  0.0054500834\n",
      "iter  3558  loss  -0.34712642  grad l2 norm  0.005431698\n",
      "iter  3559  loss  -0.3471265  grad l2 norm  0.005420781\n",
      "iter  3560  loss  -0.34712663  grad l2 norm  0.005430466\n",
      "iter  3561  loss  -0.34712645  grad l2 norm  0.0054419204\n",
      "iter  3562  loss  -0.34712654  grad l2 norm  0.0054661003\n",
      "iter  3563  loss  -0.3471263  grad l2 norm  0.0054813176\n",
      "iter  3564  loss  -0.3471264  grad l2 norm  0.0054983\n",
      "iter  3565  loss  -0.34712628  grad l2 norm  0.005499669\n",
      "iter  3566  loss  -0.34712642  grad l2 norm  0.0054979553\n",
      "iter  3567  loss  -0.34712654  grad l2 norm  0.0054780217\n",
      "iter  3568  loss  -0.34712696  grad l2 norm  0.005462198\n",
      "iter  3569  loss  -0.34712702  grad l2 norm  0.005443179\n",
      "iter  3570  loss  -0.34712726  grad l2 norm  0.0054404293\n",
      "iter  3571  loss  -0.3471273  grad l2 norm  0.005438688\n",
      "iter  3572  loss  -0.3471273  grad l2 norm  0.005451908\n",
      "iter  3573  loss  -0.34712726  grad l2 norm  0.005462691\n",
      "iter  3574  loss  -0.34712738  grad l2 norm  0.0054835742\n",
      "iter  3575  loss  -0.34712717  grad l2 norm  0.0054993504\n",
      "iter  3576  loss  -0.34712717  grad l2 norm  0.0055254716\n",
      "iter  3577  loss  -0.34712684  grad l2 norm  0.00554093\n",
      "iter  3578  loss  -0.34712705  grad l2 norm  0.005536776\n",
      "iter  3579  loss  -0.34712735  grad l2 norm  0.0054978416\n",
      "iter  3580  loss  -0.34712777  grad l2 norm  0.0054611033\n",
      "iter  3581  loss  -0.3471279  grad l2 norm  0.005429479\n",
      "iter  3582  loss  -0.3471282  grad l2 norm  0.0054245493\n",
      "iter  3583  loss  -0.3471281  grad l2 norm  0.0054290853\n",
      "iter  3584  loss  -0.34712806  grad l2 norm  0.0054544033\n",
      "iter  3585  loss  -0.3471278  grad l2 norm  0.0054781274\n",
      "iter  3586  loss  -0.3471278  grad l2 norm  0.005502995\n",
      "iter  3587  loss  -0.3471278  grad l2 norm  0.0055050007\n",
      "iter  3588  loss  -0.34712803  grad l2 norm  0.0055013276\n",
      "iter  3589  loss  -0.34712818  grad l2 norm  0.0054830313\n",
      "iter  3590  loss  -0.3471284  grad l2 norm  0.005471304\n",
      "iter  3591  loss  -0.34712845  grad l2 norm  0.005456608\n",
      "iter  3592  loss  -0.34712866  grad l2 norm  0.005455382\n",
      "iter  3593  loss  -0.34712863  grad l2 norm  0.0054531423\n",
      "iter  3594  loss  -0.34712878  grad l2 norm  0.0054638516\n",
      "iter  3595  loss  -0.34712875  grad l2 norm  0.0054701976\n",
      "iter  3596  loss  -0.3471289  grad l2 norm  0.005484867\n",
      "iter  3597  loss  -0.3471288  grad l2 norm  0.005492255\n",
      "iter  3598  loss  -0.34712893  grad l2 norm  0.0055072946\n",
      "iter  3599  loss  -0.34712872  grad l2 norm  0.005518725\n",
      "iter  3600  loss  -0.34712863  grad l2 norm  0.005540074\n",
      "iter  3601  loss  -0.3471285  grad l2 norm  0.0055373157\n",
      "iter  3602  loss  -0.34712896  grad l2 norm  0.0055147307\n",
      "iter  3603  loss  -0.34712923  grad l2 norm  0.0054757316\n",
      "iter  3604  loss  -0.34712946  grad l2 norm  0.0054547396\n",
      "iter  3605  loss  -0.34712946  grad l2 norm  0.00544359\n",
      "iter  3606  loss  -0.34712952  grad l2 norm  0.0054493416\n",
      "iter  3607  loss  -0.34712955  grad l2 norm  0.0054522622\n",
      "iter  3608  loss  -0.34712967  grad l2 norm  0.005467636\n",
      "iter  3609  loss  -0.34712958  grad l2 norm  0.00547768\n",
      "iter  3610  loss  -0.34712964  grad l2 norm  0.005494008\n",
      "iter  3611  loss  -0.34712955  grad l2 norm  0.0054969233\n",
      "iter  3612  loss  -0.3471298  grad l2 norm  0.0055006635\n",
      "iter  3613  loss  -0.3471297  grad l2 norm  0.005491609\n",
      "iter  3614  loss  -0.34712988  grad l2 norm  0.005488053\n",
      "iter  3615  loss  -0.34713006  grad l2 norm  0.0054782415\n",
      "iter  3616  loss  -0.34713018  grad l2 norm  0.005477535\n",
      "iter  3617  loss  -0.3471303  grad l2 norm  0.0054725655\n",
      "iter  3618  loss  -0.34713048  grad l2 norm  0.0054779868\n",
      "iter  3619  loss  -0.34713045  grad l2 norm  0.0054791393\n",
      "iter  3620  loss  -0.3471306  grad l2 norm  0.005490714\n",
      "iter  3621  loss  -0.3471305  grad l2 norm  0.0054987688\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PlayGround/ergodic_mmd/notebooks/../ergodic_mmd/aug_lagrange_solver.py:76\u001b[0m, in \u001b[0;36mAugmentedLagrangeSolver.solve\u001b[0;34m(self, args, max_iter, eps, alpha)\u001b[0m\n\u001b[1;32m     72\u001b[0m _prev_val   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# self.solution, _val, self.avg_sq_grad = self.step(self.solution, args, self.avg_sq_grad, self.c)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolution, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_solution, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_sq_grad, _val, _dldx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_sq_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     _grad_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     78\u001b[0m     _N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver.solve(max_iter=15_000, eps=1e-3, alpha=1+1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol = solver.solution\n",
    "# X = sol['X']\n",
    "# vertices = vmap(get_fk)(X).translation().astype(np.float32).T\n",
    "# scene.vis['lines_segments'].set_object(mc_geom.Line(mc_geom.PointsGeometry(vertices), mc_geom.MeshLambertMaterial(color=0xff0000)))\n",
    "# ee_frame = scene.vis['ee']\n",
    "# ee_frame.set_object(mc_geom.triad(scale=0.2))\n",
    "# for q in X:\n",
    "#     _tf = np.array(get_fk(q).as_matrix()).astype(np.float64)\n",
    "#     ee_frame.set_transform(_tf)\n",
    "#     for i, jt in enumerate(q):\n",
    "#         robot[i] = jt\n",
    "#     time.sleep(0.01)\n",
    "#     scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7012/static/\n"
     ]
    }
   ],
   "source": [
    "scene = Scene()\n",
    "sol = solver.solution\n",
    "X = sol['X']\n",
    "\n",
    "vertices = vmap(get_fk)(X).translation().astype(np.float32).T\n",
    "scene.vis['lines_segments'].set_object(mc_geom.Line(mc_geom.PointsGeometry(vertices), mc_geom.MeshLambertMaterial(color=0xff0000, linewidth=2)))\n",
    "\n",
    "\n",
    "robot_img = []\n",
    "for i, q in enumerate(X[::int(T/5)]):\n",
    "    # _tf = np.array(get_fk(q).as_matrix()).astype(np.float64)\n",
    "    # ee_frame.set_transform(_tf)\n",
    "    _robot = Robot(name='robot{}'.format(i),urdf_path=PandaLoader().df_path, mesh_folder_path=Path(PandaLoader().model_path).parent.parent, opacity=0.75)\n",
    "    robot_img.append(_robot)\n",
    "    scene.add_robot(_robot)\n",
    "    for j, jt in enumerate(q):\n",
    "        _robot[j] = jt\n",
    "\"Render the initial scene\"\n",
    "scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ps.set_up_dir(\"z_up\")\n",
    "# # ps.set_front_dir(\"neg_y_front\")\n",
    "# ps.init()\n",
    "\n",
    "\n",
    "# ps_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "# # ps_points = ps.register_point_cloud(\"sampled points\", args['points'][:,:3])\n",
    "# # ps_mesh.add_scalar_quantity(\"face vals\",vmap(info_distr, in_axes=(0,))(mesh.vertices))\n",
    "\n",
    "# # ps_points.add_scalar_quantity(\"results\", args['P_XI'])\n",
    "\n",
    "# # ps_bunny_mesh.set_transparency(0.8)\n",
    "# # ps_bunny_mesh.add_scalar_quantity('info_distr', mesh_func.func_vals, defined_on='vertices', cmap='blues')\n",
    "\n",
    "# ps_traj         = ps.register_curve_network(\"trajectory\", vmap(get_fk)(X).translation() , edges=\"line\")\n",
    "\n",
    "\n",
    "# for _ in range(1000):\n",
    "#     solver.solve(eps=1e-5, max_iter=10)\n",
    "#     sol = solver.solution\n",
    "#     X = sol['X']\n",
    "\n",
    "#     ps_traj.update_node_positions(vmap(get_fk)(X).translation() )\n",
    "#     # ps_traj.add_vector_quantity(\"vec img\", X[:,:3], enabled=True)\n",
    "#     ps.frame_tick()\n",
    "\n",
    "#     time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.solve(eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solver.solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 4.1 Metal - 88\n"
     ]
    }
   ],
   "source": [
    "X = sol['X']\n",
    "\n",
    "ps.init()\n",
    "\n",
    "ps_bunny_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "# ps_bunny_mesh.set_transparency(0.8)\n",
    "ps_bunny_mesh.add_scalar_quantity('info_distr', vmap(info_distr, in_axes=(0,))(mesh.vertices), defined_on='vertices', cmap='blues')\n",
    "\n",
    "ps_traj         = ps.register_curve_network(\"trajectory\", vmap(get_fk)(X).translation() , edges=\"line\")\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
