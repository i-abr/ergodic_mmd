{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import jax \n",
    "import jax.numpy as jnp \n",
    "import numpy as np\n",
    "from jaxlie import SE3, SO3\n",
    "from jax.random import normal\n",
    "from jax import grad, hessian, vmap, pmap\n",
    "from jax.flatten_util import ravel_pytree\n",
    "from jax.lax import scan\n",
    "from functools import partial\n",
    "import jax.random as jax_random\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "from plyfile import PlyData\n",
    "import open3d as o3d \n",
    "import trimesh as tm\n",
    "import trimesh\n",
    "import time\n",
    "import jaxopt\n",
    "import polyscope as ps\n",
    "\n",
    "\n",
    "import meshcat\n",
    "import meshcat.geometry as mc_geom\n",
    "import meshcat.transformations as mc_tf\n",
    "# from ergodic_mmd.aug_lagrange_jaxopt import AugmentedLagrangeSolver\n",
    "from ergodic_mmd.aug_lagrange_solver import AugmentedLagrangeSolver\n",
    "\n",
    "import adam\n",
    "from adam.jax import KinDynComputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can open the visualizer by visiting the following URL:\n",
      "http://127.0.0.1:7046/static/\n"
     ]
    }
   ],
   "source": [
    "from robomeshcat import Object, Robot, Scene\n",
    "from example_robot_data.robots_loader import PandaLoader\n",
    "from pathlib import Path\n",
    "\n",
    "\"Create a scene that stores all objects and robots and has rendering capability\"\n",
    "\n",
    "scene = Scene()\n",
    "\n",
    "\n",
    "robot = Robot(urdf_path=PandaLoader().df_path, mesh_folder_path=Path(PandaLoader().model_path).parent.parent)\n",
    "scene.add_robot(robot)\n",
    "\"Render the initial scene\"\n",
    "scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mesh = tm.primitives.Box([0.25, 0.25, 0.25])\n",
    "# mesh.apply_scale(1.2)\n",
    "# mesh.apply_transform(\n",
    "#     SE3.from_rotation(SO3.from_x_radians(np.pi/2)).as_matrix()\n",
    "#     )\n",
    "mesh.apply_translation(np.array([0.5,0,0.125]))\n",
    "\n",
    "num_points = 1000  # Change this number based on your requirement\n",
    "points, face_indices = tm.sample.sample_surface(mesh, num_points)\n",
    "\n",
    "info_distr = lambda x, n: jnp.maximum(0, jnp.dot(n, jnp.array([0,1,0])) + jnp.dot(n,jnp.array([0,0,1])))#jnp.exp(-20*(x[0]-0.5)**2 - 20*(x[1]+0.1)**2 - 20*(x[2]-0.4)**2)\n",
    "P_XI = vmap(info_distr, in_axes=(0,0))(points, mesh.face_normals[face_indices])\n",
    "P_XI_NORM = jnp.sum(P_XI)\n",
    "P_XI = P_XI/P_XI_NORM\n",
    "h = 0.01\n",
    "args = {'h' : h, \n",
    "        'points' : points+0.05*mesh.face_normals[face_indices], \n",
    "        'normals' : mesh.face_normals[face_indices], \n",
    "        'P_XI' : P_XI}\n",
    "\n",
    "\n",
    "# exp_obj = trimesh.exchange.obj.export_obj(mesh)\n",
    "# mc_obj = mc_geom.ObjMeshGeometry.from_stream(trimesh.util.wrap_as_stream(exp_obj))\n",
    "# mc_obj = Object(mc_obj, name='red_sphere', opacity=0.5, color=[1., 0., 0.])\n",
    "# scene.add_object(mc_obj)\n",
    "\n",
    "mesh_vertex_color_info = np.ones_like(mesh.vertices)\n",
    "# _color_map = vmap(info_distr)(mesh.vertices)\n",
    "# _color_map = _color_map - _color_map.min()\n",
    "# # _color_map = _color_map/_color_map. max()\n",
    "# mesh_vertex_color_info[:,0] = _color_map\n",
    "# mc_obj = mc_geom.TriangularMeshGeometry(mesh.vertices, mesh.faces, color=mesh_vertex_color_info)\n",
    "# scene.vis['obj'].set_object(\n",
    "#     mc_obj, \n",
    "#     mc_geom.MeshPhongMaterial()\n",
    "# )\n",
    "box = meshcat.geometry.Box([0.25, 0.25, 0.25])\n",
    "scene.vis['obj'].set_object(box, mc_geom.MeshLambertMaterial(color=0xADD8E6, reflectivity=0.8))\n",
    "scene.vis['obj'].set_transform(mc_tf.translation_matrix([0.5,0,0.125]))\n",
    "color_map = np.zeros_like(args['points'].T)\n",
    "color_map[0,:] = P_XI*P_XI_NORM\n",
    "scene.vis['obj_samples'].set_object(\n",
    "        mc_geom.PointsGeometry(position=args['points'].T, color=color_map),\n",
    "        mc_geom.PointsMaterial(size=0.01)\n",
    ")\n",
    "\n",
    "# mc_obj = Object(mc_obj, name='red_sphere', opacity=1)\n",
    "# scene.add_object(mc_obj)\n",
    "\n",
    "# verts = np.random.rand(3, 100000)\n",
    "# scene.vis[\"perception/pointclouds/random\"].set_object(\n",
    "#     mc_geom.PointCloud(position=verts, color=verts))\n",
    "# scene.vis[\"perception/pointclouds/random\"].set_transform(\n",
    "#     mc_tf.translation_matrix([0, 1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link0']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link1']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link2']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link4']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link5']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link6']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_link7']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_hand']/collision[1]\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_leftfinger']/collision[1]\n",
      "Unknown tag \"contact\" in /robot[@name='panda']/link[@name='panda_leftfinger']\n",
      "Unknown tag \"material\" in /robot[@name='panda']/link[@name='panda_rightfinger']/collision[1]\n",
      "Unknown tag \"contact\" in /robot[@name='panda']/link[@name='panda_rightfinger']\n"
     ]
    }
   ],
   "source": [
    "model_path = \"../assets/panda.urdf\"\n",
    "# The joint list\n",
    "joints_name_list = [\n",
    "    'panda_joint1', 'panda_joint2', 'panda_joint3', 'panda_joint4', \n",
    "    'panda_joint5', 'panda_joint6', 'panda_joint7'\n",
    "]\n",
    "\n",
    "kinDyn = KinDynComputations(model_path, joints_name_list)\n",
    "# kinDyn.set_frame_velocity_representation(adam.Representations.BODY_FIXED_REPRESENTATION)\n",
    "w_H_ee = kinDyn.forward_kinematics_fun(\"panda_grasptarget\")\n",
    "w_H_b = np.eye(4) # base frame \n",
    "\n",
    "q0 = jnp.array([0,-np.pi/3,0,-3, 0, 2.5, -0.7853])\n",
    "args.update({'q0':q0})\n",
    "# w_H_ee(w_H_b, q0)\n",
    "for i, jt in enumerate(q0):\n",
    "    robot[i] = jt\n",
    "scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_fk(q):\n",
    "#     return SE3.from_matrix(w_H_ee(w_H_b, q))\n",
    "\n",
    "def RBF_kernel(x, xp, h=0.01):\n",
    "    return jnp.exp(\n",
    "        -jnp.sum((x-xp)**2)/h\n",
    "    )\n",
    "\n",
    "# def camera_view_penalty(g, v, h=0.01):\n",
    "#     p = g.translation()\n",
    "#     rot = g.rotation()\n",
    "#     w = rot.apply(jnp.array([0.,0.,1.]))\n",
    "#     root, norm  = jnp.split(v, 2)\n",
    "#     return jnp.exp(-jnp.sum((root-p)**2)/h - jnp.sum((-norm-w)**2)/h)\n",
    "#     # return jnp.exp(-jnp.sum((root-p)**2)/h)*jnp.sum((-norm-w)**2)\n",
    "\n",
    "# def camera_view_penalty(g, v, h=0.01):\n",
    "#     p = g.translation()\n",
    "#     rot = g.rotation()\n",
    "#     w = rot.apply(jnp.array([0.,0.,1.]))\n",
    "#     root, norm  = jnp.split(v, 2)\n",
    "#     # return jnp.exp(-jnp.sum((root-p)**2)/h - 0.01*jnp.sum((-norm-w)**2)/h)\n",
    "#     return jnp.exp(\n",
    "#         -jnp.sum((root-p)**2)/h #- 1e-1*jnp.sum((norm-w)**2)\n",
    "#     )*jnp.dot(-norm,w)\n",
    "\n",
    "\n",
    "\n",
    "# def RBF_kernel(g, v, h=0.01):\n",
    "#     p = g.translation()\n",
    "#     rot = g.rotation()\n",
    "#     w = rot.apply(jnp.array([0.,0.,1.]))\n",
    "#     root, norm  = jnp.split(v, 2)\n",
    "#     return jnp.exp(\n",
    "#         -jnp.sum((root-p)**2)/h #- 1e-1*jnp.sum((norm-w)**2)\n",
    "#     ) * jnp.exp(\n",
    "#         -10*jnp.sum((-norm-w)**2)\n",
    "#     )\n",
    "\n",
    "def create_kernel_matrix(kernel):\n",
    "    return vmap(vmap(kernel, in_axes=(0, None, None)), in_axes=(None, 0, None))\n",
    "\n",
    "# camera_view_matrix = create_kernel_matrix(camera_view_penalty)\n",
    "KernelMatrix = create_kernel_matrix(RBF_kernel)\n",
    "def emmd_loss(params, args):\n",
    "    X = params['X']\n",
    "    T = X.shape[0]\n",
    "    h = args['h']\n",
    "    # q0 = args['q0']\n",
    "    points    = args['points']\n",
    "    norms     = args['normals']\n",
    "    P_XI      = args['P_XI']\n",
    "\n",
    "    # g = vmap(get_fk)(X)\n",
    "    # p = g.translation()\n",
    "    \n",
    "    # view_matrix = camera_view_matrix(g, jnp.hstack([points, norms]), 0.1)\n",
    "\n",
    "    # return np.sum(KernelMatrix(p, p, h))/(T**2) \\\n",
    "    #         - 2 * np.sum(P_XI @ KernelMatrix(p, points, h))/T \\\n",
    "    #         - np.sum(P_XI@view_matrix)/T \\\n",
    "    #                 + 1e-5*jnp.mean(jnp.square(X-q0)) \\\n",
    "    #                     + jnp.mean(jnp.square(X[1:] - X[:-1]))\n",
    "    return np.sum(KernelMatrix(X, X, h))/(T**2) \\\n",
    "            - 2 * np.sum(P_XI @ KernelMatrix(X, points, h))/T \\\n",
    "                        + 2*jnp.mean(jnp.square(X[1:] - X[:-1]))\n",
    "            # - np.sum(P_XI@view_matrix)/T \\\n",
    "                    # + 1e-5*jnp.mean(jnp.square(X-q0)) \\\n",
    "\n",
    "\n",
    "def eq_constr(params, args):\n",
    "    # X = params['X']\n",
    "    # q0 = args['q0']\n",
    "    return jnp.zeros(1)\n",
    "\n",
    "def ineq_constr(params, args):\n",
    "    # X = params['X']\n",
    "    # return jnp.square(X[1:] - X[:-1]) - 0.1**2\n",
    "    return jnp.zeros(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 200\n",
    "X = jnp.linspace(mesh.bounds[0], mesh.bounds[1], num=T)\n",
    "\n",
    "params = {'X' : X}\n",
    "\n",
    "# solver = AugmentedLagrangeSolver(params, emmd_loss, eq_constr, ineq_constr, args=args)\n",
    "solver = AugmentedLagrangeSolver(\n",
    "    params, emmd_loss, eq_constr, ineq_constr, \n",
    "    step_size=1e-3,\n",
    "    args=args)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter  0  loss  0.17885551  grad l2 norm  0.065232284\n",
      "iter  1  loss  0.17604311  grad l2 norm  0.063169934\n",
      "iter  2  loss  0.17402367  grad l2 norm  0.06206339\n",
      "iter  3  loss  0.17225997  grad l2 norm  0.061440434\n",
      "iter  4  loss  0.17050748  grad l2 norm  0.061793398\n",
      "iter  5  loss  0.16864784  grad l2 norm  0.06339442\n",
      "iter  6  loss  0.16660519  grad l2 norm  0.06614388\n",
      "iter  7  loss  0.16431867  grad l2 norm  0.07007309\n",
      "iter  8  loss  0.1617877  grad l2 norm  0.07466231\n",
      "iter  9  loss  0.15903734  grad l2 norm  0.07949477\n",
      "iter  10  loss  0.15612721  grad l2 norm  0.084200464\n",
      "iter  11  loss  0.15313171  grad l2 norm  0.08844079\n",
      "iter  12  loss  0.15011  grad l2 norm  0.092045315\n",
      "iter  13  loss  0.14709891  grad l2 norm  0.09498549\n",
      "iter  14  loss  0.14411637  grad l2 norm  0.09731086\n",
      "iter  15  loss  0.14116532  grad l2 norm  0.09910727\n",
      "iter  16  loss  0.13823684  grad l2 norm  0.10047378\n",
      "iter  17  loss  0.1353187  grad l2 norm  0.10149333\n",
      "iter  18  loss  0.13240756  grad l2 norm  0.10220882\n",
      "iter  19  loss  0.1295109  grad l2 norm  0.10263415\n",
      "iter  20  loss  0.12664063  grad l2 norm  0.10277682\n",
      "iter  21  loss  0.12380943  grad l2 norm  0.10264787\n",
      "iter  22  loss  0.12102863  grad l2 norm  0.102264985\n",
      "iter  23  loss  0.11830695  grad l2 norm  0.101652525\n",
      "iter  24  loss  0.11565035  grad l2 norm  0.100837775\n",
      "iter  25  loss  0.113063134  grad l2 norm  0.09984807\n",
      "iter  26  loss  0.11054798  grad l2 norm  0.09871035\n",
      "iter  27  loss  0.108106084  grad l2 norm  0.097451195\n",
      "iter  28  loss  0.10573727  grad l2 norm  0.096096456\n",
      "iter  29  loss  0.10344005  grad l2 norm  0.094670676\n",
      "iter  30  loss  0.101212144  grad l2 norm  0.093196034\n",
      "iter  31  loss  0.09905086  grad l2 norm  0.091691345\n",
      "iter  32  loss  0.09695366  grad l2 norm  0.09017167\n",
      "iter  33  loss  0.09491816  grad l2 norm  0.088649124\n",
      "iter  34  loss  0.09294185  grad l2 norm  0.087133735\n",
      "iter  35  loss  0.091022335  grad l2 norm  0.08563375\n",
      "iter  36  loss  0.089157015  grad l2 norm  0.084155835\n",
      "iter  37  loss  0.087343186  grad l2 norm  0.08270521\n",
      "iter  38  loss  0.08557815  grad l2 norm  0.08128531\n",
      "iter  39  loss  0.0838593  grad l2 norm  0.07989736\n",
      "iter  40  loss  0.08218458  grad l2 norm  0.07854034\n",
      "iter  41  loss  0.08055243  grad l2 norm  0.07721205\n",
      "iter  42  loss  0.07896165  grad l2 norm  0.07591013\n",
      "iter  43  loss  0.077411264  grad l2 norm  0.07463256\n",
      "iter  44  loss  0.075900294  grad l2 norm  0.073377945\n",
      "iter  45  loss  0.07442776  grad l2 norm  0.07214561\n",
      "iter  46  loss  0.072992615  grad l2 norm  0.07093569\n",
      "iter  47  loss  0.07159362  grad l2 norm  0.0697489\n",
      "iter  48  loss  0.070229486  grad l2 norm  0.0685863\n",
      "iter  49  loss  0.06889883  grad l2 norm  0.06744896\n",
      "iter  50  loss  0.06760029  grad l2 norm  0.06633774\n",
      "iter  51  loss  0.06633264  grad l2 norm  0.06525338\n",
      "iter  52  loss  0.06509469  grad l2 norm  0.06419665\n",
      "iter  53  loss  0.063885264  grad l2 norm  0.063168526\n",
      "iter  54  loss  0.06270316  grad l2 norm  0.062170073\n",
      "iter  55  loss  0.061547108  grad l2 norm  0.061202407\n",
      "iter  56  loss  0.06041589  grad l2 norm  0.060266513\n",
      "iter  57  loss  0.059308197  grad l2 norm  0.059363246\n",
      "iter  58  loss  0.058222722  grad l2 norm  0.05849326\n",
      "iter  59  loss  0.05715811  grad l2 norm  0.057657134\n",
      "iter  60  loss  0.056112967  grad l2 norm  0.05685535\n",
      "iter  61  loss  0.05508584  grad l2 norm  0.056088224\n",
      "iter  62  loss  0.054075196  grad l2 norm  0.05535577\n",
      "iter  63  loss  0.05307957  grad l2 norm  0.054657433\n",
      "iter  64  loss  0.052097652  grad l2 norm  0.053992014\n",
      "iter  65  loss  0.051128298  grad l2 norm  0.053357746\n",
      "iter  66  loss  0.050170608  grad l2 norm  0.05275264\n",
      "iter  67  loss  0.049223892  grad l2 norm  0.052174665\n",
      "iter  68  loss  0.048287477  grad l2 norm  0.051621847\n",
      "iter  69  loss  0.04736093  grad l2 norm  0.05109207\n",
      "iter  70  loss  0.04644384  grad l2 norm  0.050583065\n",
      "iter  71  loss  0.045535963  grad l2 norm  0.05009252\n",
      "iter  72  loss  0.04463717  grad l2 norm  0.04961837\n",
      "iter  73  loss  0.04374733  grad l2 norm  0.04915898\n",
      "iter  74  loss  0.042866196  grad l2 norm  0.04871308\n",
      "iter  75  loss  0.041993503  grad l2 norm  0.04827969\n",
      "iter  76  loss  0.041128956  grad l2 norm  0.04785791\n",
      "iter  77  loss  0.0402722  grad l2 norm  0.047446813\n",
      "iter  78  loss  0.039422866  grad l2 norm  0.04704535\n",
      "iter  79  loss  0.03858063  grad l2 norm  0.046652343\n",
      "iter  80  loss  0.037745267  grad l2 norm  0.04626649\n",
      "iter  81  loss  0.03691658  grad l2 norm  0.045886364\n",
      "iter  82  loss  0.036094494  grad l2 norm  0.045510445\n",
      "iter  83  loss  0.035279065  grad l2 norm  0.045137145\n",
      "iter  84  loss  0.034470428  grad l2 norm  0.04476485\n",
      "iter  85  loss  0.033668894  grad l2 norm  0.044392034\n",
      "iter  86  loss  0.03287483  grad l2 norm  0.044017367\n",
      "iter  87  loss  0.032088682  grad l2 norm  0.04363983\n",
      "iter  88  loss  0.031310853  grad l2 norm  0.043258797\n",
      "iter  89  loss  0.030541802  grad l2 norm  0.042874\n",
      "iter  90  loss  0.029781846  grad l2 norm  0.04248551\n",
      "iter  91  loss  0.029031325  grad l2 norm  0.042093597\n",
      "iter  92  loss  0.028290454  grad l2 norm  0.041698705\n",
      "iter  93  loss  0.027559435  grad l2 norm  0.041301403\n",
      "iter  94  loss  0.026838467  grad l2 norm  0.040902425\n",
      "iter  95  loss  0.02612769  grad l2 norm  0.04050271\n",
      "iter  96  loss  0.025427155  grad l2 norm  0.040103406\n",
      "iter  97  loss  0.024736868  grad l2 norm  0.039705873\n",
      "iter  98  loss  0.024056785  grad l2 norm  0.039311618\n",
      "iter  99  loss  0.023386728  grad l2 norm  0.03892224\n",
      "iter  100  loss  0.022726437  grad l2 norm  0.038539357\n",
      "iter  101  loss  0.022075608  grad l2 norm  0.038164567\n",
      "iter  102  loss  0.021433827  grad l2 norm  0.037799332\n",
      "iter  103  loss  0.02080065  grad l2 norm  0.03744489\n",
      "iter  104  loss  0.020175617  grad l2 norm  0.0371022\n",
      "iter  105  loss  0.019558268  grad l2 norm  0.036771834\n",
      "iter  106  loss  0.018948212  grad l2 norm  0.03645401\n",
      "iter  107  loss  0.018345071  grad l2 norm  0.03614859\n",
      "iter  108  loss  0.017748576  grad l2 norm  0.035855133\n",
      "iter  109  loss  0.017158458  grad l2 norm  0.035572905\n",
      "iter  110  loss  0.016574532  grad l2 norm  0.035300944\n",
      "iter  111  loss  0.015996655  grad l2 norm  0.035038076\n",
      "iter  112  loss  0.015424735  grad l2 norm  0.03478302\n",
      "iter  113  loss  0.01485875  grad l2 norm  0.034534458\n",
      "iter  114  loss  0.014298692  grad l2 norm  0.034291096\n",
      "iter  115  loss  0.013744625  grad l2 norm  0.034051742\n",
      "iter  116  loss  0.0131966425  grad l2 norm  0.033815335\n",
      "iter  117  loss  0.012654843  grad l2 norm  0.03358095\n",
      "iter  118  loss  0.012119387  grad l2 norm  0.033347804\n",
      "iter  119  loss  0.011590447  grad l2 norm  0.03311527\n",
      "iter  120  loss  0.011068209  grad l2 norm  0.03288288\n",
      "iter  121  loss  0.010552857  grad l2 norm  0.032650363\n",
      "iter  122  loss  0.010044582  grad l2 norm  0.0324176\n",
      "iter  123  loss  0.009543572  grad l2 norm  0.032184627\n",
      "iter  124  loss  0.0090499185  grad l2 norm  0.0319516\n",
      "iter  125  loss  0.008563741  grad l2 norm  0.03171879\n",
      "iter  126  loss  0.008085092  grad l2 norm  0.031486563\n",
      "iter  127  loss  0.007613974  grad l2 norm  0.03125536\n",
      "iter  128  loss  0.007150329  grad l2 norm  0.031025661\n",
      "iter  129  loss  0.006694067  grad l2 norm  0.03079799\n",
      "iter  130  loss  0.0062449956  grad l2 norm  0.030572848\n",
      "iter  131  loss  0.0058029285  grad l2 norm  0.030350668\n",
      "iter  132  loss  0.0053676115  grad l2 norm  0.030131815\n",
      "iter  133  loss  0.004938764  grad l2 norm  0.029916536\n",
      "iter  134  loss  0.0045160786  grad l2 norm  0.029704923\n",
      "iter  135  loss  0.004099278  grad l2 norm  0.029496947\n",
      "iter  136  loss  0.0036880681  grad l2 norm  0.029292451\n",
      "iter  137  loss  0.0032821973  grad l2 norm  0.029091235\n",
      "iter  138  loss  0.0028813998  grad l2 norm  0.028893048\n",
      "iter  139  loss  0.0024854876  grad l2 norm  0.028697656\n",
      "iter  140  loss  0.002094248  grad l2 norm  0.028504858\n",
      "iter  141  loss  0.0017075199  grad l2 norm  0.028314508\n",
      "iter  142  loss  0.001325124  grad l2 norm  0.02812648\n",
      "iter  143  loss  0.0009469248  grad l2 norm  0.0279407\n",
      "iter  144  loss  0.0005727713  grad l2 norm  0.027757127\n",
      "iter  145  loss  0.00020255493  grad l2 norm  0.027575752\n",
      "iter  146  loss  -0.00016386258  grad l2 norm  0.027396599\n",
      "iter  147  loss  -0.0005265674  grad l2 norm  0.027219713\n",
      "iter  148  loss  -0.0008856676  grad l2 norm  0.027045157\n",
      "iter  149  loss  -0.0012412223  grad l2 norm  0.02687301\n",
      "iter  150  loss  -0.0015933184  grad l2 norm  0.026703337\n",
      "iter  151  loss  -0.001941994  grad l2 norm  0.026536211\n",
      "iter  152  loss  -0.0022872936  grad l2 norm  0.026371693\n",
      "iter  153  loss  -0.0026292228  grad l2 norm  0.026209842\n",
      "iter  154  loss  -0.0029678172  grad l2 norm  0.026050724\n",
      "iter  155  loss  -0.0033030703  grad l2 norm  0.0258944\n",
      "iter  156  loss  -0.003634995  grad l2 norm  0.025740925\n",
      "iter  157  loss  -0.0039635967  grad l2 norm  0.025590362\n",
      "iter  158  loss  -0.0042888755  grad l2 norm  0.025442759\n",
      "iter  159  loss  -0.0046108384  grad l2 norm  0.025298163\n",
      "iter  160  loss  -0.004929502  grad l2 norm  0.025156608\n",
      "iter  161  loss  -0.0052448907  grad l2 norm  0.025018107\n",
      "iter  162  loss  -0.005557027  grad l2 norm  0.024882661\n",
      "iter  163  loss  -0.0058659376  grad l2 norm  0.024750238\n",
      "iter  164  loss  -0.006171672  grad l2 norm  0.024620786\n",
      "iter  165  loss  -0.006474274  grad l2 norm  0.024494225\n",
      "iter  166  loss  -0.006773776  grad l2 norm  0.024370467\n",
      "iter  167  loss  -0.007070232  grad l2 norm  0.024249397\n",
      "iter  168  loss  -0.00736371  grad l2 norm  0.024130886\n",
      "iter  169  loss  -0.007654246  grad l2 norm  0.02401482\n",
      "iter  170  loss  -0.007941909  grad l2 norm  0.02390106\n",
      "iter  171  loss  -0.00822676  grad l2 norm  0.023789467\n",
      "iter  172  loss  -0.008508844  grad l2 norm  0.023679916\n",
      "iter  173  loss  -0.008788244  grad l2 norm  0.023572275\n",
      "iter  174  loss  -0.009065002  grad l2 norm  0.023466418\n",
      "iter  175  loss  -0.009339197  grad l2 norm  0.023362204\n",
      "iter  176  loss  -0.0096108625  grad l2 norm  0.023259526\n",
      "iter  177  loss  -0.00988007  grad l2 norm  0.023158243\n",
      "iter  178  loss  -0.010146865  grad l2 norm  0.023058224\n",
      "iter  179  loss  -0.010411274  grad l2 norm  0.02295933\n",
      "iter  180  loss  -0.010673358  grad l2 norm  0.0228614\n",
      "iter  181  loss  -0.010933132  grad l2 norm  0.022764266\n",
      "iter  182  loss  -0.0111906165  grad l2 norm  0.02266773\n",
      "iter  183  loss  -0.011445825  grad l2 norm  0.022571603\n",
      "iter  184  loss  -0.011698747  grad l2 norm  0.022475692\n",
      "iter  185  loss  -0.011949369  grad l2 norm  0.022379817\n",
      "iter  186  loss  -0.012197685  grad l2 norm  0.02228383\n",
      "iter  187  loss  -0.012443679  grad l2 norm  0.022187607\n",
      "iter  188  loss  -0.01268733  grad l2 norm  0.022091059\n",
      "iter  189  loss  -0.01292863  grad l2 norm  0.021994123\n",
      "iter  190  loss  -0.013167581  grad l2 norm  0.02189676\n",
      "iter  191  loss  -0.013404182  grad l2 norm  0.021798957\n",
      "iter  192  loss  -0.013638431  grad l2 norm  0.021700703\n",
      "iter  193  loss  -0.013870366  grad l2 norm  0.021602008\n",
      "iter  194  loss  -0.014099992  grad l2 norm  0.021502886\n",
      "iter  195  loss  -0.014327351  grad l2 norm  0.021403335\n",
      "iter  196  loss  -0.014552473  grad l2 norm  0.021303391\n",
      "iter  197  loss  -0.014775383  grad l2 norm  0.021203056\n",
      "iter  198  loss  -0.01499614  grad l2 norm  0.021102339\n",
      "iter  199  loss  -0.0152147645  grad l2 norm  0.021001251\n",
      "iter  200  loss  -0.015431285  grad l2 norm  0.020899812\n",
      "iter  201  loss  -0.015645767  grad l2 norm  0.020798028\n",
      "iter  202  loss  -0.015858212  grad l2 norm  0.02069592\n",
      "iter  203  loss  -0.016068676  grad l2 norm  0.020593502\n",
      "iter  204  loss  -0.01627718  grad l2 norm  0.020490808\n",
      "iter  205  loss  -0.016483748  grad l2 norm  0.020387871\n",
      "iter  206  loss  -0.0166884  grad l2 norm  0.02028472\n",
      "iter  207  loss  -0.016891154  grad l2 norm  0.020181399\n",
      "iter  208  loss  -0.017092038  grad l2 norm  0.020077953\n",
      "iter  209  loss  -0.017291073  grad l2 norm  0.019974425\n",
      "iter  210  loss  -0.017488252  grad l2 norm  0.019870864\n",
      "iter  211  loss  -0.017683607  grad l2 norm  0.019767314\n",
      "iter  212  loss  -0.01787715  grad l2 norm  0.019663826\n",
      "iter  213  loss  -0.018068884  grad l2 norm  0.019560436\n",
      "iter  214  loss  -0.018258806  grad l2 norm  0.019457193\n",
      "iter  215  loss  -0.018446943  grad l2 norm  0.019354131\n",
      "iter  216  loss  -0.018633293  grad l2 norm  0.019251283\n",
      "iter  217  loss  -0.018817855  grad l2 norm  0.019148681\n",
      "iter  218  loss  -0.019000644  grad l2 norm  0.019046353\n",
      "iter  219  loss  -0.019181646  grad l2 norm  0.018944327\n",
      "iter  220  loss  -0.019360868  grad l2 norm  0.018842623\n",
      "iter  221  loss  -0.01953832  grad l2 norm  0.018741267\n",
      "iter  222  loss  -0.019713989  grad l2 norm  0.01864027\n",
      "iter  223  loss  -0.019887872  grad l2 norm  0.018539658\n",
      "iter  224  loss  -0.020059966  grad l2 norm  0.018439451\n",
      "iter  225  loss  -0.02023028  grad l2 norm  0.018339662\n",
      "iter  226  loss  -0.020398794  grad l2 norm  0.018240297\n",
      "iter  227  loss  -0.020565525  grad l2 norm  0.018141374\n",
      "iter  228  loss  -0.020730456  grad l2 norm  0.0180429\n",
      "iter  229  loss  -0.020893598  grad l2 norm  0.017944882\n",
      "iter  230  loss  -0.02105494  grad l2 norm  0.017847318\n",
      "iter  231  loss  -0.021214496  grad l2 norm  0.017750205\n",
      "iter  232  loss  -0.021372262  grad l2 norm  0.017653534\n",
      "iter  233  loss  -0.02152825  grad l2 norm  0.017557299\n",
      "iter  234  loss  -0.02168246  grad l2 norm  0.017461482\n",
      "iter  235  loss  -0.021834912  grad l2 norm  0.017366067\n",
      "iter  236  loss  -0.021985611  grad l2 norm  0.017271029\n",
      "iter  237  loss  -0.022134569  grad l2 norm  0.017176347\n",
      "iter  238  loss  -0.0222818  grad l2 norm  0.017081995\n",
      "iter  239  loss  -0.022427332  grad l2 norm  0.016987948\n",
      "iter  240  loss  -0.022571161  grad l2 norm  0.016894177\n",
      "iter  241  loss  -0.022713315  grad l2 norm  0.016800657\n",
      "iter  242  loss  -0.022853805  grad l2 norm  0.016707364\n",
      "iter  243  loss  -0.022992663  grad l2 norm  0.01661428\n",
      "iter  244  loss  -0.023129895  grad l2 norm  0.016521381\n",
      "iter  245  loss  -0.023265526  grad l2 norm  0.016428657\n",
      "iter  246  loss  -0.023399584  grad l2 norm  0.016336102\n",
      "iter  247  loss  -0.023532081  grad l2 norm  0.016243698\n",
      "iter  248  loss  -0.023663038  grad l2 norm  0.01615145\n",
      "iter  249  loss  -0.023792483  grad l2 norm  0.016059348\n",
      "iter  250  loss  -0.02392044  grad l2 norm  0.015967397\n",
      "iter  251  loss  -0.024046931  grad l2 norm  0.015875593\n",
      "iter  252  loss  -0.024171973  grad l2 norm  0.015783936\n",
      "iter  253  loss  -0.024295587  grad l2 norm  0.01569242\n",
      "iter  254  loss  -0.0244178  grad l2 norm  0.015601048\n",
      "iter  255  loss  -0.024538644  grad l2 norm  0.015509806\n",
      "iter  256  loss  -0.024658114  grad l2 norm  0.0154186785\n",
      "iter  257  loss  -0.024776235  grad l2 norm  0.015327653\n",
      "iter  258  loss  -0.02489305  grad l2 norm  0.015236705\n",
      "iter  259  loss  -0.025008535  grad l2 norm  0.015145808\n",
      "iter  260  loss  -0.025122743  grad l2 norm  0.015054925\n",
      "iter  261  loss  -0.025235664  grad l2 norm  0.014964017\n",
      "iter  262  loss  -0.025347319  grad l2 norm  0.014873049\n",
      "iter  263  loss  -0.02545772  grad l2 norm  0.014781968\n",
      "iter  264  loss  -0.025566874  grad l2 norm  0.01469072\n",
      "iter  265  loss  -0.02567479  grad l2 norm  0.014599256\n",
      "iter  266  loss  -0.025781486  grad l2 norm  0.014507523\n",
      "iter  267  loss  -0.025886964  grad l2 norm  0.014415462\n",
      "iter  268  loss  -0.025991239  grad l2 norm  0.014323021\n",
      "iter  269  loss  -0.026094312  grad l2 norm  0.014230152\n",
      "iter  270  loss  -0.026196202  grad l2 norm  0.014136804\n",
      "iter  271  loss  -0.026296888  grad l2 norm  0.014042936\n",
      "iter  272  loss  -0.02639643  grad l2 norm  0.013948513\n",
      "iter  273  loss  -0.026494794  grad l2 norm  0.013853501\n",
      "iter  274  loss  -0.026592012  grad l2 norm  0.013757876\n",
      "iter  275  loss  -0.026688077  grad l2 norm  0.013661617\n",
      "iter  276  loss  -0.026783021  grad l2 norm  0.013564714\n",
      "iter  277  loss  -0.02687683  grad l2 norm  0.013467164\n",
      "iter  278  loss  -0.026969532  grad l2 norm  0.013368964\n",
      "iter  279  loss  -0.027061127  grad l2 norm  0.013270117\n",
      "iter  280  loss  -0.027151637  grad l2 norm  0.0131706325\n",
      "iter  281  loss  -0.027241068  grad l2 norm  0.0130705265\n",
      "iter  282  loss  -0.027329443  grad l2 norm  0.012969813\n",
      "iter  283  loss  -0.02741676  grad l2 norm  0.012868516\n",
      "iter  284  loss  -0.027503049  grad l2 norm  0.012766658\n",
      "iter  285  loss  -0.027588319  grad l2 norm  0.012664268\n",
      "iter  286  loss  -0.027672585  grad l2 norm  0.012561366\n",
      "iter  287  loss  -0.02775585  grad l2 norm  0.012457996\n",
      "iter  288  loss  -0.027838154  grad l2 norm  0.012354179\n",
      "iter  289  loss  -0.027919501  grad l2 norm  0.012249955\n",
      "iter  290  loss  -0.02799992  grad l2 norm  0.012145366\n",
      "iter  291  loss  -0.028079426  grad l2 norm  0.01204045\n",
      "iter  292  loss  -0.028158039  grad l2 norm  0.011935248\n",
      "iter  293  loss  -0.028235776  grad l2 norm  0.011829806\n",
      "iter  294  loss  -0.028312663  grad l2 norm  0.011724171\n",
      "iter  295  loss  -0.028388731  grad l2 norm  0.011618394\n",
      "iter  296  loss  -0.028463986  grad l2 norm  0.011512528\n",
      "iter  297  loss  -0.02853846  grad l2 norm  0.01140663\n",
      "iter  298  loss  -0.02861217  grad l2 norm  0.011300763\n",
      "iter  299  loss  -0.028685145  grad l2 norm  0.011194989\n",
      "iter  300  loss  -0.0287574  grad l2 norm  0.011089375\n",
      "iter  301  loss  -0.028828965  grad l2 norm  0.010983999\n",
      "iter  302  loss  -0.02889986  grad l2 norm  0.010878934\n",
      "iter  303  loss  -0.028970093  grad l2 norm  0.01077426\n",
      "iter  304  loss  -0.029039692  grad l2 norm  0.010670064\n",
      "iter  305  loss  -0.029108679  grad l2 norm  0.010566431\n",
      "iter  306  loss  -0.029177066  grad l2 norm  0.010463448\n",
      "iter  307  loss  -0.029244864  grad l2 norm  0.010361203\n",
      "iter  308  loss  -0.029312083  grad l2 norm  0.010259791\n",
      "iter  309  loss  -0.029378759  grad l2 norm  0.010159299\n",
      "iter  310  loss  -0.029444888  grad l2 norm  0.010059818\n",
      "iter  311  loss  -0.029510476  grad l2 norm  0.009961424\n",
      "iter  312  loss  -0.029575542  grad l2 norm  0.009864206\n",
      "iter  313  loss  -0.029640097  grad l2 norm  0.009768238\n",
      "iter  314  loss  -0.029704137  grad l2 norm  0.009673594\n",
      "iter  315  loss  -0.029767673  grad l2 norm  0.00958033\n",
      "iter  316  loss  -0.029830713  grad l2 norm  0.009488512\n",
      "iter  317  loss  -0.029893266  grad l2 norm  0.009398192\n",
      "iter  318  loss  -0.029955328  grad l2 norm  0.009309421\n",
      "iter  319  loss  -0.0300169  grad l2 norm  0.00922224\n",
      "iter  320  loss  -0.030077994  grad l2 norm  0.009136692\n",
      "iter  321  loss  -0.030138595  grad l2 norm  0.009052814\n",
      "iter  322  loss  -0.030198729  grad l2 norm  0.008970646\n",
      "iter  323  loss  -0.030258376  grad l2 norm  0.008890221\n",
      "iter  324  loss  -0.03031754  grad l2 norm  0.008811573\n",
      "iter  325  loss  -0.030376216  grad l2 norm  0.008734736\n",
      "iter  326  loss  -0.030434411  grad l2 norm  0.008659742\n",
      "iter  327  loss  -0.030492125  grad l2 norm  0.008586624\n",
      "iter  328  loss  -0.03054934  grad l2 norm  0.008515409\n",
      "iter  329  loss  -0.030606061  grad l2 norm  0.008446127\n",
      "iter  330  loss  -0.030662289  grad l2 norm  0.008378805\n",
      "iter  331  loss  -0.030718016  grad l2 norm  0.008313459\n",
      "iter  332  loss  -0.030773228  grad l2 norm  0.008250111\n",
      "iter  333  loss  -0.030827953  grad l2 norm  0.008188767\n",
      "iter  334  loss  -0.03088217  grad l2 norm  0.0081294365\n",
      "iter  335  loss  -0.030935869  grad l2 norm  0.0080721155\n",
      "iter  336  loss  -0.030989079  grad l2 norm  0.008016794\n",
      "iter  337  loss  -0.031041777  grad l2 norm  0.007963454\n",
      "iter  338  loss  -0.031093977  grad l2 norm  0.007912067\n",
      "iter  339  loss  -0.031145677  grad l2 norm  0.007862596\n",
      "iter  340  loss  -0.03119689  grad l2 norm  0.007814991\n",
      "iter  341  loss  -0.031247627  grad l2 norm  0.0077692023\n",
      "iter  342  loss  -0.031297874  grad l2 norm  0.0077251634\n",
      "iter  343  loss  -0.03134765  grad l2 norm  0.007682802\n",
      "iter  344  loss  -0.031396978  grad l2 norm  0.0076420433\n",
      "iter  345  loss  -0.031445842  grad l2 norm  0.0076028057\n",
      "iter  346  loss  -0.031494275  grad l2 norm  0.0075650034\n",
      "iter  347  loss  -0.031542283  grad l2 norm  0.0075285523\n",
      "iter  348  loss  -0.03158987  grad l2 norm  0.0074933656\n",
      "iter  349  loss  -0.031637054  grad l2 norm  0.0074593583\n",
      "iter  350  loss  -0.031683836  grad l2 norm  0.00742645\n",
      "iter  351  loss  -0.03173025  grad l2 norm  0.0073945657\n",
      "iter  352  loss  -0.031776298  grad l2 norm  0.007363633\n",
      "iter  353  loss  -0.03182198  grad l2 norm  0.007333586\n",
      "iter  354  loss  -0.03186734  grad l2 norm  0.0073043704\n",
      "iter  355  loss  -0.031912364  grad l2 norm  0.007275937\n",
      "iter  356  loss  -0.031957097  grad l2 norm  0.0072482424\n",
      "iter  357  loss  -0.032001525  grad l2 norm  0.0072212555\n",
      "iter  358  loss  -0.03204568  grad l2 norm  0.0071949507\n",
      "iter  359  loss  -0.03208959  grad l2 norm  0.007169311\n",
      "iter  360  loss  -0.032133248  grad l2 norm  0.0071443208\n",
      "iter  361  loss  -0.03217668  grad l2 norm  0.0071199727\n",
      "iter  362  loss  -0.03221991  grad l2 norm  0.0070962603\n",
      "iter  363  loss  -0.03226294  grad l2 norm  0.007073178\n",
      "iter  364  loss  -0.032305796  grad l2 norm  0.0070507145\n",
      "iter  365  loss  -0.03234849  grad l2 norm  0.007028853\n",
      "iter  366  loss  -0.032391034  grad l2 norm  0.007007571\n",
      "iter  367  loss  -0.03243343  grad l2 norm  0.0069868327\n",
      "iter  368  loss  -0.0324757  grad l2 norm  0.0069665904\n",
      "iter  369  loss  -0.03251784  grad l2 norm  0.006946785\n",
      "iter  370  loss  -0.032559857  grad l2 norm  0.0069273417\n",
      "iter  371  loss  -0.03260176  grad l2 norm  0.006908176\n",
      "iter  372  loss  -0.032643516  grad l2 norm  0.0068891905\n",
      "iter  373  loss  -0.03268516  grad l2 norm  0.0068702856\n",
      "iter  374  loss  -0.032726668  grad l2 norm  0.0068513476\n",
      "iter  375  loss  -0.032768037  grad l2 norm  0.006832271\n",
      "iter  376  loss  -0.03280924  grad l2 norm  0.0068129445\n",
      "iter  377  loss  -0.032850303  grad l2 norm  0.006793268\n",
      "iter  378  loss  -0.03289119  grad l2 norm  0.0067731407\n",
      "iter  379  loss  -0.032931898  grad l2 norm  0.0067524747\n",
      "iter  380  loss  -0.03297242  grad l2 norm  0.006731189\n",
      "iter  381  loss  -0.033012748  grad l2 norm  0.006709211\n",
      "iter  382  loss  -0.03305286  grad l2 norm  0.0066864877\n",
      "iter  383  loss  -0.03309275  grad l2 norm  0.006662965\n",
      "iter  384  loss  -0.03313242  grad l2 norm  0.0066386084\n",
      "iter  385  loss  -0.033171866  grad l2 norm  0.006613384\n",
      "iter  386  loss  -0.03321105  grad l2 norm  0.006587277\n",
      "iter  387  loss  -0.033249997  grad l2 norm  0.0065602716\n",
      "iter  388  loss  -0.033288687  grad l2 norm  0.0065323664\n",
      "iter  389  loss  -0.033327114  grad l2 norm  0.0065035545\n",
      "iter  390  loss  -0.033365283  grad l2 norm  0.0064738477\n",
      "iter  391  loss  -0.033403173  grad l2 norm  0.0064432505\n",
      "iter  392  loss  -0.033440784  grad l2 norm  0.006411777\n",
      "iter  393  loss  -0.033478107  grad l2 norm  0.006379439\n",
      "iter  394  loss  -0.03351515  grad l2 norm  0.0063462527\n",
      "iter  395  loss  -0.0335519  grad l2 norm  0.0063122464\n",
      "iter  396  loss  -0.03358836  grad l2 norm  0.006277437\n",
      "iter  397  loss  -0.033624507  grad l2 norm  0.0062418557\n",
      "iter  398  loss  -0.033660345  grad l2 norm  0.006205533\n",
      "iter  399  loss  -0.033695873  grad l2 norm  0.006168507\n",
      "iter  400  loss  -0.033731077  grad l2 norm  0.006130811\n",
      "iter  401  loss  -0.033765983  grad l2 norm  0.006092493\n",
      "iter  402  loss  -0.033800546  grad l2 norm  0.006053602\n",
      "iter  403  loss  -0.03383479  grad l2 norm  0.006014183\n",
      "iter  404  loss  -0.0338687  grad l2 norm  0.0059742946\n",
      "iter  405  loss  -0.033902273  grad l2 norm  0.0059339874\n",
      "iter  406  loss  -0.03393553  grad l2 norm  0.0058933212\n",
      "iter  407  loss  -0.03396844  grad l2 norm  0.0058523505\n",
      "iter  408  loss  -0.034001026  grad l2 norm  0.005811129\n",
      "iter  409  loss  -0.034033265  grad l2 norm  0.005769714\n",
      "iter  410  loss  -0.034065172  grad l2 norm  0.005728155\n",
      "iter  411  loss  -0.034096755  grad l2 norm  0.0056865\n",
      "iter  412  loss  -0.03412801  grad l2 norm  0.0056447973\n",
      "iter  413  loss  -0.03415894  grad l2 norm  0.0056030843\n",
      "iter  414  loss  -0.03418955  grad l2 norm  0.0055613983\n",
      "iter  415  loss  -0.034219828  grad l2 norm  0.005519771\n",
      "iter  416  loss  -0.034249797  grad l2 norm  0.005478231\n",
      "iter  417  loss  -0.03427947  grad l2 norm  0.0054368014\n",
      "iter  418  loss  -0.03430882  grad l2 norm  0.005395499\n",
      "iter  419  loss  -0.034337893  grad l2 norm  0.005354343\n",
      "iter  420  loss  -0.034366664  grad l2 norm  0.0053133406\n",
      "iter  421  loss  -0.034395147  grad l2 norm  0.005272504\n",
      "iter  422  loss  -0.034423366  grad l2 norm  0.005231836\n",
      "iter  423  loss  -0.034451295  grad l2 norm  0.005191344\n",
      "iter  424  loss  -0.03447898  grad l2 norm  0.005151025\n",
      "iter  425  loss  -0.034506403  grad l2 norm  0.005110883\n",
      "iter  426  loss  -0.034533564  grad l2 norm  0.0050709173\n",
      "iter  427  loss  -0.034560498  grad l2 norm  0.005031126\n",
      "iter  428  loss  -0.03458719  grad l2 norm  0.0049915095\n",
      "iter  429  loss  -0.034613654  grad l2 norm  0.0049520675\n",
      "iter  430  loss  -0.034639914  grad l2 norm  0.0049128025\n",
      "iter  431  loss  -0.03466595  grad l2 norm  0.0048737153\n",
      "iter  432  loss  -0.03469179  grad l2 norm  0.004834813\n",
      "iter  433  loss  -0.03471743  grad l2 norm  0.0047961017\n",
      "iter  434  loss  -0.034742888  grad l2 norm  0.004757589\n",
      "iter  435  loss  -0.034768153  grad l2 norm  0.0047192923\n",
      "iter  436  loss  -0.034793254  grad l2 norm  0.004681225\n",
      "iter  437  loss  -0.03481818  grad l2 norm  0.0046434077\n",
      "iter  438  loss  -0.034842964  grad l2 norm  0.004605865\n",
      "iter  439  loss  -0.034867566  grad l2 norm  0.0045686257\n",
      "iter  440  loss  -0.034892038  grad l2 norm  0.0045317193\n",
      "iter  441  loss  -0.03491637  grad l2 norm  0.0044951825\n",
      "iter  442  loss  -0.034940545  grad l2 norm  0.0044590556\n",
      "iter  443  loss  -0.034964595  grad l2 norm  0.0044233827\n",
      "iter  444  loss  -0.034988523  grad l2 norm  0.0043882104\n",
      "iter  445  loss  -0.035012312  grad l2 norm  0.0043535875\n",
      "iter  446  loss  -0.035035975  grad l2 norm  0.004319568\n",
      "iter  447  loss  -0.03505952  grad l2 norm  0.0042862077\n",
      "iter  448  loss  -0.03508294  grad l2 norm  0.00425356\n",
      "iter  449  loss  -0.035106253  grad l2 norm  0.0042216866\n",
      "iter  450  loss  -0.03512945  grad l2 norm  0.0041906442\n",
      "iter  451  loss  -0.035152525  grad l2 norm  0.004160493\n",
      "iter  452  loss  -0.0351755  grad l2 norm  0.0041312887\n",
      "iter  453  loss  -0.03519836  grad l2 norm  0.0041030906\n",
      "iter  454  loss  -0.03522112  grad l2 norm  0.0040759533\n",
      "iter  455  loss  -0.03524378  grad l2 norm  0.004049927\n",
      "iter  456  loss  -0.035266336  grad l2 norm  0.004025063\n",
      "iter  457  loss  -0.035288796  grad l2 norm  0.0040014046\n",
      "iter  458  loss  -0.035311174  grad l2 norm  0.0039789956\n",
      "iter  459  loss  -0.035333447  grad l2 norm  0.003957872\n",
      "iter  460  loss  -0.03535565  grad l2 norm  0.0039380644\n",
      "iter  461  loss  -0.035377774  grad l2 norm  0.0039196014\n",
      "iter  462  loss  -0.03539984  grad l2 norm  0.0039025026\n",
      "iter  463  loss  -0.035421807  grad l2 norm  0.0038867865\n",
      "iter  464  loss  -0.03544372  grad l2 norm  0.0038724618\n",
      "iter  465  loss  -0.03546558  grad l2 norm  0.0038595337\n",
      "iter  466  loss  -0.03548738  grad l2 norm  0.0038480018\n",
      "iter  467  loss  -0.035509143  grad l2 norm  0.0038378616\n",
      "iter  468  loss  -0.035530854  grad l2 norm  0.0038291027\n",
      "iter  469  loss  -0.035552543  grad l2 norm  0.0038217076\n",
      "iter  470  loss  -0.035574194  grad l2 norm  0.0038156547\n",
      "iter  471  loss  -0.035595816  grad l2 norm  0.0038109184\n",
      "iter  472  loss  -0.035617415  grad l2 norm  0.0038074697\n",
      "iter  473  loss  -0.035639014  grad l2 norm  0.0038052679\n",
      "iter  474  loss  -0.035660587  grad l2 norm  0.003804274\n",
      "iter  475  loss  -0.035682157  grad l2 norm  0.0038044413\n",
      "iter  476  loss  -0.035703726  grad l2 norm  0.0038057175\n",
      "iter  477  loss  -0.035725273  grad l2 norm  0.0038080513\n",
      "iter  478  loss  -0.035746835  grad l2 norm  0.00381138\n",
      "iter  479  loss  -0.035768386  grad l2 norm  0.0038156435\n",
      "iter  480  loss  -0.03578995  grad l2 norm  0.0038207779\n",
      "iter  481  loss  -0.035811517  grad l2 norm  0.0038267137\n",
      "iter  482  loss  -0.03583308  grad l2 norm  0.003833383\n",
      "iter  483  loss  -0.035854656  grad l2 norm  0.0038407126\n",
      "iter  484  loss  -0.035876237  grad l2 norm  0.003848633\n",
      "iter  485  loss  -0.03589782  grad l2 norm  0.0038570685\n",
      "iter  486  loss  -0.035919398  grad l2 norm  0.0038659489\n",
      "iter  487  loss  -0.035941  grad l2 norm  0.0038751997\n",
      "iter  488  loss  -0.03596259  grad l2 norm  0.00388475\n",
      "iter  489  loss  -0.03598419  grad l2 norm  0.0038945286\n",
      "iter  490  loss  -0.036005773  grad l2 norm  0.0039044637\n",
      "iter  491  loss  -0.036027376  grad l2 norm  0.003914487\n",
      "iter  492  loss  -0.03604898  grad l2 norm  0.0039245314\n",
      "iter  493  loss  -0.036070567  grad l2 norm  0.003934531\n",
      "iter  494  loss  -0.03609216  grad l2 norm  0.0039444193\n",
      "iter  495  loss  -0.03611375  grad l2 norm  0.003954138\n",
      "iter  496  loss  -0.036135335  grad l2 norm  0.0039636274\n",
      "iter  497  loss  -0.036156923  grad l2 norm  0.0039728275\n",
      "iter  498  loss  -0.036178507  grad l2 norm  0.0039816853\n",
      "iter  499  loss  -0.036200084  grad l2 norm  0.003990148\n",
      "iter  500  loss  -0.036221664  grad l2 norm  0.0039981697\n",
      "iter  501  loss  -0.036243234  grad l2 norm  0.0040057\n",
      "iter  502  loss  -0.03626481  grad l2 norm  0.0040126983\n",
      "iter  503  loss  -0.03628637  grad l2 norm  0.004019125\n",
      "iter  504  loss  -0.03630794  grad l2 norm  0.0040249443\n",
      "iter  505  loss  -0.036329493  grad l2 norm  0.0040301247\n",
      "iter  506  loss  -0.036351062  grad l2 norm  0.004034635\n",
      "iter  507  loss  -0.036372613  grad l2 norm  0.0040384526\n",
      "iter  508  loss  -0.036394175  grad l2 norm  0.0040415535\n",
      "iter  509  loss  -0.036415722  grad l2 norm  0.0040439214\n",
      "iter  510  loss  -0.036437284  grad l2 norm  0.0040455395\n",
      "iter  511  loss  -0.036458842  grad l2 norm  0.0040463977\n",
      "iter  512  loss  -0.036480393  grad l2 norm  0.004046485\n",
      "iter  513  loss  -0.03650195  grad l2 norm  0.004045794\n",
      "iter  514  loss  -0.03652351  grad l2 norm  0.0040443176\n",
      "iter  515  loss  -0.03654506  grad l2 norm  0.0040420494\n",
      "iter  516  loss  -0.03656662  grad l2 norm  0.0040389826\n",
      "iter  517  loss  -0.036588162  grad l2 norm  0.0040351143\n",
      "iter  518  loss  -0.036609694  grad l2 norm  0.0040304335\n",
      "iter  519  loss  -0.036631223  grad l2 norm  0.004024931\n",
      "iter  520  loss  -0.03665272  grad l2 norm  0.0040185973\n",
      "iter  521  loss  -0.036674198  grad l2 norm  0.0040114154\n",
      "iter  522  loss  -0.036695648  grad l2 norm  0.0040033753\n",
      "iter  523  loss  -0.03671707  grad l2 norm  0.003994458\n",
      "iter  524  loss  -0.03673844  grad l2 norm  0.003984646\n",
      "iter  525  loss  -0.036759768  grad l2 norm  0.003973924\n",
      "iter  526  loss  -0.03678103  grad l2 norm  0.003962277\n",
      "iter  527  loss  -0.03680224  grad l2 norm  0.00394969\n",
      "iter  528  loss  -0.03682339  grad l2 norm  0.0039361576\n",
      "iter  529  loss  -0.036844455  grad l2 norm  0.0039216685\n",
      "iter  530  loss  -0.036865447  grad l2 norm  0.0039062274\n",
      "iter  531  loss  -0.036886346  grad l2 norm  0.0038898364\n",
      "iter  532  loss  -0.03690717  grad l2 norm  0.0038725093\n",
      "iter  533  loss  -0.03692789  grad l2 norm  0.0038542612\n",
      "iter  534  loss  -0.036948524  grad l2 norm  0.003835119\n",
      "iter  535  loss  -0.036969066  grad l2 norm  0.003815109\n",
      "iter  536  loss  -0.036989503  grad l2 norm  0.0037942685\n",
      "iter  537  loss  -0.037009854  grad l2 norm  0.0037726364\n",
      "iter  538  loss  -0.037030086  grad l2 norm  0.0037502563\n",
      "iter  539  loss  -0.037050225  grad l2 norm  0.0037271779\n",
      "iter  540  loss  -0.037070267  grad l2 norm  0.003703447\n",
      "iter  541  loss  -0.037090227  grad l2 norm  0.0036791197\n",
      "iter  542  loss  -0.037110068  grad l2 norm  0.0036542485\n",
      "iter  543  loss  -0.03712982  grad l2 norm  0.0036288872\n",
      "iter  544  loss  -0.037149474  grad l2 norm  0.0036030922\n",
      "iter  545  loss  -0.037169047  grad l2 norm  0.0035769185\n",
      "iter  546  loss  -0.037188515  grad l2 norm  0.0035504203\n",
      "iter  547  loss  -0.037207905  grad l2 norm  0.0035236499\n",
      "iter  548  loss  -0.03722721  grad l2 norm  0.0034966609\n",
      "iter  549  loss  -0.03724642  grad l2 norm  0.0034695026\n",
      "iter  550  loss  -0.03726555  grad l2 norm  0.0034422206\n",
      "iter  551  loss  -0.037284605  grad l2 norm  0.0034148637\n",
      "iter  552  loss  -0.037303586  grad l2 norm  0.0033874724\n",
      "iter  553  loss  -0.03732247  grad l2 norm  0.003360089\n",
      "iter  554  loss  -0.037341285  grad l2 norm  0.0033327497\n",
      "iter  555  loss  -0.037360024  grad l2 norm  0.0033054939\n",
      "iter  556  loss  -0.037378687  grad l2 norm  0.0032783516\n",
      "iter  557  loss  -0.037397284  grad l2 norm  0.0032513537\n",
      "iter  558  loss  -0.037415802  grad l2 norm  0.0032245312\n",
      "iter  559  loss  -0.03743424  grad l2 norm  0.0031979103\n",
      "iter  560  loss  -0.037452593  grad l2 norm  0.003171518\n",
      "iter  561  loss  -0.03747087  grad l2 norm  0.0031453818\n",
      "iter  562  loss  -0.037489068  grad l2 norm  0.003119526\n",
      "iter  563  loss  -0.037507176  grad l2 norm  0.0030939784\n",
      "iter  564  loss  -0.037525203  grad l2 norm  0.0030687666\n",
      "iter  565  loss  -0.037543144  grad l2 norm  0.0030439224\n",
      "iter  566  loss  -0.037560992  grad l2 norm  0.0030194772\n",
      "iter  567  loss  -0.037578747  grad l2 norm  0.0029954698\n",
      "iter  568  loss  -0.037596397  grad l2 norm  0.0029719388\n",
      "iter  569  loss  -0.037613958  grad l2 norm  0.0029489284\n",
      "iter  570  loss  -0.03763141  grad l2 norm  0.002926487\n",
      "iter  571  loss  -0.037648764  grad l2 norm  0.0029046652\n",
      "iter  572  loss  -0.037666008  grad l2 norm  0.0028835216\n",
      "iter  573  loss  -0.037683133  grad l2 norm  0.002863111\n",
      "iter  574  loss  -0.03770015  grad l2 norm  0.0028434936\n",
      "iter  575  loss  -0.03771707  grad l2 norm  0.0028247316\n",
      "iter  576  loss  -0.03773388  grad l2 norm  0.002806885\n",
      "iter  577  loss  -0.03775057  grad l2 norm  0.0027900136\n",
      "iter  578  loss  -0.03776716  grad l2 norm  0.0027741764\n",
      "iter  579  loss  -0.037783623  grad l2 norm  0.0027594278\n",
      "iter  580  loss  -0.037799988  grad l2 norm  0.0027458195\n",
      "iter  581  loss  -0.037816253  grad l2 norm  0.0027333952\n",
      "iter  582  loss  -0.037832413  grad l2 norm  0.0027221972\n",
      "iter  583  loss  -0.037848476  grad l2 norm  0.002712261\n",
      "iter  584  loss  -0.037864428  grad l2 norm  0.002703611\n",
      "iter  585  loss  -0.037880294  grad l2 norm  0.00269627\n",
      "iter  586  loss  -0.03789607  grad l2 norm  0.0026902487\n",
      "iter  587  loss  -0.03791176  grad l2 norm  0.0026855555\n",
      "iter  588  loss  -0.037927367  grad l2 norm  0.0026821853\n",
      "iter  589  loss  -0.0379429  grad l2 norm  0.0026801315\n",
      "iter  590  loss  -0.037958365  grad l2 norm  0.002679379\n",
      "iter  591  loss  -0.03797375  grad l2 norm  0.002679905\n",
      "iter  592  loss  -0.03798909  grad l2 norm  0.002681682\n",
      "iter  593  loss  -0.038004365  grad l2 norm  0.002684675\n",
      "iter  594  loss  -0.038019594  grad l2 norm  0.0026888459\n",
      "iter  595  loss  -0.038034774  grad l2 norm  0.002694151\n",
      "iter  596  loss  -0.0380499  grad l2 norm  0.0027005437\n",
      "iter  597  loss  -0.03806501  grad l2 norm  0.0027079731\n",
      "iter  598  loss  -0.038080078  grad l2 norm  0.0027163848\n",
      "iter  599  loss  -0.03809514  grad l2 norm  0.0027257216\n",
      "iter  600  loss  -0.03811017  grad l2 norm  0.0027359268\n",
      "iter  601  loss  -0.03812518  grad l2 norm  0.002746939\n",
      "iter  602  loss  -0.038140204  grad l2 norm  0.0027586957\n",
      "iter  603  loss  -0.038155198  grad l2 norm  0.0027711357\n",
      "iter  604  loss  -0.03817021  grad l2 norm  0.002784195\n",
      "iter  605  loss  -0.038185216  grad l2 norm  0.0027978083\n",
      "iter  606  loss  -0.03820024  grad l2 norm  0.0028119136\n",
      "iter  607  loss  -0.038215265  grad l2 norm  0.0028264457\n",
      "iter  608  loss  -0.038230315  grad l2 norm  0.00284134\n",
      "iter  609  loss  -0.038245372  grad l2 norm  0.0028565351\n",
      "iter  610  loss  -0.038260456  grad l2 norm  0.0028719674\n",
      "iter  611  loss  -0.038275566  grad l2 norm  0.0028875757\n",
      "iter  612  loss  -0.038290694  grad l2 norm  0.002903299\n",
      "iter  613  loss  -0.03830584  grad l2 norm  0.0029190767\n",
      "iter  614  loss  -0.03832103  grad l2 norm  0.0029348535\n",
      "iter  615  loss  -0.03833623  grad l2 norm  0.0029505705\n",
      "iter  616  loss  -0.038351454  grad l2 norm  0.002966172\n",
      "iter  617  loss  -0.038366716  grad l2 norm  0.0029816062\n",
      "iter  618  loss  -0.038382005  grad l2 norm  0.0029968217\n",
      "iter  619  loss  -0.038397316  grad l2 norm  0.0030117682\n",
      "iter  620  loss  -0.03841266  grad l2 norm  0.0030263949\n",
      "iter  621  loss  -0.038428023  grad l2 norm  0.003040659\n",
      "iter  622  loss  -0.03844342  grad l2 norm  0.0030545166\n",
      "iter  623  loss  -0.03845885  grad l2 norm  0.0030679244\n",
      "iter  624  loss  -0.038474303  grad l2 norm  0.0030808435\n",
      "iter  625  loss  -0.038489785  grad l2 norm  0.0030932396\n",
      "iter  626  loss  -0.038505297  grad l2 norm  0.0031050749\n",
      "iter  627  loss  -0.038520835  grad l2 norm  0.0031163222\n",
      "iter  628  loss  -0.038536415  grad l2 norm  0.0031269519\n",
      "iter  629  loss  -0.038552023  grad l2 norm  0.003136937\n",
      "iter  630  loss  -0.038567662  grad l2 norm  0.0031462589\n",
      "iter  631  loss  -0.03858333  grad l2 norm  0.0031548934\n",
      "iter  632  loss  -0.038599044  grad l2 norm  0.003162828\n",
      "iter  633  loss  -0.038614787  grad l2 norm  0.003170051\n",
      "iter  634  loss  -0.038630567  grad l2 norm  0.0031765522\n",
      "iter  635  loss  -0.038646404  grad l2 norm  0.0031823262\n",
      "iter  636  loss  -0.03866226  grad l2 norm  0.0031873665\n",
      "iter  637  loss  -0.03867818  grad l2 norm  0.0031916765\n",
      "iter  638  loss  -0.03869413  grad l2 norm  0.0031952583\n",
      "iter  639  loss  -0.03871013  grad l2 norm  0.0031981154\n",
      "iter  640  loss  -0.03872617  grad l2 norm  0.0032002528\n",
      "iter  641  loss  -0.038742263  grad l2 norm  0.0032016824\n",
      "iter  642  loss  -0.03875841  grad l2 norm  0.0032024095\n",
      "iter  643  loss  -0.0387746  grad l2 norm  0.003202445\n",
      "iter  644  loss  -0.038790826  grad l2 norm  0.003201801\n",
      "iter  645  loss  -0.038807116  grad l2 norm  0.0032004856\n",
      "iter  646  loss  -0.03882345  grad l2 norm  0.0031985072\n",
      "iter  647  loss  -0.038839854  grad l2 norm  0.0031958786\n",
      "iter  648  loss  -0.03885628  grad l2 norm  0.0031926052\n",
      "iter  649  loss  -0.03887276  grad l2 norm  0.003188696\n",
      "iter  650  loss  -0.03888929  grad l2 norm  0.0031841584\n",
      "iter  651  loss  -0.03890588  grad l2 norm  0.0031789977\n",
      "iter  652  loss  -0.03892249  grad l2 norm  0.003173221\n",
      "iter  653  loss  -0.038939144  grad l2 norm  0.0031668376\n",
      "iter  654  loss  -0.03895584  grad l2 norm  0.003159853\n",
      "iter  655  loss  -0.03897258  grad l2 norm  0.0031522813\n",
      "iter  656  loss  -0.038989346  grad l2 norm  0.0031441343\n",
      "iter  657  loss  -0.039006144  grad l2 norm  0.0031354302\n",
      "iter  658  loss  -0.03902296  grad l2 norm  0.003126193\n",
      "iter  659  loss  -0.0390398  grad l2 norm  0.0031164503\n",
      "iter  660  loss  -0.039056662  grad l2 norm  0.0031062383\n",
      "iter  661  loss  -0.039073545  grad l2 norm  0.0030955991\n",
      "iter  662  loss  -0.039090436  grad l2 norm  0.0030845809\n",
      "iter  663  loss  -0.03910733  grad l2 norm  0.0030732437\n",
      "iter  664  loss  -0.03912424  grad l2 norm  0.003061648\n",
      "iter  665  loss  -0.03914115  grad l2 norm  0.0030498689\n",
      "iter  666  loss  -0.03915807  grad l2 norm  0.0030379812\n",
      "iter  667  loss  -0.03917498  grad l2 norm  0.003026067\n",
      "iter  668  loss  -0.039191898  grad l2 norm  0.0030142143\n",
      "iter  669  loss  -0.039208803  grad l2 norm  0.0030025116\n",
      "iter  670  loss  -0.039225724  grad l2 norm  0.0029910493\n",
      "iter  671  loss  -0.03924264  grad l2 norm  0.0029799193\n",
      "iter  672  loss  -0.03925958  grad l2 norm  0.0029692117\n",
      "iter  673  loss  -0.03927649  grad l2 norm  0.0029590144\n",
      "iter  674  loss  -0.039293427  grad l2 norm  0.0029494097\n",
      "iter  675  loss  -0.039310366  grad l2 norm  0.002940479\n",
      "iter  676  loss  -0.039327316  grad l2 norm  0.002932296\n",
      "iter  677  loss  -0.039344277  grad l2 norm  0.0029249294\n",
      "iter  678  loss  -0.039361257  grad l2 norm  0.0029184413\n",
      "iter  679  loss  -0.03937826  grad l2 norm  0.0029128857\n",
      "iter  680  loss  -0.039395288  grad l2 norm  0.0029083122\n",
      "iter  681  loss  -0.03941235  grad l2 norm  0.0029047627\n",
      "iter  682  loss  -0.039429422  grad l2 norm  0.0029022705\n",
      "iter  683  loss  -0.039446548  grad l2 norm  0.0029008617\n",
      "iter  684  loss  -0.039463706  grad l2 norm  0.0029005588\n",
      "iter  685  loss  -0.03948091  grad l2 norm  0.0029013758\n",
      "iter  686  loss  -0.03949814  grad l2 norm  0.0029033208\n",
      "iter  687  loss  -0.039515413  grad l2 norm  0.0029063954\n",
      "iter  688  loss  -0.03953275  grad l2 norm  0.002910596\n",
      "iter  689  loss  -0.03955012  grad l2 norm  0.002915912\n",
      "iter  690  loss  -0.039567545  grad l2 norm  0.0029223303\n",
      "iter  691  loss  -0.03958503  grad l2 norm  0.0029298302\n",
      "iter  692  loss  -0.03960257  grad l2 norm  0.0029383868\n",
      "iter  693  loss  -0.03962017  grad l2 norm  0.0029479708\n",
      "iter  694  loss  -0.03963782  grad l2 norm  0.0029585476\n",
      "iter  695  loss  -0.03965553  grad l2 norm  0.00297008\n",
      "iter  696  loss  -0.039673295  grad l2 norm  0.0029825268\n",
      "iter  697  loss  -0.039691135  grad l2 norm  0.0029958412\n",
      "iter  698  loss  -0.039709035  grad l2 norm  0.0030099761\n",
      "iter  699  loss  -0.039726984  grad l2 norm  0.0030248782\n",
      "iter  700  loss  -0.039745  grad l2 norm  0.003040495\n",
      "iter  701  loss  -0.039763086  grad l2 norm  0.00305677\n",
      "iter  702  loss  -0.039781235  grad l2 norm  0.0030736434\n",
      "iter  703  loss  -0.03979944  grad l2 norm  0.003091058\n",
      "iter  704  loss  -0.039817687  grad l2 norm  0.003108953\n",
      "iter  705  loss  -0.039836023  grad l2 norm  0.003127266\n",
      "iter  706  loss  -0.03985441  grad l2 norm  0.0031459373\n",
      "iter  707  loss  -0.039872855  grad l2 norm  0.0031649047\n",
      "iter  708  loss  -0.039891362  grad l2 norm  0.0031841085\n",
      "iter  709  loss  -0.039909918  grad l2 norm  0.003203489\n",
      "iter  710  loss  -0.039928526  grad l2 norm  0.0032229873\n",
      "iter  711  loss  -0.039947182  grad l2 norm  0.0032425465\n",
      "iter  712  loss  -0.03996591  grad l2 norm  0.003262111\n",
      "iter  713  loss  -0.03998465  grad l2 norm  0.003281625\n",
      "iter  714  loss  -0.040003467  grad l2 norm  0.003301039\n",
      "iter  715  loss  -0.04002232  grad l2 norm  0.0033203\n",
      "iter  716  loss  -0.040041205  grad l2 norm  0.0033393616\n",
      "iter  717  loss  -0.040060144  grad l2 norm  0.003358175\n",
      "iter  718  loss  -0.040079106  grad l2 norm  0.003376697\n",
      "iter  719  loss  -0.04009811  grad l2 norm  0.003394884\n",
      "iter  720  loss  -0.04011714  grad l2 norm  0.0034126954\n",
      "iter  721  loss  -0.040136218  grad l2 norm  0.0034300915\n",
      "iter  722  loss  -0.04015531  grad l2 norm  0.0034470367\n",
      "iter  723  loss  -0.04017443  grad l2 norm  0.003463494\n",
      "iter  724  loss  -0.04019359  grad l2 norm  0.0034794307\n",
      "iter  725  loss  -0.04021276  grad l2 norm  0.0034948147\n",
      "iter  726  loss  -0.04023195  grad l2 norm  0.003509615\n",
      "iter  727  loss  -0.040251166  grad l2 norm  0.003523805\n",
      "iter  728  loss  -0.040270403  grad l2 norm  0.0035373564\n",
      "iter  729  loss  -0.04028965  grad l2 norm  0.0035502464\n",
      "iter  730  loss  -0.040308926  grad l2 norm  0.0035624492\n",
      "iter  731  loss  -0.040328205  grad l2 norm  0.0035739478\n",
      "iter  732  loss  -0.040347505  grad l2 norm  0.0035847218\n",
      "iter  733  loss  -0.040366814  grad l2 norm  0.003594754\n",
      "iter  734  loss  -0.040386137  grad l2 norm  0.0036040326\n",
      "iter  735  loss  -0.040405467  grad l2 norm  0.003612544\n",
      "iter  736  loss  -0.04042481  grad l2 norm  0.0036202793\n",
      "iter  737  loss  -0.040444165  grad l2 norm  0.003627232\n",
      "iter  738  loss  -0.040463533  grad l2 norm  0.0036333976\n",
      "iter  739  loss  -0.04048291  grad l2 norm  0.003638775\n",
      "iter  740  loss  -0.04050229  grad l2 norm  0.0036433623\n",
      "iter  741  loss  -0.04052168  grad l2 norm  0.0036471633\n",
      "iter  742  loss  -0.040541086  grad l2 norm  0.0036501808\n",
      "iter  743  loss  -0.040560488  grad l2 norm  0.0036524234\n",
      "iter  744  loss  -0.04057989  grad l2 norm  0.0036538967\n",
      "iter  745  loss  -0.040599324  grad l2 norm  0.003654609\n",
      "iter  746  loss  -0.040618744  grad l2 norm  0.0036545736\n",
      "iter  747  loss  -0.04063817  grad l2 norm  0.0036537973\n",
      "iter  748  loss  -0.0406576  grad l2 norm  0.0036522935\n",
      "iter  749  loss  -0.040677052  grad l2 norm  0.0036500755\n",
      "iter  750  loss  -0.040696483  grad l2 norm  0.0036471547\n",
      "iter  751  loss  -0.04071593  grad l2 norm  0.0036435428\n",
      "iter  752  loss  -0.040735368  grad l2 norm  0.0036392529\n",
      "iter  753  loss  -0.04075481  grad l2 norm  0.0036342978\n",
      "iter  754  loss  -0.040774252  grad l2 norm  0.003628694\n",
      "iter  755  loss  -0.040793683  grad l2 norm  0.0036224546\n",
      "iter  756  loss  -0.040813103  grad l2 norm  0.0036155928\n",
      "iter  757  loss  -0.040832523  grad l2 norm  0.0036081276\n",
      "iter  758  loss  -0.040851936  grad l2 norm  0.0036000733\n",
      "iter  759  loss  -0.04087131  grad l2 norm  0.0035914509\n",
      "iter  760  loss  -0.040890686  grad l2 norm  0.0035822804\n",
      "iter  761  loss  -0.04091003  grad l2 norm  0.0035725834\n",
      "iter  762  loss  -0.04092935  grad l2 norm  0.0035623854\n",
      "iter  763  loss  -0.04094864  grad l2 norm  0.0035517102\n",
      "iter  764  loss  -0.040967904  grad l2 norm  0.0035405916\n",
      "iter  765  loss  -0.040987127  grad l2 norm  0.0035290567\n",
      "iter  766  loss  -0.041006308  grad l2 norm  0.0035171444\n",
      "iter  767  loss  -0.041025445  grad l2 norm  0.0035048903\n",
      "iter  768  loss  -0.04104453  grad l2 norm  0.003492335\n",
      "iter  769  loss  -0.04106357  grad l2 norm  0.003479523\n",
      "iter  770  loss  -0.04108255  grad l2 norm  0.0034664997\n",
      "iter  771  loss  -0.041101474  grad l2 norm  0.0034533162\n",
      "iter  772  loss  -0.041120335  grad l2 norm  0.0034400239\n",
      "iter  773  loss  -0.04113913  grad l2 norm  0.003426676\n",
      "iter  774  loss  -0.041157875  grad l2 norm  0.003413332\n",
      "iter  775  loss  -0.04117652  grad l2 norm  0.0034000496\n",
      "iter  776  loss  -0.041195102  grad l2 norm  0.0033868894\n",
      "iter  777  loss  -0.041213617  grad l2 norm  0.0033739146\n",
      "iter  778  loss  -0.041232046  grad l2 norm  0.0033611846\n",
      "iter  779  loss  -0.0412504  grad l2 norm  0.003348765\n",
      "iter  780  loss  -0.04126868  grad l2 norm  0.0033367192\n",
      "iter  781  loss  -0.041286882  grad l2 norm  0.0033251075\n",
      "iter  782  loss  -0.04130499  grad l2 norm  0.0033139915\n",
      "iter  783  loss  -0.041323025  grad l2 norm  0.0033034284\n",
      "iter  784  loss  -0.041340984  grad l2 norm  0.003293475\n",
      "iter  785  loss  -0.04135886  grad l2 norm  0.003284184\n",
      "iter  786  loss  -0.041376665  grad l2 norm  0.0032756045\n",
      "iter  787  loss  -0.04139438  grad l2 norm  0.0032677795\n",
      "iter  788  loss  -0.04141202  grad l2 norm  0.00326075\n",
      "iter  789  loss  -0.041429587  grad l2 norm  0.00325455\n",
      "iter  790  loss  -0.04144709  grad l2 norm  0.0032492082\n",
      "iter  791  loss  -0.0414645  grad l2 norm  0.003244746\n",
      "iter  792  loss  -0.041481853  grad l2 norm  0.0032411804\n",
      "iter  793  loss  -0.04149913  grad l2 norm  0.0032385194\n",
      "iter  794  loss  -0.041516356  grad l2 norm  0.0032367657\n",
      "iter  795  loss  -0.041533504  grad l2 norm  0.0032359154\n",
      "iter  796  loss  -0.0415506  grad l2 norm  0.0032359574\n",
      "iter  797  loss  -0.041567627  grad l2 norm  0.003236873\n",
      "iter  798  loss  -0.04158459  grad l2 norm  0.0032386382\n",
      "iter  799  loss  -0.04160151  grad l2 norm  0.0032412247\n",
      "iter  800  loss  -0.04161836  grad l2 norm  0.003244595\n",
      "iter  801  loss  -0.041635167  grad l2 norm  0.0032487104\n",
      "iter  802  loss  -0.041651916  grad l2 norm  0.0032535256\n",
      "iter  803  loss  -0.041668624  grad l2 norm  0.0032589913\n",
      "iter  804  loss  -0.041685272  grad l2 norm  0.0032650558\n",
      "iter  805  loss  -0.041701876  grad l2 norm  0.0032716664\n",
      "iter  806  loss  -0.04171842  grad l2 norm  0.0032787644\n",
      "iter  807  loss  -0.041734934  grad l2 norm  0.0032862918\n",
      "iter  808  loss  -0.041751407  grad l2 norm  0.0032941927\n",
      "iter  809  loss  -0.04176784  grad l2 norm  0.0033024081\n",
      "iter  810  loss  -0.041784223  grad l2 norm  0.0033108809\n",
      "iter  811  loss  -0.041800566  grad l2 norm  0.0033195538\n",
      "iter  812  loss  -0.041816868  grad l2 norm  0.003328374\n",
      "iter  813  loss  -0.041833144  grad l2 norm  0.003337288\n",
      "iter  814  loss  -0.041849375  grad l2 norm  0.0033462448\n",
      "iter  815  loss  -0.041865572  grad l2 norm  0.003355199\n",
      "iter  816  loss  -0.041881733  grad l2 norm  0.0033641076\n",
      "iter  817  loss  -0.04189787  grad l2 norm  0.003372929\n",
      "iter  818  loss  -0.041913986  grad l2 norm  0.0033816246\n",
      "iter  819  loss  -0.04193006  grad l2 norm  0.0033901604\n",
      "iter  820  loss  -0.041946106  grad l2 norm  0.0033985071\n",
      "iter  821  loss  -0.04196213  grad l2 norm  0.0034066318\n",
      "iter  822  loss  -0.041978113  grad l2 norm  0.0034145105\n",
      "iter  823  loss  -0.041994087  grad l2 norm  0.0034221192\n",
      "iter  824  loss  -0.04201004  grad l2 norm  0.0034294324\n",
      "iter  825  loss  -0.042025965  grad l2 norm  0.0034364322\n",
      "iter  826  loss  -0.04204188  grad l2 norm  0.0034430944\n",
      "iter  827  loss  -0.042057745  grad l2 norm  0.003449401\n",
      "iter  828  loss  -0.04207363  grad l2 norm  0.0034553288\n",
      "iter  829  loss  -0.042089477  grad l2 norm  0.0034608615\n",
      "iter  830  loss  -0.04210532  grad l2 norm  0.0034659754\n",
      "iter  831  loss  -0.042121135  grad l2 norm  0.0034706485\n",
      "iter  832  loss  -0.042136934  grad l2 norm  0.0034748598\n",
      "iter  833  loss  -0.04215272  grad l2 norm  0.003478583\n",
      "iter  834  loss  -0.042168498  grad l2 norm  0.0034817932\n",
      "iter  835  loss  -0.04218424  grad l2 norm  0.003484465\n",
      "iter  836  loss  -0.042199973  grad l2 norm  0.003486571\n",
      "iter  837  loss  -0.0422157  grad l2 norm  0.00348808\n",
      "iter  838  loss  -0.042231396  grad l2 norm  0.003488967\n",
      "iter  839  loss  -0.042247068  grad l2 norm  0.0034892012\n",
      "iter  840  loss  -0.042262714  grad l2 norm  0.0034887525\n",
      "iter  841  loss  -0.042278342  grad l2 norm  0.0034875933\n",
      "iter  842  loss  -0.042293936  grad l2 norm  0.0034856962\n",
      "iter  843  loss  -0.0423095  grad l2 norm  0.003483031\n",
      "iter  844  loss  -0.042325042  grad l2 norm  0.0034795746\n",
      "iter  845  loss  -0.04234054  grad l2 norm  0.0034753026\n",
      "iter  846  loss  -0.042355996  grad l2 norm  0.0034701934\n",
      "iter  847  loss  -0.04237141  grad l2 norm  0.003464226\n",
      "iter  848  loss  -0.042386774  grad l2 norm  0.0034573863\n",
      "iter  849  loss  -0.04240209  grad l2 norm  0.0034496584\n",
      "iter  850  loss  -0.04241736  grad l2 norm  0.0034410323\n",
      "iter  851  loss  -0.04243255  grad l2 norm  0.003431502\n",
      "iter  852  loss  -0.0424477  grad l2 norm  0.0034210589\n",
      "iter  853  loss  -0.04246277  grad l2 norm  0.0034097035\n",
      "iter  854  loss  -0.04247778  grad l2 norm  0.003397437\n",
      "iter  855  loss  -0.042492725  grad l2 norm  0.0033842642\n",
      "iter  856  loss  -0.04250758  grad l2 norm  0.0033701903\n",
      "iter  857  loss  -0.04252236  grad l2 norm  0.003355227\n",
      "iter  858  loss  -0.04253705  grad l2 norm  0.0033393851\n",
      "iter  859  loss  -0.042551666  grad l2 norm  0.0033226789\n",
      "iter  860  loss  -0.04256619  grad l2 norm  0.0033051244\n",
      "iter  861  loss  -0.04258061  grad l2 norm  0.00328674\n",
      "iter  862  loss  -0.042594936  grad l2 norm  0.0032675464\n",
      "iter  863  loss  -0.042609163  grad l2 norm  0.0032475656\n",
      "iter  864  loss  -0.042623285  grad l2 norm  0.0032268176\n",
      "iter  865  loss  -0.042637322  grad l2 norm  0.0032053313\n",
      "iter  866  loss  -0.042651232  grad l2 norm  0.003183129\n",
      "iter  867  loss  -0.04266505  grad l2 norm  0.003160238\n",
      "iter  868  loss  -0.042678747  grad l2 norm  0.0031366872\n",
      "iter  869  loss  -0.04269233  grad l2 norm  0.0031125054\n",
      "iter  870  loss  -0.042705815  grad l2 norm  0.0030877239\n",
      "iter  871  loss  -0.04271917  grad l2 norm  0.003062374\n",
      "iter  872  loss  -0.04273241  grad l2 norm  0.0030364871\n",
      "iter  873  loss  -0.04274554  grad l2 norm  0.003010098\n",
      "iter  874  loss  -0.042758547  grad l2 norm  0.002983241\n",
      "iter  875  loss  -0.042771447  grad l2 norm  0.0029559522\n",
      "iter  876  loss  -0.04278421  grad l2 norm  0.002928267\n",
      "iter  877  loss  -0.042796865  grad l2 norm  0.0029002253\n",
      "iter  878  loss  -0.0428094  grad l2 norm  0.002871865\n",
      "iter  879  loss  -0.042821813  grad l2 norm  0.0028432258\n",
      "iter  880  loss  -0.042834107  grad l2 norm  0.0028143455\n",
      "iter  881  loss  -0.04284629  grad l2 norm  0.0027852682\n",
      "iter  882  loss  -0.042858336  grad l2 norm  0.0027560347\n",
      "iter  883  loss  -0.042870265  grad l2 norm  0.0027266878\n",
      "iter  884  loss  -0.042882085  grad l2 norm  0.00269727\n",
      "iter  885  loss  -0.042893775  grad l2 norm  0.0026678245\n",
      "iter  886  loss  -0.04290536  grad l2 norm  0.002638397\n",
      "iter  887  loss  -0.042916816  grad l2 norm  0.002609029\n",
      "iter  888  loss  -0.042928156  grad l2 norm  0.0025797663\n",
      "iter  889  loss  -0.04293939  grad l2 norm  0.0025506562\n",
      "iter  890  loss  -0.042950504  grad l2 norm  0.002521741\n",
      "iter  891  loss  -0.0429615  grad l2 norm  0.0024930658\n",
      "iter  892  loss  -0.042972382  grad l2 norm  0.002464676\n",
      "iter  893  loss  -0.04298316  grad l2 norm  0.002436617\n",
      "iter  894  loss  -0.04299383  grad l2 norm  0.0024089292\n",
      "iter  895  loss  -0.043004382  grad l2 norm  0.0023816607\n",
      "iter  896  loss  -0.043014843  grad l2 norm  0.0023548536\n",
      "iter  897  loss  -0.043025192  grad l2 norm  0.0023285486\n",
      "iter  898  loss  -0.04303544  grad l2 norm  0.002302788\n",
      "iter  899  loss  -0.043045588  grad l2 norm  0.0022776127\n",
      "iter  900  loss  -0.04305563  grad l2 norm  0.0022530607\n",
      "iter  901  loss  -0.04306558  grad l2 norm  0.0022291725\n",
      "iter  902  loss  -0.043075453  grad l2 norm  0.0022059842\n",
      "iter  903  loss  -0.043085225  grad l2 norm  0.0021835314\n",
      "iter  904  loss  -0.043094918  grad l2 norm  0.0021618481\n",
      "iter  905  loss  -0.04310453  grad l2 norm  0.0021409672\n",
      "iter  906  loss  -0.043114055  grad l2 norm  0.002120921\n",
      "iter  907  loss  -0.043123506  grad l2 norm  0.0021017375\n",
      "iter  908  loss  -0.04313287  grad l2 norm  0.0020834457\n",
      "iter  909  loss  -0.04314218  grad l2 norm  0.0020660684\n",
      "iter  910  loss  -0.043151412  grad l2 norm  0.0020496321\n",
      "iter  911  loss  -0.043160576  grad l2 norm  0.0020341552\n",
      "iter  912  loss  -0.04316969  grad l2 norm  0.0020196592\n",
      "iter  913  loss  -0.04317873  grad l2 norm  0.0020061578\n",
      "iter  914  loss  -0.04318773  grad l2 norm  0.0019936636\n",
      "iter  915  loss  -0.043196663  grad l2 norm  0.0019821872\n",
      "iter  916  loss  -0.04320556  grad l2 norm  0.0019717345\n",
      "iter  917  loss  -0.043214403  grad l2 norm  0.00196231\n",
      "iter  918  loss  -0.043223202  grad l2 norm  0.0019539106\n",
      "iter  919  loss  -0.043231964  grad l2 norm  0.001946532\n",
      "iter  920  loss  -0.043240685  grad l2 norm  0.0019401658\n",
      "iter  921  loss  -0.043249372  grad l2 norm  0.0019347989\n",
      "iter  922  loss  -0.043258037  grad l2 norm  0.0019304125\n",
      "iter  923  loss  -0.04326666  grad l2 norm  0.0019269859\n",
      "iter  924  loss  -0.043275263  grad l2 norm  0.0019244939\n",
      "iter  925  loss  -0.04328386  grad l2 norm  0.0019229073\n",
      "iter  926  loss  -0.043292418  grad l2 norm  0.0019221919\n",
      "iter  927  loss  -0.04330097  grad l2 norm  0.0019223103\n",
      "iter  928  loss  -0.043309506  grad l2 norm  0.0019232219\n",
      "iter  929  loss  -0.043318033  grad l2 norm  0.0019248843\n",
      "iter  930  loss  -0.043326538  grad l2 norm  0.0019272505\n",
      "iter  931  loss  -0.04333505  grad l2 norm  0.001930272\n",
      "iter  932  loss  -0.043343555  grad l2 norm  0.0019338977\n",
      "iter  933  loss  -0.04335205  grad l2 norm  0.001938076\n",
      "iter  934  loss  -0.04336055  grad l2 norm  0.0019427517\n",
      "iter  935  loss  -0.043369047  grad l2 norm  0.0019478706\n",
      "iter  936  loss  -0.043377545  grad l2 norm  0.0019533776\n",
      "iter  937  loss  -0.04338605  grad l2 norm  0.0019592175\n",
      "iter  938  loss  -0.043394558  grad l2 norm  0.001965334\n",
      "iter  939  loss  -0.04340307  grad l2 norm  0.0019716725\n",
      "iter  940  loss  -0.04341159  grad l2 norm  0.00197818\n",
      "iter  941  loss  -0.043420132  grad l2 norm  0.0019848035\n",
      "iter  942  loss  -0.043428663  grad l2 norm  0.001991493\n",
      "iter  943  loss  -0.04343722  grad l2 norm  0.0019982003\n",
      "iter  944  loss  -0.043445785  grad l2 norm  0.002004879\n",
      "iter  945  loss  -0.04345435  grad l2 norm  0.002011487\n",
      "iter  946  loss  -0.043462936  grad l2 norm  0.002017984\n",
      "iter  947  loss  -0.043471508  grad l2 norm  0.0020243335\n",
      "iter  948  loss  -0.043480117  grad l2 norm  0.0020305018\n",
      "iter  949  loss  -0.043488726  grad l2 norm  0.0020364611\n",
      "iter  950  loss  -0.043497358  grad l2 norm  0.0020421841\n",
      "iter  951  loss  -0.043505978  grad l2 norm  0.0020476496\n",
      "iter  952  loss  -0.043514628  grad l2 norm  0.0020528382\n",
      "iter  953  loss  -0.043523286  grad l2 norm  0.002057733\n",
      "iter  954  loss  -0.043531947  grad l2 norm  0.0020623233\n",
      "iter  955  loss  -0.043540604  grad l2 norm  0.0020665976\n",
      "iter  956  loss  -0.043549277  grad l2 norm  0.0020705485\n",
      "iter  957  loss  -0.04355797  grad l2 norm  0.0020741692\n",
      "iter  958  loss  -0.043566644  grad l2 norm  0.002077457\n",
      "iter  959  loss  -0.043575328  grad l2 norm  0.0020804103\n",
      "iter  960  loss  -0.043584023  grad l2 norm  0.0020830252\n",
      "iter  961  loss  -0.043592714  grad l2 norm  0.0020853018\n",
      "iter  962  loss  -0.043601412  grad l2 norm  0.002087241\n",
      "iter  963  loss  -0.043610096  grad l2 norm  0.002088843\n",
      "iter  964  loss  -0.04361879  grad l2 norm  0.0020901086\n",
      "iter  965  loss  -0.043627482  grad l2 norm  0.0020910408\n",
      "iter  966  loss  -0.043636166  grad l2 norm  0.0020916406\n",
      "iter  967  loss  -0.043644845  grad l2 norm  0.0020919098\n",
      "iter  968  loss  -0.043653518  grad l2 norm  0.0020918504\n",
      "iter  969  loss  -0.043662187  grad l2 norm  0.002091465\n",
      "iter  970  loss  -0.043670844  grad l2 norm  0.0020907556\n",
      "iter  971  loss  -0.04367949  grad l2 norm  0.0020897263\n",
      "iter  972  loss  -0.043688126  grad l2 norm  0.0020883786\n",
      "iter  973  loss  -0.04369676  grad l2 norm  0.0020867183\n",
      "iter  974  loss  -0.04370536  grad l2 norm  0.0020847472\n",
      "iter  975  loss  -0.043713953  grad l2 norm  0.002082469\n",
      "iter  976  loss  -0.04372254  grad l2 norm  0.0020798873\n",
      "iter  977  loss  -0.04373111  grad l2 norm  0.0020770095\n",
      "iter  978  loss  -0.043739647  grad l2 norm  0.0020738395\n",
      "iter  979  loss  -0.043748166  grad l2 norm  0.002070382\n",
      "iter  980  loss  -0.043756686  grad l2 norm  0.0020666425\n",
      "iter  981  loss  -0.043765176  grad l2 norm  0.0020626313\n",
      "iter  982  loss  -0.043773632  grad l2 norm  0.0020583507\n",
      "iter  983  loss  -0.043782078  grad l2 norm  0.0020538098\n",
      "iter  984  loss  -0.04379049  grad l2 norm  0.002049019\n",
      "iter  985  loss  -0.04379889  grad l2 norm  0.0020439858\n",
      "iter  986  loss  -0.04380726  grad l2 norm  0.0020387196\n",
      "iter  987  loss  -0.04381559  grad l2 norm  0.0020332278\n",
      "iter  988  loss  -0.04382391  grad l2 norm  0.0020275237\n",
      "iter  989  loss  -0.0438322  grad l2 norm  0.0020216187\n",
      "iter  990  loss  -0.043840438  grad l2 norm  0.0020155236\n",
      "iter  991  loss  -0.04384868  grad l2 norm  0.0020092486\n",
      "iter  992  loss  -0.043856885  grad l2 norm  0.002002809\n",
      "iter  993  loss  -0.043865036  grad l2 norm  0.0019962154\n",
      "iter  994  loss  -0.043873172  grad l2 norm  0.0019894824\n",
      "iter  995  loss  -0.04388128  grad l2 norm  0.0019826235\n",
      "iter  996  loss  -0.04388934  grad l2 norm  0.0019756502\n",
      "iter  997  loss  -0.043897383  grad l2 norm  0.0019685796\n",
      "iter  998  loss  -0.043905403  grad l2 norm  0.0019614245\n",
      "iter  999  loss  -0.043913376  grad l2 norm  0.0019542007\n",
      "iter  1000  loss  -0.04392132  grad l2 norm  0.0019469216\n",
      "iter  1001  loss  -0.043929216  grad l2 norm  0.0019396016\n",
      "iter  1002  loss  -0.04393709  grad l2 norm  0.0019322576\n",
      "iter  1003  loss  -0.04394493  grad l2 norm  0.0019249032\n",
      "iter  1004  loss  -0.043952744  grad l2 norm  0.0019175546\n",
      "iter  1005  loss  -0.043960515  grad l2 norm  0.0019102257\n",
      "iter  1006  loss  -0.043968275  grad l2 norm  0.0019029333\n",
      "iter  1007  loss  -0.04397598  grad l2 norm  0.001895692\n",
      "iter  1008  loss  -0.04398365  grad l2 norm  0.001888518\n",
      "iter  1009  loss  -0.043991297  grad l2 norm  0.0018814297\n",
      "iter  1010  loss  -0.043998912  grad l2 norm  0.0018744394\n",
      "iter  1011  loss  -0.044006478  grad l2 norm  0.0018675674\n",
      "iter  1012  loss  -0.04401403  grad l2 norm  0.0018608287\n",
      "iter  1013  loss  -0.044021558  grad l2 norm  0.0018542428\n",
      "iter  1014  loss  -0.044029035  grad l2 norm  0.0018478263\n",
      "iter  1015  loss  -0.044036496  grad l2 norm  0.001841599\n",
      "iter  1016  loss  -0.044043932  grad l2 norm  0.0018355778\n",
      "iter  1017  loss  -0.044051323  grad l2 norm  0.0018297803\n",
      "iter  1018  loss  -0.0440587  grad l2 norm  0.0018242287\n",
      "iter  1019  loss  -0.044066016  grad l2 norm  0.00181894\n",
      "iter  1020  loss  -0.04407332  grad l2 norm  0.0018139343\n",
      "iter  1021  loss  -0.044080615  grad l2 norm  0.0018092297\n",
      "iter  1022  loss  -0.044087864  grad l2 norm  0.0018048446\n",
      "iter  1023  loss  -0.0440951  grad l2 norm  0.0018007956\n",
      "iter  1024  loss  -0.044102304  grad l2 norm  0.0017971008\n",
      "iter  1025  loss  -0.044109482  grad l2 norm  0.0017937738\n",
      "iter  1026  loss  -0.044116642  grad l2 norm  0.001790831\n",
      "iter  1027  loss  -0.04412378  grad l2 norm  0.0017882844\n",
      "iter  1028  loss  -0.044130888  grad l2 norm  0.0017861436\n",
      "iter  1029  loss  -0.044137977  grad l2 norm  0.001784418\n",
      "iter  1030  loss  -0.04414504  grad l2 norm  0.0017831144\n",
      "iter  1031  loss  -0.0441521  grad l2 norm  0.0017822367\n",
      "iter  1032  loss  -0.044159118  grad l2 norm  0.0017817878\n",
      "iter  1033  loss  -0.04416612  grad l2 norm  0.0017817664\n",
      "iter  1034  loss  -0.04417311  grad l2 norm  0.0017821686\n",
      "iter  1035  loss  -0.044180095  grad l2 norm  0.0017829903\n",
      "iter  1036  loss  -0.044187054  grad l2 norm  0.0017842221\n",
      "iter  1037  loss  -0.044193998  grad l2 norm  0.0017858538\n",
      "iter  1038  loss  -0.04420093  grad l2 norm  0.001787873\n",
      "iter  1039  loss  -0.044207845  grad l2 norm  0.0017902625\n",
      "iter  1040  loss  -0.04421476  grad l2 norm  0.0017930072\n",
      "iter  1041  loss  -0.044221636  grad l2 norm  0.0017960868\n",
      "iter  1042  loss  -0.044228528  grad l2 norm  0.0017994795\n",
      "iter  1043  loss  -0.04423541  grad l2 norm  0.0018031633\n",
      "iter  1044  loss  -0.044242278  grad l2 norm  0.001807113\n",
      "iter  1045  loss  -0.044249136  grad l2 norm  0.0018113026\n",
      "iter  1046  loss  -0.044255983  grad l2 norm  0.0018157063\n",
      "iter  1047  loss  -0.04426281  grad l2 norm  0.0018202959\n",
      "iter  1048  loss  -0.04426965  grad l2 norm  0.0018250433\n",
      "iter  1049  loss  -0.044276476  grad l2 norm  0.0018299201\n",
      "iter  1050  loss  -0.044283293  grad l2 norm  0.0018348969\n",
      "iter  1051  loss  -0.04429011  grad l2 norm  0.0018399467\n",
      "iter  1052  loss  -0.04429693  grad l2 norm  0.0018450399\n",
      "iter  1053  loss  -0.044303745  grad l2 norm  0.0018501483\n",
      "iter  1054  loss  -0.04431054  grad l2 norm  0.0018552463\n",
      "iter  1055  loss  -0.04431735  grad l2 norm  0.0018603054\n",
      "iter  1056  loss  -0.044324145  grad l2 norm  0.0018653006\n",
      "iter  1057  loss  -0.04433093  grad l2 norm  0.0018702084\n",
      "iter  1058  loss  -0.04433774  grad l2 norm  0.0018750058\n",
      "iter  1059  loss  -0.044344526  grad l2 norm  0.00187967\n",
      "iter  1060  loss  -0.044351313  grad l2 norm  0.0018841804\n",
      "iter  1061  loss  -0.0443581  grad l2 norm  0.0018885186\n",
      "iter  1062  loss  -0.044364892  grad l2 norm  0.001892666\n",
      "iter  1063  loss  -0.04437168  grad l2 norm  0.0018966069\n",
      "iter  1064  loss  -0.044378467  grad l2 norm  0.0019003274\n",
      "iter  1065  loss  -0.04438524  grad l2 norm  0.0019038141\n",
      "iter  1066  loss  -0.044392034  grad l2 norm  0.0019070545\n",
      "iter  1067  loss  -0.044398814  grad l2 norm  0.0019100392\n",
      "iter  1068  loss  -0.04440559  grad l2 norm  0.0019127597\n",
      "iter  1069  loss  -0.04441236  grad l2 norm  0.0019152093\n",
      "iter  1070  loss  -0.044419136  grad l2 norm  0.0019173799\n",
      "iter  1071  loss  -0.0444259  grad l2 norm  0.0019192698\n",
      "iter  1072  loss  -0.044432674  grad l2 norm  0.0019208746\n",
      "iter  1073  loss  -0.044439446  grad l2 norm  0.001922191\n",
      "iter  1074  loss  -0.044446215  grad l2 norm  0.0019232201\n",
      "iter  1075  loss  -0.044452954  grad l2 norm  0.0019239604\n",
      "iter  1076  loss  -0.04445971  grad l2 norm  0.0019244134\n",
      "iter  1077  loss  -0.044466466  grad l2 norm  0.001924581\n",
      "iter  1078  loss  -0.04447321  grad l2 norm  0.0019244638\n",
      "iter  1079  loss  -0.044479944  grad l2 norm  0.0019240675\n",
      "iter  1080  loss  -0.04448667  grad l2 norm  0.0019233943\n",
      "iter  1081  loss  -0.04449341  grad l2 norm  0.0019224489\n",
      "iter  1082  loss  -0.044500124  grad l2 norm  0.0019212348\n",
      "iter  1083  loss  -0.044506826  grad l2 norm  0.0019197587\n",
      "iter  1084  loss  -0.044513535  grad l2 norm  0.0019180243\n",
      "iter  1085  loss  -0.044520237  grad l2 norm  0.0019160383\n",
      "iter  1086  loss  -0.044526905  grad l2 norm  0.0019138064\n",
      "iter  1087  loss  -0.044533584  grad l2 norm  0.0019113339\n",
      "iter  1088  loss  -0.044540253  grad l2 norm  0.001908629\n",
      "iter  1089  loss  -0.0445469  grad l2 norm  0.0019056962\n",
      "iter  1090  loss  -0.044553544  grad l2 norm  0.0019025459\n",
      "iter  1091  loss  -0.044560164  grad l2 norm  0.0018991806\n",
      "iter  1092  loss  -0.04456679  grad l2 norm  0.001895612\n",
      "iter  1093  loss  -0.044573393  grad l2 norm  0.0018918485\n",
      "iter  1094  loss  -0.04457999  grad l2 norm  0.0018878952\n",
      "iter  1095  loss  -0.044586554  grad l2 norm  0.0018837635\n",
      "iter  1096  loss  -0.04459311  grad l2 norm  0.0018794583\n",
      "iter  1097  loss  -0.04459967  grad l2 norm  0.0018749931\n",
      "iter  1098  loss  -0.044606186  grad l2 norm  0.001870374\n",
      "iter  1099  loss  -0.044612713  grad l2 norm  0.0018656125\n",
      "iter  1100  loss  -0.044619214  grad l2 norm  0.001860717\n",
      "iter  1101  loss  -0.04462569  grad l2 norm  0.0018556989\n",
      "iter  1102  loss  -0.044632155  grad l2 norm  0.0018505681\n",
      "iter  1103  loss  -0.044638596  grad l2 norm  0.0018453355\n",
      "iter  1104  loss  -0.044645026  grad l2 norm  0.0018400109\n",
      "iter  1105  loss  -0.044651426  grad l2 norm  0.0018346049\n",
      "iter  1106  loss  -0.044657815  grad l2 norm  0.0018291305\n",
      "iter  1107  loss  -0.044664178  grad l2 norm  0.0018235956\n",
      "iter  1108  loss  -0.04467054  grad l2 norm  0.0018180141\n",
      "iter  1109  loss  -0.044676878  grad l2 norm  0.0018123939\n",
      "iter  1110  loss  -0.044683173  grad l2 norm  0.0018067493\n",
      "iter  1111  loss  -0.044689465  grad l2 norm  0.0018010918\n",
      "iter  1112  loss  -0.044695735  grad l2 norm  0.0017954285\n",
      "iter  1113  loss  -0.04470199  grad l2 norm  0.0017897731\n",
      "iter  1114  loss  -0.04470823  grad l2 norm  0.0017841344\n",
      "iter  1115  loss  -0.044714443  grad l2 norm  0.0017785248\n",
      "iter  1116  loss  -0.04472064  grad l2 norm  0.001772953\n",
      "iter  1117  loss  -0.04472681  grad l2 norm  0.0017674294\n",
      "iter  1118  loss  -0.044732977  grad l2 norm  0.0017619667\n",
      "iter  1119  loss  -0.044739123  grad l2 norm  0.0017565719\n",
      "iter  1120  loss  -0.04474525  grad l2 norm  0.0017512548\n",
      "iter  1121  loss  -0.044751357  grad l2 norm  0.0017460254\n",
      "iter  1122  loss  -0.044757456  grad l2 norm  0.0017408943\n",
      "iter  1123  loss  -0.044763535  grad l2 norm  0.0017358697\n",
      "iter  1124  loss  -0.044769593  grad l2 norm  0.001730963\n",
      "iter  1125  loss  -0.044775628  grad l2 norm  0.0017261817\n",
      "iter  1126  loss  -0.044781674  grad l2 norm  0.0017215366\n",
      "iter  1127  loss  -0.044787697  grad l2 norm  0.0017170387\n",
      "iter  1128  loss  -0.04479371  grad l2 norm  0.0017126938\n",
      "iter  1129  loss  -0.044799708  grad l2 norm  0.0017085152\n",
      "iter  1130  loss  -0.04480571  grad l2 norm  0.0017045118\n",
      "iter  1131  loss  -0.04481169  grad l2 norm  0.0017006934\n",
      "iter  1132  loss  -0.044817656  grad l2 norm  0.0016970692\n",
      "iter  1133  loss  -0.04482362  grad l2 norm  0.0016936514\n",
      "iter  1134  loss  -0.04482958  grad l2 norm  0.0016904463\n",
      "iter  1135  loss  -0.04483554  grad l2 norm  0.0016874652\n",
      "iter  1136  loss  -0.044841487  grad l2 norm  0.0016847166\n",
      "iter  1137  loss  -0.044847425  grad l2 norm  0.0016822073\n",
      "iter  1138  loss  -0.044853367  grad l2 norm  0.001679946\n",
      "iter  1139  loss  -0.04485933  grad l2 norm  0.0016779392\n",
      "iter  1140  loss  -0.044865258  grad l2 norm  0.0016761908\n",
      "iter  1141  loss  -0.044871185  grad l2 norm  0.0016747082\n",
      "iter  1142  loss  -0.044877127  grad l2 norm  0.0016734916\n",
      "iter  1143  loss  -0.04488306  grad l2 norm  0.0016725447\n",
      "iter  1144  loss  -0.044889007  grad l2 norm  0.0016718664\n",
      "iter  1145  loss  -0.04489495  grad l2 norm  0.0016714573\n",
      "iter  1146  loss  -0.044900898  grad l2 norm  0.0016713148\n",
      "iter  1147  loss  -0.04490684  grad l2 norm  0.0016714347\n",
      "iter  1148  loss  -0.0449128  grad l2 norm  0.0016718124\n",
      "iter  1149  loss  -0.04491876  grad l2 norm  0.0016724413\n",
      "iter  1150  loss  -0.044924743  grad l2 norm  0.0016733146\n",
      "iter  1151  loss  -0.0449307  grad l2 norm  0.0016744249\n",
      "iter  1152  loss  -0.044936676  grad l2 norm  0.0016757606\n",
      "iter  1153  loss  -0.044942684  grad l2 norm  0.0016773129\n",
      "iter  1154  loss  -0.04494868  grad l2 norm  0.0016790691\n",
      "iter  1155  loss  -0.04495469  grad l2 norm  0.0016810192\n",
      "iter  1156  loss  -0.04496071  grad l2 norm  0.0016831507\n",
      "iter  1157  loss  -0.04496674  grad l2 norm  0.0016854513\n",
      "iter  1158  loss  -0.044972796  grad l2 norm  0.0016879081\n",
      "iter  1159  loss  -0.04497885  grad l2 norm  0.0016905068\n",
      "iter  1160  loss  -0.044984914  grad l2 norm  0.0016932362\n",
      "iter  1161  loss  -0.044990994  grad l2 norm  0.0016960829\n",
      "iter  1162  loss  -0.044997096  grad l2 norm  0.0016990332\n",
      "iter  1163  loss  -0.045003206  grad l2 norm  0.0017020742\n",
      "iter  1164  loss  -0.04500932  grad l2 norm  0.0017051928\n",
      "iter  1165  loss  -0.045015443  grad l2 norm  0.0017083767\n",
      "iter  1166  loss  -0.04502159  grad l2 norm  0.0017116107\n",
      "iter  1167  loss  -0.045027755  grad l2 norm  0.0017148852\n",
      "iter  1168  loss  -0.045033924  grad l2 norm  0.0017181841\n",
      "iter  1169  loss  -0.045040116  grad l2 norm  0.001721497\n",
      "iter  1170  loss  -0.045046326  grad l2 norm  0.001724808\n",
      "iter  1171  loss  -0.04505253  grad l2 norm  0.001728106\n",
      "iter  1172  loss  -0.04505877  grad l2 norm  0.0017313756\n",
      "iter  1173  loss  -0.045065004  grad l2 norm  0.001734606\n",
      "iter  1174  loss  -0.04507126  grad l2 norm  0.0017377816\n",
      "iter  1175  loss  -0.045077525  grad l2 norm  0.0017408894\n",
      "iter  1176  loss  -0.0450838  grad l2 norm  0.0017439174\n",
      "iter  1177  loss  -0.045090087  grad l2 norm  0.0017468511\n",
      "iter  1178  loss  -0.04509641  grad l2 norm  0.001749677\n",
      "iter  1179  loss  -0.045102715  grad l2 norm  0.0017523855\n",
      "iter  1180  loss  -0.045109037  grad l2 norm  0.0017549633\n",
      "iter  1181  loss  -0.045115378  grad l2 norm  0.0017573984\n",
      "iter  1182  loss  -0.045121696  grad l2 norm  0.0017596813\n",
      "iter  1183  loss  -0.045128044  grad l2 norm  0.0017618025\n",
      "iter  1184  loss  -0.04513441  grad l2 norm  0.0017637538\n",
      "iter  1185  loss  -0.045140762  grad l2 norm  0.001765528\n",
      "iter  1186  loss  -0.045147147  grad l2 norm  0.0017671173\n",
      "iter  1187  loss  -0.045153495  grad l2 norm  0.0017685193\n",
      "iter  1188  loss  -0.045159873  grad l2 norm  0.0017697291\n",
      "iter  1189  loss  -0.04516624  grad l2 norm  0.001770744\n",
      "iter  1190  loss  -0.04517262  grad l2 norm  0.0017715638\n",
      "iter  1191  loss  -0.045179002  grad l2 norm  0.0017721903\n",
      "iter  1192  loss  -0.04518538  grad l2 norm  0.0017726192\n",
      "iter  1193  loss  -0.045191742  grad l2 norm  0.0017728589\n",
      "iter  1194  loss  -0.045198113  grad l2 norm  0.0017729101\n",
      "iter  1195  loss  -0.045204483  grad l2 norm  0.0017727764\n",
      "iter  1196  loss  -0.045210842  grad l2 norm  0.0017724645\n",
      "iter  1197  loss  -0.045217205  grad l2 norm  0.0017719779\n",
      "iter  1198  loss  -0.045223556  grad l2 norm  0.0017713237\n",
      "iter  1199  loss  -0.0452299  grad l2 norm  0.0017705072\n",
      "iter  1200  loss  -0.04523625  grad l2 norm  0.0017695366\n",
      "iter  1201  loss  -0.045242567  grad l2 norm  0.0017684175\n",
      "iter  1202  loss  -0.0452489  grad l2 norm  0.0017671563\n",
      "iter  1203  loss  -0.04525522  grad l2 norm  0.0017657594\n",
      "iter  1204  loss  -0.04526152  grad l2 norm  0.0017642351\n",
      "iter  1205  loss  -0.045267813  grad l2 norm  0.001762589\n",
      "iter  1206  loss  -0.0452741  grad l2 norm  0.0017608267\n",
      "iter  1207  loss  -0.045280382  grad l2 norm  0.0017589574\n",
      "iter  1208  loss  -0.045286633  grad l2 norm  0.0017569824\n",
      "iter  1209  loss  -0.0452929  grad l2 norm  0.0017549106\n",
      "iter  1210  loss  -0.04529914  grad l2 norm  0.0017527473\n",
      "iter  1211  loss  -0.04530537  grad l2 norm  0.001750495\n",
      "iter  1212  loss  -0.04531159  grad l2 norm  0.0017481616\n",
      "iter  1213  loss  -0.045317806  grad l2 norm  0.0017457518\n",
      "iter  1214  loss  -0.045323994  grad l2 norm  0.0017432674\n",
      "iter  1215  loss  -0.045330197  grad l2 norm  0.001740713\n",
      "iter  1216  loss  -0.045336362  grad l2 norm  0.001738093\n",
      "iter  1217  loss  -0.045342524  grad l2 norm  0.0017354084\n",
      "iter  1218  loss  -0.04534867  grad l2 norm  0.0017326665\n",
      "iter  1219  loss  -0.045354825  grad l2 norm  0.0017298645\n",
      "iter  1220  loss  -0.04536095  grad l2 norm  0.0017270087\n",
      "iter  1221  loss  -0.045367066  grad l2 norm  0.0017240982\n",
      "iter  1222  loss  -0.045373168  grad l2 norm  0.0017211357\n",
      "iter  1223  loss  -0.04537926  grad l2 norm  0.0017181233\n",
      "iter  1224  loss  -0.045385342  grad l2 norm  0.0017150617\n",
      "iter  1225  loss  -0.04539141  grad l2 norm  0.0017119512\n",
      "iter  1226  loss  -0.04539746  grad l2 norm  0.0017087924\n",
      "iter  1227  loss  -0.045403503  grad l2 norm  0.0017055856\n",
      "iter  1228  loss  -0.045409523  grad l2 norm  0.0017023312\n",
      "iter  1229  loss  -0.04541555  grad l2 norm  0.0016990306\n",
      "iter  1230  loss  -0.04542155  grad l2 norm  0.0016956836\n",
      "iter  1231  loss  -0.04542754  grad l2 norm  0.0016922873\n",
      "iter  1232  loss  -0.045433514  grad l2 norm  0.001688849\n",
      "iter  1233  loss  -0.04543948  grad l2 norm  0.0016853631\n",
      "iter  1234  loss  -0.045445427  grad l2 norm  0.0016818335\n",
      "iter  1235  loss  -0.04545137  grad l2 norm  0.0016782604\n",
      "iter  1236  loss  -0.0454573  grad l2 norm  0.0016746474\n",
      "iter  1237  loss  -0.045463197  grad l2 norm  0.0016709953\n",
      "iter  1238  loss  -0.045469098  grad l2 norm  0.0016673058\n",
      "iter  1239  loss  -0.04547498  grad l2 norm  0.001663585\n",
      "iter  1240  loss  -0.045480866  grad l2 norm  0.001659835\n",
      "iter  1241  loss  -0.045486715  grad l2 norm  0.0016560601\n",
      "iter  1242  loss  -0.045492556  grad l2 norm  0.0016522674\n",
      "iter  1243  loss  -0.045498386  grad l2 norm  0.0016484618\n",
      "iter  1244  loss  -0.04550421  grad l2 norm  0.0016446488\n",
      "iter  1245  loss  -0.045510013  grad l2 norm  0.0016408362\n",
      "iter  1246  loss  -0.04551579  grad l2 norm  0.00163703\n",
      "iter  1247  loss  -0.04552156  grad l2 norm  0.0016332396\n",
      "iter  1248  loss  -0.04552734  grad l2 norm  0.0016294712\n",
      "iter  1249  loss  -0.045533083  grad l2 norm  0.0016257348\n",
      "iter  1250  loss  -0.04553883  grad l2 norm  0.0016220368\n",
      "iter  1251  loss  -0.045544542  grad l2 norm  0.0016183843\n",
      "iter  1252  loss  -0.04555026  grad l2 norm  0.0016147852\n",
      "iter  1253  loss  -0.045555964  grad l2 norm  0.0016112477\n",
      "iter  1254  loss  -0.04556164  grad l2 norm  0.0016077788\n",
      "iter  1255  loss  -0.045567337  grad l2 norm  0.0016043845\n",
      "iter  1256  loss  -0.045572996  grad l2 norm  0.0016010707\n",
      "iter  1257  loss  -0.045578655  grad l2 norm  0.0015978409\n",
      "iter  1258  loss  -0.0455843  grad l2 norm  0.0015947032\n",
      "iter  1259  loss  -0.045589946  grad l2 norm  0.0015916594\n",
      "iter  1260  loss  -0.045595556  grad l2 norm  0.0015887164\n",
      "iter  1261  loss  -0.045601178  grad l2 norm  0.0015858742\n",
      "iter  1262  loss  -0.045606785  grad l2 norm  0.0015831371\n",
      "iter  1263  loss  -0.045612387  grad l2 norm  0.0015805082\n",
      "iter  1264  loss  -0.04561798  grad l2 norm  0.0015779895\n",
      "iter  1265  loss  -0.045623578  grad l2 norm  0.0015755842\n",
      "iter  1266  loss  -0.045629147  grad l2 norm  0.0015732899\n",
      "iter  1267  loss  -0.045634724  grad l2 norm  0.001571114\n",
      "iter  1268  loss  -0.04564029  grad l2 norm  0.0015690544\n",
      "iter  1269  loss  -0.04564585  grad l2 norm  0.0015671144\n",
      "iter  1270  loss  -0.045651414  grad l2 norm  0.001565297\n",
      "iter  1271  loss  -0.045656975  grad l2 norm  0.001563602\n",
      "iter  1272  loss  -0.045662537  grad l2 norm  0.0015620341\n",
      "iter  1273  loss  -0.04566808  grad l2 norm  0.0015605937\n",
      "iter  1274  loss  -0.045673627  grad l2 norm  0.0015592847\n",
      "iter  1275  loss  -0.045679178  grad l2 norm  0.0015581088\n",
      "iter  1276  loss  -0.045684718  grad l2 norm  0.0015570695\n",
      "iter  1277  loss  -0.045690265  grad l2 norm  0.0015561706\n",
      "iter  1278  loss  -0.04569582  grad l2 norm  0.0015554144\n",
      "iter  1279  loss  -0.045701355  grad l2 norm  0.0015548033\n",
      "iter  1280  loss  -0.045706905  grad l2 norm  0.0015543409\n",
      "iter  1281  loss  -0.04571245  grad l2 norm  0.0015540292\n",
      "iter  1282  loss  -0.045718003  grad l2 norm  0.0015538706\n",
      "iter  1283  loss  -0.045723557  grad l2 norm  0.0015538664\n",
      "iter  1284  loss  -0.045729127  grad l2 norm  0.0015540175\n",
      "iter  1285  loss  -0.04573468  grad l2 norm  0.0015543257\n",
      "iter  1286  loss  -0.045740247  grad l2 norm  0.0015547897\n",
      "iter  1287  loss  -0.04574581  grad l2 norm  0.0015554095\n",
      "iter  1288  loss  -0.04575138  grad l2 norm  0.0015561839\n",
      "iter  1289  loss  -0.045756966  grad l2 norm  0.0015571103\n",
      "iter  1290  loss  -0.045762543  grad l2 norm  0.0015581842\n",
      "iter  1291  loss  -0.045768127  grad l2 norm  0.001559403\n",
      "iter  1292  loss  -0.045773733  grad l2 norm  0.0015607621\n",
      "iter  1293  loss  -0.045779336  grad l2 norm  0.0015622565\n",
      "iter  1294  loss  -0.045784943  grad l2 norm  0.0015638797\n",
      "iter  1295  loss  -0.04579056  grad l2 norm  0.0015656254\n",
      "iter  1296  loss  -0.04579618  grad l2 norm  0.0015674868\n",
      "iter  1297  loss  -0.04580181  grad l2 norm  0.0015694557\n",
      "iter  1298  loss  -0.04580744  grad l2 norm  0.0015715268\n",
      "iter  1299  loss  -0.04581309  grad l2 norm  0.0015736909\n",
      "iter  1300  loss  -0.045818746  grad l2 norm  0.0015759397\n",
      "iter  1301  loss  -0.04582441  grad l2 norm  0.0015782651\n",
      "iter  1302  loss  -0.045830082  grad l2 norm  0.0015806609\n",
      "iter  1303  loss  -0.045835752  grad l2 norm  0.0015831186\n",
      "iter  1304  loss  -0.045841448  grad l2 norm  0.0015856294\n",
      "iter  1305  loss  -0.04584713  grad l2 norm  0.0015881883\n",
      "iter  1306  loss  -0.045852836  grad l2 norm  0.0015907872\n",
      "iter  1307  loss  -0.04585854  grad l2 norm  0.001593421\n",
      "iter  1308  loss  -0.045864258  grad l2 norm  0.0015960796\n",
      "iter  1309  loss  -0.045869976  grad l2 norm  0.001598759\n",
      "iter  1310  loss  -0.045875695  grad l2 norm  0.0016014546\n",
      "iter  1311  loss  -0.045881424  grad l2 norm  0.0016041588\n",
      "iter  1312  loss  -0.04588716  grad l2 norm  0.0016068661\n",
      "iter  1313  loss  -0.0458929  grad l2 norm  0.0016095696\n",
      "iter  1314  loss  -0.04589865  grad l2 norm  0.0016122649\n",
      "iter  1315  loss  -0.04590439  grad l2 norm  0.0016149459\n",
      "iter  1316  loss  -0.045910146  grad l2 norm  0.0016176056\n",
      "iter  1317  loss  -0.0459159  grad l2 norm  0.0016202388\n",
      "iter  1318  loss  -0.045921654  grad l2 norm  0.001622836\n",
      "iter  1319  loss  -0.045927417  grad l2 norm  0.001625393\n",
      "iter  1320  loss  -0.045933165  grad l2 norm  0.0016279009\n",
      "iter  1321  loss  -0.045938924  grad l2 norm  0.0016303529\n",
      "iter  1322  loss  -0.04594468  grad l2 norm  0.00163274\n",
      "iter  1323  loss  -0.045950424  grad l2 norm  0.0016350561\n",
      "iter  1324  loss  -0.045956183  grad l2 norm  0.0016372915\n",
      "iter  1325  loss  -0.045961935  grad l2 norm  0.0016394362\n",
      "iter  1326  loss  -0.04596768  grad l2 norm  0.0016414834\n",
      "iter  1327  loss  -0.045973424  grad l2 norm  0.0016434251\n",
      "iter  1328  loss  -0.04597917  grad l2 norm  0.0016452504\n",
      "iter  1329  loss  -0.045984898  grad l2 norm  0.0016469512\n",
      "iter  1330  loss  -0.045990624  grad l2 norm  0.0016485194\n",
      "iter  1331  loss  -0.045996357  grad l2 norm  0.0016499461\n",
      "iter  1332  loss  -0.046002075  grad l2 norm  0.001651222\n",
      "iter  1333  loss  -0.046007786  grad l2 norm  0.0016523395\n",
      "iter  1334  loss  -0.0460135  grad l2 norm  0.0016532926\n",
      "iter  1335  loss  -0.04601919  grad l2 norm  0.0016540687\n",
      "iter  1336  loss  -0.046024892  grad l2 norm  0.0016546656\n",
      "iter  1337  loss  -0.046030577  grad l2 norm  0.0016550734\n",
      "iter  1338  loss  -0.046036243  grad l2 norm  0.0016552868\n",
      "iter  1339  loss  -0.046041906  grad l2 norm  0.001655299\n",
      "iter  1340  loss  -0.046047576  grad l2 norm  0.0016551052\n",
      "iter  1341  loss  -0.046053223  grad l2 norm  0.0016547008\n",
      "iter  1342  loss  -0.046058875  grad l2 norm  0.0016540838\n",
      "iter  1343  loss  -0.046064496  grad l2 norm  0.0016532474\n",
      "iter  1344  loss  -0.04607013  grad l2 norm  0.001652191\n",
      "iter  1345  loss  -0.046075758  grad l2 norm  0.0016509127\n",
      "iter  1346  loss  -0.046081357  grad l2 norm  0.0016494129\n",
      "iter  1347  loss  -0.046086963  grad l2 norm  0.0016476919\n",
      "iter  1348  loss  -0.046092555  grad l2 norm  0.0016457491\n",
      "iter  1349  loss  -0.046098128  grad l2 norm  0.001643588\n",
      "iter  1350  loss  -0.04610372  grad l2 norm  0.001641212\n",
      "iter  1351  loss  -0.046109278  grad l2 norm  0.0016386248\n",
      "iter  1352  loss  -0.046114825  grad l2 norm  0.0016358313\n",
      "iter  1353  loss  -0.046120364  grad l2 norm  0.0016328406\n",
      "iter  1354  loss  -0.0461259  grad l2 norm  0.0016296575\n",
      "iter  1355  loss  -0.04613144  grad l2 norm  0.0016262871\n",
      "iter  1356  loss  -0.04613694  grad l2 norm  0.0016227427\n",
      "iter  1357  loss  -0.046142448  grad l2 norm  0.0016190299\n",
      "iter  1358  loss  -0.046147943  grad l2 norm  0.0016151601\n",
      "iter  1359  loss  -0.04615343  grad l2 norm  0.0016111436\n",
      "iter  1360  loss  -0.046158906  grad l2 norm  0.0016069902\n",
      "iter  1361  loss  -0.046164364  grad l2 norm  0.0016027109\n",
      "iter  1362  loss  -0.046169817  grad l2 norm  0.001598315\n",
      "iter  1363  loss  -0.046175264  grad l2 norm  0.0015938149\n",
      "iter  1364  loss  -0.046180695  grad l2 norm  0.0015892213\n",
      "iter  1365  loss  -0.04618611  grad l2 norm  0.0015845443\n",
      "iter  1366  loss  -0.04619154  grad l2 norm  0.0015797934\n",
      "iter  1367  loss  -0.046196938  grad l2 norm  0.0015749804\n",
      "iter  1368  loss  -0.04620233  grad l2 norm  0.0015701152\n",
      "iter  1369  loss  -0.04620773  grad l2 norm  0.0015652064\n",
      "iter  1370  loss  -0.046213094  grad l2 norm  0.0015602639\n",
      "iter  1371  loss  -0.046218455  grad l2 norm  0.0015552978\n",
      "iter  1372  loss  -0.04622381  grad l2 norm  0.0015503151\n",
      "iter  1373  loss  -0.04622917  grad l2 norm  0.0015453268\n",
      "iter  1374  loss  -0.04623448  grad l2 norm  0.0015403404\n",
      "iter  1375  loss  -0.04623981  grad l2 norm  0.0015353629\n",
      "iter  1376  loss  -0.046245128  grad l2 norm  0.001530404\n",
      "iter  1377  loss  -0.046250448  grad l2 norm  0.0015254716\n",
      "iter  1378  loss  -0.04625573  grad l2 norm  0.0015205729\n",
      "iter  1379  loss  -0.046261005  grad l2 norm  0.0015157175\n",
      "iter  1380  loss  -0.046266276  grad l2 norm  0.0015109131\n",
      "iter  1381  loss  -0.04627155  grad l2 norm  0.0015061675\n",
      "iter  1382  loss  -0.046276793  grad l2 norm  0.0015014899\n",
      "iter  1383  loss  -0.046282034  grad l2 norm  0.0014968896\n",
      "iter  1384  loss  -0.046287272  grad l2 norm  0.0014923762\n",
      "iter  1385  loss  -0.04629248  grad l2 norm  0.0014879594\n",
      "iter  1386  loss  -0.04629768  grad l2 norm  0.00148365\n",
      "iter  1387  loss  -0.04630287  grad l2 norm  0.0014794574\n",
      "iter  1388  loss  -0.046308056  grad l2 norm  0.0014753935\n",
      "iter  1389  loss  -0.04631321  grad l2 norm  0.0014714709\n",
      "iter  1390  loss  -0.046318375  grad l2 norm  0.0014677008\n",
      "iter  1391  loss  -0.046323508  grad l2 norm  0.0014640971\n",
      "iter  1392  loss  -0.046328638  grad l2 norm  0.0014606706\n",
      "iter  1393  loss  -0.04633377  grad l2 norm  0.001457436\n",
      "iter  1394  loss  -0.046338867  grad l2 norm  0.0014544054\n",
      "iter  1395  loss  -0.046343964  grad l2 norm  0.0014515942\n",
      "iter  1396  loss  -0.046349045  grad l2 norm  0.0014490137\n",
      "iter  1397  loss  -0.04635412  grad l2 norm  0.0014466781\n",
      "iter  1398  loss  -0.046359185  grad l2 norm  0.0014446023\n",
      "iter  1399  loss  -0.046364233  grad l2 norm  0.0014427946\n",
      "iter  1400  loss  -0.04636927  grad l2 norm  0.0014412701\n",
      "iter  1401  loss  -0.04637431  grad l2 norm  0.0014400389\n",
      "iter  1402  loss  -0.046379317  grad l2 norm  0.0014391097\n",
      "iter  1403  loss  -0.04638434  grad l2 norm  0.001438495\n",
      "iter  1404  loss  -0.046389334  grad l2 norm  0.0014381999\n",
      "iter  1405  loss  -0.046394352  grad l2 norm  0.0014382317\n",
      "iter  1406  loss  -0.04639934  grad l2 norm  0.0014385963\n",
      "iter  1407  loss  -0.04640433  grad l2 norm  0.0014392986\n",
      "iter  1408  loss  -0.046409313  grad l2 norm  0.0014403389\n",
      "iter  1409  loss  -0.04641429  grad l2 norm  0.0014417209\n",
      "iter  1410  loss  -0.046419278  grad l2 norm  0.0014434437\n",
      "iter  1411  loss  -0.04642424  grad l2 norm  0.0014455046\n",
      "iter  1412  loss  -0.04642921  grad l2 norm  0.0014479004\n",
      "iter  1413  loss  -0.046434186  grad l2 norm  0.0014506285\n",
      "iter  1414  loss  -0.04643916  grad l2 norm  0.0014536809\n",
      "iter  1415  loss  -0.046444148  grad l2 norm  0.0014570529\n",
      "iter  1416  loss  -0.046449117  grad l2 norm  0.0014607349\n",
      "iter  1417  loss  -0.0464541  grad l2 norm  0.0014647188\n",
      "iter  1418  loss  -0.04645909  grad l2 norm  0.0014689948\n",
      "iter  1419  loss  -0.046464067  grad l2 norm  0.0014735521\n",
      "iter  1420  loss  -0.046469077  grad l2 norm  0.0014783784\n",
      "iter  1421  loss  -0.046474073  grad l2 norm  0.0014834624\n",
      "iter  1422  loss  -0.04647909  grad l2 norm  0.0014887912\n",
      "iter  1423  loss  -0.0464841  grad l2 norm  0.0014943501\n",
      "iter  1424  loss  -0.04648912  grad l2 norm  0.0015001274\n",
      "iter  1425  loss  -0.04649416  grad l2 norm  0.0015061055\n",
      "iter  1426  loss  -0.046499204  grad l2 norm  0.001512271\n",
      "iter  1427  loss  -0.04650427  grad l2 norm  0.0015186064\n",
      "iter  1428  loss  -0.04650932  grad l2 norm  0.0015250964\n",
      "iter  1429  loss  -0.046514392  grad l2 norm  0.0015317227\n",
      "iter  1430  loss  -0.04651949  grad l2 norm  0.0015384684\n",
      "iter  1431  loss  -0.046524584  grad l2 norm  0.0015453135\n",
      "iter  1432  loss  -0.04652971  grad l2 norm  0.00155224\n",
      "iter  1433  loss  -0.046534825  grad l2 norm  0.0015592264\n",
      "iter  1434  loss  -0.04653996  grad l2 norm  0.0015662519\n",
      "iter  1435  loss  -0.046545118  grad l2 norm  0.0015732974\n",
      "iter  1436  loss  -0.04655027  grad l2 norm  0.001580338\n",
      "iter  1437  loss  -0.046555433  grad l2 norm  0.0015873521\n",
      "iter  1438  loss  -0.046560638  grad l2 norm  0.0015943173\n",
      "iter  1439  loss  -0.04656582  grad l2 norm  0.0016012094\n",
      "iter  1440  loss  -0.046571042  grad l2 norm  0.001608006\n",
      "iter  1441  loss  -0.046576265  grad l2 norm  0.0016146823\n",
      "iter  1442  loss  -0.046581484  grad l2 norm  0.0016212164\n",
      "iter  1443  loss  -0.04658674  grad l2 norm  0.0016275841\n",
      "iter  1444  loss  -0.04659199  grad l2 norm  0.0016337625\n",
      "iter  1445  loss  -0.04659726  grad l2 norm  0.0016397296\n",
      "iter  1446  loss  -0.046602547  grad l2 norm  0.0016454633\n",
      "iter  1447  loss  -0.04660782  grad l2 norm  0.0016509426\n",
      "iter  1448  loss  -0.046613116  grad l2 norm  0.0016561483\n",
      "iter  1449  loss  -0.046618417  grad l2 norm  0.0016610618\n",
      "iter  1450  loss  -0.046623725  grad l2 norm  0.0016656631\n",
      "iter  1451  loss  -0.046629045  grad l2 norm  0.0016699386\n",
      "iter  1452  loss  -0.04663435  grad l2 norm  0.0016738725\n",
      "iter  1453  loss  -0.04663967  grad l2 norm  0.0016774505\n",
      "iter  1454  loss  -0.046645  grad l2 norm  0.0016806629\n",
      "iter  1455  loss  -0.046650328  grad l2 norm  0.0016834971\n",
      "iter  1456  loss  -0.04665566  grad l2 norm  0.0016859472\n",
      "iter  1457  loss  -0.046661  grad l2 norm  0.0016880032\n",
      "iter  1458  loss  -0.04666632  grad l2 norm  0.001689665\n",
      "iter  1459  loss  -0.046671644  grad l2 norm  0.0016909238\n",
      "iter  1460  loss  -0.046676986  grad l2 norm  0.0016917806\n",
      "iter  1461  loss  -0.04668232  grad l2 norm  0.001692234\n",
      "iter  1462  loss  -0.046687625  grad l2 norm  0.001692288\n",
      "iter  1463  loss  -0.046692938  grad l2 norm  0.0016919408\n",
      "iter  1464  loss  -0.046698265  grad l2 norm  0.0016911993\n",
      "iter  1465  loss  -0.046703566  grad l2 norm  0.0016900665\n",
      "iter  1466  loss  -0.04670887  grad l2 norm  0.0016885501\n",
      "iter  1467  loss  -0.046714153  grad l2 norm  0.0016866592\n",
      "iter  1468  loss  -0.046719454  grad l2 norm  0.0016843972\n",
      "iter  1469  loss  -0.046724726  grad l2 norm  0.0016817782\n",
      "iter  1470  loss  -0.046729993  grad l2 norm  0.0016788109\n",
      "iter  1471  loss  -0.046735264  grad l2 norm  0.0016755039\n",
      "iter  1472  loss  -0.0467405  grad l2 norm  0.0016718723\n",
      "iter  1473  loss  -0.04674575  grad l2 norm  0.0016679278\n",
      "iter  1474  loss  -0.046750966  grad l2 norm  0.0016636815\n",
      "iter  1475  loss  -0.046756174  grad l2 norm  0.0016591499\n",
      "iter  1476  loss  -0.04676139  grad l2 norm  0.0016543462\n",
      "iter  1477  loss  -0.046766568  grad l2 norm  0.0016492845\n",
      "iter  1478  loss  -0.04677174  grad l2 norm  0.0016439852\n",
      "iter  1479  loss  -0.046776902  grad l2 norm  0.0016384609\n",
      "iter  1480  loss  -0.046782054  grad l2 norm  0.00163273\n",
      "iter  1481  loss  -0.046787187  grad l2 norm  0.0016268113\n",
      "iter  1482  loss  -0.046792317  grad l2 norm  0.0016207258\n",
      "iter  1483  loss  -0.04679742  grad l2 norm  0.0016144882\n",
      "iter  1484  loss  -0.046802513  grad l2 norm  0.0016081216\n",
      "iter  1485  loss  -0.04680759  grad l2 norm  0.001601646\n",
      "iter  1486  loss  -0.04681265  grad l2 norm  0.0015950825\n",
      "iter  1487  loss  -0.046817698  grad l2 norm  0.0015884561\n",
      "iter  1488  loss  -0.046822734  grad l2 norm  0.0015817843\n",
      "iter  1489  loss  -0.046827752  grad l2 norm  0.0015750926\n",
      "iter  1490  loss  -0.046832766  grad l2 norm  0.0015684032\n",
      "iter  1491  loss  -0.04683774  grad l2 norm  0.0015617431\n",
      "iter  1492  loss  -0.046842717  grad l2 norm  0.0015551297\n",
      "iter  1493  loss  -0.046847686  grad l2 norm  0.0015485925\n",
      "iter  1494  loss  -0.04685263  grad l2 norm  0.0015421548\n",
      "iter  1495  loss  -0.046857562  grad l2 norm  0.0015358389\n",
      "iter  1496  loss  -0.046862483  grad l2 norm  0.001529672\n",
      "iter  1497  loss  -0.046867378  grad l2 norm  0.0015236752\n",
      "iter  1498  loss  -0.046872284  grad l2 norm  0.0015178748\n",
      "iter  1499  loss  -0.046877164  grad l2 norm  0.0015122935\n",
      "iter  1500  loss  -0.04688202  grad l2 norm  0.0015069542\n",
      "iter  1501  loss  -0.046886895  grad l2 norm  0.0015018798\n",
      "iter  1502  loss  -0.046891734  grad l2 norm  0.0014970921\n",
      "iter  1503  loss  -0.04689657  grad l2 norm  0.0014926123\n",
      "iter  1504  loss  -0.0469014  grad l2 norm  0.0014884613\n",
      "iter  1505  loss  -0.046906207  grad l2 norm  0.0014846574\n",
      "iter  1506  loss  -0.04691102  grad l2 norm  0.0014812201\n",
      "iter  1507  loss  -0.046915818  grad l2 norm  0.0014781659\n",
      "iter  1508  loss  -0.04692061  grad l2 norm  0.00147551\n",
      "iter  1509  loss  -0.046925396  grad l2 norm  0.0014732688\n",
      "iter  1510  loss  -0.046930183  grad l2 norm  0.0014714533\n",
      "iter  1511  loss  -0.046934962  grad l2 norm  0.0014700762\n",
      "iter  1512  loss  -0.046939716  grad l2 norm  0.0014691473\n",
      "iter  1513  loss  -0.04694449  grad l2 norm  0.0014686745\n",
      "iter  1514  loss  -0.046949264  grad l2 norm  0.0014686642\n",
      "iter  1515  loss  -0.04695403  grad l2 norm  0.0014691234\n",
      "iter  1516  loss  -0.046958786  grad l2 norm  0.0014700547\n",
      "iter  1517  loss  -0.04696355  grad l2 norm  0.001471458\n",
      "iter  1518  loss  -0.046968307  grad l2 norm  0.001473335\n",
      "iter  1519  loss  -0.04697308  grad l2 norm  0.0014756839\n",
      "iter  1520  loss  -0.046977848  grad l2 norm  0.0014785003\n",
      "iter  1521  loss  -0.046982624  grad l2 norm  0.0014817822\n",
      "iter  1522  loss  -0.046987414  grad l2 norm  0.0014855199\n",
      "iter  1523  loss  -0.04699219  grad l2 norm  0.0014897068\n",
      "iter  1524  loss  -0.04699699  grad l2 norm  0.0014943351\n",
      "iter  1525  loss  -0.047001783  grad l2 norm  0.0014993921\n",
      "iter  1526  loss  -0.04700659  grad l2 norm  0.0015048682\n",
      "iter  1527  loss  -0.0470114  grad l2 norm  0.0015107498\n",
      "iter  1528  loss  -0.04701623  grad l2 norm  0.0015170227\n",
      "iter  1529  loss  -0.047021057  grad l2 norm  0.0015236739\n",
      "iter  1530  loss  -0.047025893  grad l2 norm  0.0015306867\n",
      "iter  1531  loss  -0.047030758  grad l2 norm  0.0015380455\n",
      "iter  1532  loss  -0.047035616  grad l2 norm  0.001545733\n",
      "iter  1533  loss  -0.047040496  grad l2 norm  0.0015537326\n",
      "iter  1534  loss  -0.04704539  grad l2 norm  0.0015620274\n",
      "iter  1535  loss  -0.0470503  grad l2 norm  0.0015705976\n",
      "iter  1536  loss  -0.047055215  grad l2 norm  0.0015794259\n",
      "iter  1537  loss  -0.047060147  grad l2 norm  0.0015884943\n",
      "iter  1538  loss  -0.047065087  grad l2 norm  0.0015977828\n",
      "iter  1539  loss  -0.04707004  grad l2 norm  0.0016072729\n",
      "iter  1540  loss  -0.04707502  grad l2 norm  0.0016169448\n",
      "iter  1541  loss  -0.04707999  grad l2 norm  0.0016267804\n",
      "iter  1542  loss  -0.047084987  grad l2 norm  0.0016367587\n",
      "iter  1543  loss  -0.047090013  grad l2 norm  0.0016468604\n",
      "iter  1544  loss  -0.04709502  grad l2 norm  0.0016570647\n",
      "iter  1545  loss  -0.047100067  grad l2 norm  0.0016673516\n",
      "iter  1546  loss  -0.047105122  grad l2 norm  0.0016776999\n",
      "iter  1547  loss  -0.04711018  grad l2 norm  0.0016880888\n",
      "iter  1548  loss  -0.047115274  grad l2 norm  0.0016984963\n",
      "iter  1549  loss  -0.04712037  grad l2 norm  0.0017089028\n",
      "iter  1550  loss  -0.04712548  grad l2 norm  0.0017192833\n",
      "iter  1551  loss  -0.047130592  grad l2 norm  0.0017296176\n",
      "iter  1552  loss  -0.047135737  grad l2 norm  0.0017398852\n",
      "iter  1553  loss  -0.04714088  grad l2 norm  0.0017500608\n",
      "iter  1554  loss  -0.047146045  grad l2 norm  0.0017601231\n",
      "iter  1555  loss  -0.04715122  grad l2 norm  0.0017700513\n",
      "iter  1556  loss  -0.047156397  grad l2 norm  0.001779825\n",
      "iter  1557  loss  -0.04716161  grad l2 norm  0.0017894204\n",
      "iter  1558  loss  -0.047166817  grad l2 norm  0.0017988166\n",
      "iter  1559  loss  -0.047172036  grad l2 norm  0.0018079958\n",
      "iter  1560  loss  -0.047177266  grad l2 norm  0.0018169384\n",
      "iter  1561  loss  -0.047182523  grad l2 norm  0.0018256236\n",
      "iter  1562  loss  -0.047187768  grad l2 norm  0.0018340359\n",
      "iter  1563  loss  -0.04719303  grad l2 norm  0.0018421585\n",
      "iter  1564  loss  -0.047198325  grad l2 norm  0.0018499754\n",
      "iter  1565  loss  -0.047203604  grad l2 norm  0.001857472\n",
      "iter  1566  loss  -0.0472089  grad l2 norm  0.0018646361\n",
      "iter  1567  loss  -0.047214214  grad l2 norm  0.0018714546\n",
      "iter  1568  loss  -0.047219533  grad l2 norm  0.0018779202\n",
      "iter  1569  loss  -0.04722486  grad l2 norm  0.0018840216\n",
      "iter  1570  loss  -0.047230203  grad l2 norm  0.0018897508\n",
      "iter  1571  loss  -0.047235563  grad l2 norm  0.001895103\n",
      "iter  1572  loss  -0.047240905  grad l2 norm  0.0019000735\n",
      "iter  1573  loss  -0.047246277  grad l2 norm  0.0019046571\n",
      "iter  1574  loss  -0.04725164  grad l2 norm  0.0019088535\n",
      "iter  1575  loss  -0.047257036  grad l2 norm  0.0019126608\n",
      "iter  1576  loss  -0.047262423  grad l2 norm  0.0019160804\n",
      "iter  1577  loss  -0.04726781  grad l2 norm  0.0019191124\n",
      "iter  1578  loss  -0.04727321  grad l2 norm  0.0019217622\n",
      "iter  1579  loss  -0.04727861  grad l2 norm  0.0019240318\n",
      "iter  1580  loss  -0.047284022  grad l2 norm  0.0019259257\n",
      "iter  1581  loss  -0.04728943  grad l2 norm  0.0019274537\n",
      "iter  1582  loss  -0.04729485  grad l2 norm  0.0019286203\n",
      "iter  1583  loss  -0.04730028  grad l2 norm  0.0019294369\n",
      "iter  1584  loss  -0.047305703  grad l2 norm  0.0019299117\n",
      "iter  1585  loss  -0.04731112  grad l2 norm  0.0019300573\n",
      "iter  1586  loss  -0.047316555  grad l2 norm  0.0019298856\n",
      "iter  1587  loss  -0.047321983  grad l2 norm  0.0019294097\n",
      "iter  1588  loss  -0.047327418  grad l2 norm  0.0019286444\n",
      "iter  1589  loss  -0.047332834  grad l2 norm  0.0019276069\n",
      "iter  1590  loss  -0.047338262  grad l2 norm  0.0019263133\n",
      "iter  1591  loss  -0.047343694  grad l2 norm  0.0019247818\n",
      "iter  1592  loss  -0.04734911  grad l2 norm  0.0019230313\n",
      "iter  1593  loss  -0.047354538  grad l2 norm  0.0019210828\n",
      "iter  1594  loss  -0.04735996  grad l2 norm  0.0019189575\n",
      "iter  1595  loss  -0.04736537  grad l2 norm  0.0019166798\n",
      "iter  1596  loss  -0.047370784  grad l2 norm  0.001914269\n",
      "iter  1597  loss  -0.047376182  grad l2 norm  0.0019117529\n",
      "iter  1598  loss  -0.047381595  grad l2 norm  0.0019091557\n",
      "iter  1599  loss  -0.047386985  grad l2 norm  0.001906501\n",
      "iter  1600  loss  -0.047392383  grad l2 norm  0.0019038174\n",
      "iter  1601  loss  -0.047397755  grad l2 norm  0.0019011307\n",
      "iter  1602  loss  -0.04740314  grad l2 norm  0.0018984681\n",
      "iter  1603  loss  -0.047408495  grad l2 norm  0.0018958556\n",
      "iter  1604  loss  -0.047413863  grad l2 norm  0.0018933226\n",
      "iter  1605  loss  -0.047419216  grad l2 norm  0.0018908955\n",
      "iter  1606  loss  -0.04742456  grad l2 norm  0.0018886015\n",
      "iter  1607  loss  -0.047429897  grad l2 norm  0.0018864677\n",
      "iter  1608  loss  -0.047435228  grad l2 norm  0.0018845208\n",
      "iter  1609  loss  -0.04744054  grad l2 norm  0.0018827843\n",
      "iter  1610  loss  -0.047445845  grad l2 norm  0.0018812864\n",
      "iter  1611  loss  -0.047451142  grad l2 norm  0.0018800478\n",
      "iter  1612  loss  -0.047456414  grad l2 norm  0.0018790928\n",
      "iter  1613  loss  -0.047461703  grad l2 norm  0.001878442\n",
      "iter  1614  loss  -0.04746696  grad l2 norm  0.0018781153\n",
      "iter  1615  loss  -0.047472216  grad l2 norm  0.0018781282\n",
      "iter  1616  loss  -0.04747746  grad l2 norm  0.0018784992\n",
      "iter  1617  loss  -0.047482688  grad l2 norm  0.0018792407\n",
      "iter  1618  loss  -0.047487903  grad l2 norm  0.0018803654\n",
      "iter  1619  loss  -0.04749312  grad l2 norm  0.0018818822\n",
      "iter  1620  loss  -0.047498297  grad l2 norm  0.0018837987\n",
      "iter  1621  loss  -0.047503497  grad l2 norm  0.0018861188\n",
      "iter  1622  loss  -0.047508664  grad l2 norm  0.0018888445\n",
      "iter  1623  loss  -0.047513813  grad l2 norm  0.0018919751\n",
      "iter  1624  loss  -0.047518965  grad l2 norm  0.0018955067\n",
      "iter  1625  loss  -0.0475241  grad l2 norm  0.0018994338\n",
      "iter  1626  loss  -0.047529235  grad l2 norm  0.0019037472\n",
      "iter  1627  loss  -0.047534335  grad l2 norm  0.0019084372\n",
      "iter  1628  loss  -0.04753944  grad l2 norm  0.0019134883\n",
      "iter  1629  loss  -0.04754453  grad l2 norm  0.0019188846\n",
      "iter  1630  loss  -0.047549613  grad l2 norm  0.0019246065\n",
      "iter  1631  loss  -0.047554668  grad l2 norm  0.0019306348\n",
      "iter  1632  loss  -0.04755973  grad l2 norm  0.0019369442\n",
      "iter  1633  loss  -0.047564786  grad l2 norm  0.0019435113\n",
      "iter  1634  loss  -0.047569804  grad l2 norm  0.0019503065\n",
      "iter  1635  loss  -0.04757485  grad l2 norm  0.0019573064\n",
      "iter  1636  loss  -0.047579844  grad l2 norm  0.0019644757\n",
      "iter  1637  loss  -0.04758485  grad l2 norm  0.0019717864\n",
      "iter  1638  loss  -0.04758984  grad l2 norm  0.0019792048\n",
      "iter  1639  loss  -0.04759482  grad l2 norm  0.0019866978\n",
      "iter  1640  loss  -0.04759979  grad l2 norm  0.001994235\n",
      "iter  1641  loss  -0.047604762  grad l2 norm  0.0020017792\n",
      "iter  1642  loss  -0.047609705  grad l2 norm  0.0020092968\n",
      "iter  1643  loss  -0.04761463  grad l2 norm  0.002016755\n",
      "iter  1644  loss  -0.047619566  grad l2 norm  0.0020241204\n",
      "iter  1645  loss  -0.04762449  grad l2 norm  0.0020313584\n",
      "iter  1646  loss  -0.047629394  grad l2 norm  0.0020384376\n",
      "iter  1647  loss  -0.047634304  grad l2 norm  0.0020453262\n",
      "iter  1648  loss  -0.047639187  grad l2 norm  0.0020519942\n",
      "iter  1649  loss  -0.04764405  grad l2 norm  0.0020584082\n",
      "iter  1650  loss  -0.047648933  grad l2 norm  0.002064544\n",
      "iter  1651  loss  -0.047653794  grad l2 norm  0.0020703715\n",
      "iter  1652  loss  -0.047658633  grad l2 norm  0.002075865\n",
      "iter  1653  loss  -0.04766348  grad l2 norm  0.0020809995\n",
      "iter  1654  loss  -0.0476683  grad l2 norm  0.0020857481\n",
      "iter  1655  loss  -0.04767311  grad l2 norm  0.0020900914\n",
      "iter  1656  loss  -0.04767792  grad l2 norm  0.002094007\n",
      "iter  1657  loss  -0.04768271  grad l2 norm  0.002097472\n",
      "iter  1658  loss  -0.047687493  grad l2 norm  0.0021004698\n",
      "iter  1659  loss  -0.047692265  grad l2 norm  0.0021029802\n",
      "iter  1660  loss  -0.047697023  grad l2 norm  0.0021049855\n",
      "iter  1661  loss  -0.04770177  grad l2 norm  0.0021064694\n",
      "iter  1662  loss  -0.047706515  grad l2 norm  0.0021074149\n",
      "iter  1663  loss  -0.04771122  grad l2 norm  0.0021078067\n",
      "iter  1664  loss  -0.047715936  grad l2 norm  0.0021076335\n",
      "iter  1665  loss  -0.047720633  grad l2 norm  0.0021068826\n",
      "iter  1666  loss  -0.047725324  grad l2 norm  0.002105537\n",
      "iter  1667  loss  -0.04773  grad l2 norm  0.0021035883\n",
      "iter  1668  loss  -0.047734648  grad l2 norm  0.0021010283\n",
      "iter  1669  loss  -0.047739293  grad l2 norm  0.002097843\n",
      "iter  1670  loss  -0.047743928  grad l2 norm  0.00209403\n",
      "iter  1671  loss  -0.047748547  grad l2 norm  0.0020895782\n",
      "iter  1672  loss  -0.04775315  grad l2 norm  0.0020844836\n",
      "iter  1673  loss  -0.04775773  grad l2 norm  0.0020787427\n",
      "iter  1674  loss  -0.047762297  grad l2 norm  0.0020723504\n",
      "iter  1675  loss  -0.047766853  grad l2 norm  0.0020653086\n",
      "iter  1676  loss  -0.047771383  grad l2 norm  0.002057613\n",
      "iter  1677  loss  -0.047775913  grad l2 norm  0.002049266\n",
      "iter  1678  loss  -0.047780413  grad l2 norm  0.0020402744\n",
      "iter  1679  loss  -0.047784895  grad l2 norm  0.0020306364\n",
      "iter  1680  loss  -0.04778935  grad l2 norm  0.0020203602\n",
      "iter  1681  loss  -0.047793783  grad l2 norm  0.0020094535\n",
      "iter  1682  loss  -0.04779822  grad l2 norm  0.0019979232\n",
      "iter  1683  loss  -0.0478026  grad l2 norm  0.001985779\n",
      "iter  1684  loss  -0.047806982  grad l2 norm  0.0019730318\n",
      "iter  1685  loss  -0.047811333  grad l2 norm  0.0019596943\n",
      "iter  1686  loss  -0.047815666  grad l2 norm  0.0019457781\n",
      "iter  1687  loss  -0.047819957  grad l2 norm  0.0019312967\n",
      "iter  1688  loss  -0.047824237  grad l2 norm  0.0019162648\n",
      "iter  1689  loss  -0.047828484  grad l2 norm  0.0019006985\n",
      "iter  1690  loss  -0.047832713  grad l2 norm  0.0018846184\n",
      "iter  1691  loss  -0.047836896  grad l2 norm  0.0018680341\n",
      "iter  1692  loss  -0.047841053  grad l2 norm  0.0018509664\n",
      "iter  1693  loss  -0.047845185  grad l2 norm  0.0018334343\n",
      "iter  1694  loss  -0.04784928  grad l2 norm  0.0018154575\n",
      "iter  1695  loss  -0.047853336  grad l2 norm  0.0017970531\n",
      "iter  1696  loss  -0.04785737  grad l2 norm  0.0017782409\n",
      "iter  1697  loss  -0.047861367  grad l2 norm  0.0017590424\n",
      "iter  1698  loss  -0.047865335  grad l2 norm  0.0017394768\n",
      "iter  1699  loss  -0.04786926  grad l2 norm  0.0017195664\n",
      "iter  1700  loss  -0.04787315  grad l2 norm  0.0016993309\n",
      "iter  1701  loss  -0.04787702  grad l2 norm  0.001678792\n",
      "iter  1702  loss  -0.047880832  grad l2 norm  0.0016579714\n",
      "iter  1703  loss  -0.04788461  grad l2 norm  0.001636891\n",
      "iter  1704  loss  -0.047888353  grad l2 norm  0.001615573\n",
      "iter  1705  loss  -0.047892064  grad l2 norm  0.0015940369\n",
      "iter  1706  loss  -0.04789573  grad l2 norm  0.0015723056\n",
      "iter  1707  loss  -0.047899365  grad l2 norm  0.0015504004\n",
      "iter  1708  loss  -0.047902934  grad l2 norm  0.0015283449\n",
      "iter  1709  loss  -0.04790648  grad l2 norm  0.0015061586\n",
      "iter  1710  loss  -0.04790997  grad l2 norm  0.0014838633\n",
      "iter  1711  loss  -0.047913447  grad l2 norm  0.001461482\n",
      "iter  1712  loss  -0.047916867  grad l2 norm  0.0014390325\n",
      "iter  1713  loss  -0.047920264  grad l2 norm  0.0014165415\n",
      "iter  1714  loss  -0.047923602  grad l2 norm  0.0013940216\n",
      "iter  1715  loss  -0.047926906  grad l2 norm  0.0013715\n",
      "iter  1716  loss  -0.047930174  grad l2 norm  0.0013489936\n",
      "iter  1717  loss  -0.047933407  grad l2 norm  0.0013265228\n",
      "iter  1718  loss  -0.04793659  grad l2 norm  0.0013041061\n",
      "iter  1719  loss  -0.047939733  grad l2 norm  0.0012817644\n",
      "iter  1720  loss  -0.047942836  grad l2 norm  0.0012595159\n",
      "iter  1721  loss  -0.047945898  grad l2 norm  0.0012373787\n",
      "iter  1722  loss  -0.04794893  grad l2 norm  0.0012153735\n",
      "iter  1723  loss  -0.047951914  grad l2 norm  0.001193516\n",
      "iter  1724  loss  -0.047954865  grad l2 norm  0.0011718252\n",
      "iter  1725  loss  -0.047957774  grad l2 norm  0.0011503184\n",
      "iter  1726  loss  -0.047960658  grad l2 norm  0.0011290131\n",
      "iter  1727  loss  -0.047963507  grad l2 norm  0.0011079286\n",
      "iter  1728  loss  -0.047966305  grad l2 norm  0.0010870795\n",
      "iter  1729  loss  -0.047969088  grad l2 norm  0.0010664875\n",
      "iter  1730  loss  -0.047971826  grad l2 norm  0.0010461652\n",
      "iter  1731  loss  -0.047974538  grad l2 norm  0.0010261347\n",
      "iter  1732  loss  -0.0479772  grad l2 norm  0.0010064134\n",
      "iter  1733  loss  -0.04797985  grad l2 norm  0.0009870171\n",
      "done in  1733  iterations 0.0009870171\n"
     ]
    }
   ],
   "source": [
    "solver.solve(max_iter=15_000, eps=1e-3, alpha=1+1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sol = solver.solution\n",
    "# X = sol['X']\n",
    "# vertices = vmap(get_fk)(X).translation().astype(np.float32).T\n",
    "# scene.vis['lines_segments'].set_object(mc_geom.Line(mc_geom.PointsGeometry(vertices), mc_geom.MeshLambertMaterial(color=0xff0000)))\n",
    "# ee_frame = scene.vis['ee']\n",
    "# ee_frame.set_object(mc_geom.triad(scale=0.2))\n",
    "# for q in X:\n",
    "#     _tf = np.array(get_fk(q).as_matrix()).astype(np.float64)\n",
    "#     ee_frame.set_transform(_tf)\n",
    "#     for i, jt in enumerate(q):\n",
    "#         robot[i] = jt\n",
    "#     time.sleep(0.01)\n",
    "#     scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solver.solution\n",
    "X = sol['X']\n",
    "\n",
    "# vertices = vmap(get_fk)(X).translation().astype(np.float32).T\n",
    "vertices = X.astype(np.float32).T\n",
    "\n",
    "scene.vis['lines_segments'].set_object(mc_geom.Line(mc_geom.PointsGeometry(vertices), mc_geom.MeshLambertMaterial(color=0xff0000, linewidth=2)))\n",
    "\n",
    "\n",
    "# robot_img = []\n",
    "# for i, q in enumerate(X[::int(T/5)]):\n",
    "#     # _tf = np.array(get_fk(q).as_matrix()).astype(np.float64)\n",
    "#     # ee_frame.set_transform(_tf)\n",
    "#     _robot = Robot(name='robot{}'.format(i),urdf_path=PandaLoader().df_path, mesh_folder_path=Path(PandaLoader().model_path).parent.parent, opacity=0.75)\n",
    "#     robot_img.append(_robot)\n",
    "#     scene.add_robot(_robot)\n",
    "#     for j, jt in enumerate(q):\n",
    "#         _robot[j] = jt\n",
    "# \"Render the initial scene\"\n",
    "scene.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ps.set_up_dir(\"z_up\")\n",
    "# # ps.set_front_dir(\"neg_y_front\")\n",
    "# ps.init()\n",
    "\n",
    "\n",
    "# ps_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "# # ps_points = ps.register_point_cloud(\"sampled points\", args['points'][:,:3])\n",
    "# # ps_mesh.add_scalar_quantity(\"face vals\",vmap(info_distr, in_axes=(0,))(mesh.vertices))\n",
    "\n",
    "# # ps_points.add_scalar_quantity(\"results\", args['P_XI'])\n",
    "\n",
    "# # ps_bunny_mesh.set_transparency(0.8)\n",
    "# # ps_bunny_mesh.add_scalar_quantity('info_distr', mesh_func.func_vals, defined_on='vertices', cmap='blues')\n",
    "\n",
    "# ps_traj         = ps.register_curve_network(\"trajectory\", vmap(get_fk)(X).translation() , edges=\"line\")\n",
    "\n",
    "\n",
    "# for _ in range(1000):\n",
    "#     solver.solve(eps=1e-5, max_iter=10)\n",
    "#     sol = solver.solution\n",
    "#     X = sol['X']\n",
    "\n",
    "#     ps_traj.update_node_positions(vmap(get_fk)(X).translation() )\n",
    "#     # ps_traj.add_vector_quantity(\"vec img\", X[:,:3], enabled=True)\n",
    "#     ps.frame_tick()\n",
    "\n",
    "#     time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solver.solve(eps=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solver.solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 4.1 Metal - 88\n"
     ]
    }
   ],
   "source": [
    "X = sol['X']\n",
    "\n",
    "ps.init()\n",
    "\n",
    "ps_bunny_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "# ps_bunny_mesh.set_transparency(0.8)\n",
    "ps_bunny_mesh.add_scalar_quantity('info_distr', vmap(info_distr, in_axes=(0,))(mesh.vertices), defined_on='vertices', cmap='blues')\n",
    "\n",
    "ps_traj         = ps.register_curve_network(\"trajectory\", vmap(get_fk)(X).translation() , edges=\"line\")\n",
    "\n",
    "ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
