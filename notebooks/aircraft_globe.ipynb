{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1726165587.059321       1 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.\n"
     ]
    }
   ],
   "source": [
    "import sys \n",
    "sys.path.append('../')\n",
    "import jax \n",
    "jax.config.update('jax_enable_x64', True)\n",
    "\n",
    "import jax.numpy as jnp \n",
    "from jaxlie import SE3, SO3\n",
    "import numpy as np\n",
    "from jax import vmap\n",
    "import jax.random as jax_random\n",
    "# from ergodic_mmd.aug_lagrange_jaxopt import AugmentedLagrangeSolver\n",
    "from ergodic_mmd.aug_lagrange_solver import AugmentedLagrangeSolver\n",
    "\n",
    "import adam\n",
    "from adam.jax import KinDynComputations\n",
    "\n",
    "from plyfile import PlyData\n",
    "import trimesh as tm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from IPython.display import clear_output\n",
    "import polyscope as ps\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plydata = PlyData.read('../assets/sphere.ply')\n",
    "verts = np.vstack((\n",
    "    plydata['vertex']['x'],\n",
    "    plydata['vertex']['y'],\n",
    "    plydata['vertex']['z']\n",
    ")).T\n",
    "faces = np.array(np.vstack(plydata['face']['vertex_indices']))\n",
    "\n",
    "mesh = tm.Trimesh(vertices=verts, faces=faces)\n",
    "\n",
    "num_points = 1000  # Change this number based on your requirement\n",
    "points, face_indices = tm.sample.sample_surface(mesh, num_points)\n",
    "\n",
    "h = 0.1\n",
    "seed = 0 \n",
    "key = jax_random.PRNGKey(seed)\n",
    "key, subkey = jax_random.split(key, 2)\n",
    "_std = 0.1*jax_random.normal(subkey, shape=(num_points,1))\n",
    "_std = _std - _std.min()+0.1\n",
    "_points = points + _std*mesh.face_normals[face_indices]\n",
    "_points = mesh.vertices + 0.2 * mesh.vertex_normals\n",
    "\n",
    "info_distr = lambda x: (jnp.sin(x[0]*x[1])+1)*jnp.exp(-60*(x[0]-0.5)**2 - 10*(x[1]-0.5)**2) + 2*jnp.exp(-30*(x[0]-1.)**2 - 30*(x[1]-0.5)**2 - 20*(x[2])**2)\n",
    "P_XI = vmap(info_distr, in_axes=(0,))(mesh.vertices)\n",
    "P_XI = P_XI-P_XI.min()+0.01\n",
    "P_XI = P_XI/jnp.sum(P_XI)\n",
    "\n",
    "# args = {'h' : h, 'points' : jnp.hstack([_points,mesh.vertex_normals]), 'P_XI' : P_XI, \n",
    "#         'x0' : jnp.hstack([_points[0], jnp.zeros(3)]), 'xf' : jnp.hstack([_points[-1], jnp.zeros(3)])}\n",
    "args = {'h' : h, 'points' : jnp.hstack([_points,mesh.vertex_normals]), 'P_XI' : P_XI, \n",
    "        'x0' : jnp.hstack([_points[0], jnp.zeros(3)]), 'xf' : jnp.hstack([_points[-1], jnp.zeros(3)])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps.init()\n",
    "\n",
    "# ps_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "# ps_mesh.add_scalar_quantity(\"face vals\",vmap(info_distr, in_axes=(0,))(mesh.vertices))\n",
    "# ps_points = ps.register_point_cloud(\"sampled points\", args['points'])\n",
    "# ps_points.add_scalar_quantity(\"results\", args['P_XI'])\n",
    "\n",
    "\n",
    "# ps.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeDAirCraftModel(object):\n",
    "    def __init__(self) -> None:\n",
    "        self.dt = 0.1\n",
    "        self.n = 5\n",
    "        self.m = 3 \n",
    "        def dfdt(x, u):\n",
    "            # v  = np.clip(u[0], 0.1, 5)\n",
    "            # w1 = np.clip(u[1], -15,15)\n",
    "            # w2 = np.clip(u[2], -15,15) \n",
    "            v  = u[0]\n",
    "            w1 = u[1]\n",
    "            w2 = u[2]\n",
    "            w3 = u[3]\n",
    "            return jnp.array([\n",
    "                v * jnp.cos(x[4]) * jnp.cos(x[3]),\n",
    "                v * jnp.sin(x[4]) * jnp.cos(x[3]),\n",
    "                v * jnp.sin(x[3]),\n",
    "                w1, \n",
    "                w2,\n",
    "                w3\n",
    "            ]) \n",
    "        def f(x, u):\n",
    "            return x + self.dt * dfdt(x, u)\n",
    "        self.f      = f\n",
    "        self.dfdt   = dfdt\n",
    "\n",
    "robot_model = ThreeDAirCraftModel()\n",
    "\n",
    "def f_lie_constr(twist2, twist1, U):\n",
    "    dtwist = jnp.array([U[0],U[1],U[2],0.,0.,0.])\n",
    "    # dtwist = jnp.array([U[0],0.,0.,U[1],U[2], U[3]])\n",
    "    # return SE3.log(SE3.exp(twist2).inverse()@SE3.exp(twist1+dtwist))\n",
    "    return twist2 - (twist1 + dtwist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def RBF_kernel(x, xp, h=0.01):\n",
    "#     return jnp.exp(\n",
    "#         -jnp.sum((x-xp)**2)/h\n",
    "#     )\n",
    "def RBF_kernel(x, xp, h=0.01):\n",
    "    return jnp.exp(\n",
    "        -jnp.sum(SE3.log(SE3.exp(x).inverse()@SE3.exp(xp))**2)/h\n",
    "    )\n",
    "def create_kernel_matrix(kernel):\n",
    "    return vmap(vmap(kernel, in_axes=(0, None, None)), in_axes=(None, 0, None))\n",
    "\n",
    "KernelMatrix = create_kernel_matrix(RBF_kernel)\n",
    "def emmd_loss(params, args):\n",
    "    X = params['X']\n",
    "    T = X.shape[0]\n",
    "    p, w = jnp.split(X, 2, axis=1)\n",
    "    h = args['h']\n",
    "    points    = args['points']\n",
    "    P_XI      = args['P_XI']\n",
    "    return np.sum(KernelMatrix(X, X, h))/(T**2) \\\n",
    "            - 2 * np.sum(P_XI @ KernelMatrix(X, points, h))/T\n",
    "\n",
    "def eq_constr(params, args):\n",
    "    X = params['X']\n",
    "    U = params['U']\n",
    "    # p, w = jnp.split(X, 2, axis=1)\n",
    "    return jnp.vstack([\n",
    "        (X[0]-args['x0']),\n",
    "        # vmap(f_lie_constr)(X[1:], X[:-1], U[:-1]),\n",
    "        (X[-1]-args['xf'])\n",
    "    ])\n",
    "\n",
    "def ineq_constr(params, args):\n",
    "    return jnp.array(0.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 60\n",
    "# X = jnp.linspace(mesh.bounds[0], mesh.bounds[1], num=T)\n",
    "X = jnp.linspace(args['x0'], args['xf'], num=T)\n",
    "\n",
    "U = jnp.zeros((T, 4)) # forward v, theta dot, phi dot\n",
    "\n",
    "\n",
    "params = {'X' : X, 'U' : U}\n",
    "# solver = AugmentedLagrangeSolver(params, emmd_loss, eq_constr, ineq_constr, max_stepsize=1e-1, args=args)\n",
    "solver = AugmentedLagrangeSolver(params, emmd_loss, eq_constr, ineq_constr, args=args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[polyscope] Backend: openGL3_glfw -- Loaded openGL version: 4.1 Metal - 88\n",
      "iter  0  loss  0.2747486930601284  grad l2 norm  0.041355058792227686\n",
      "iter  1  loss  0.27313244373614204  grad l2 norm  0.033387571971953495\n",
      "iter  2  loss  0.2720367922418861  grad l2 norm  0.0321649470505682\n",
      "iter  3  loss  0.27106980011431236  grad l2 norm  0.03206749922495541\n",
      "iter  4  loss  0.2700018003637458  grad l2 norm  0.03277437644877966\n",
      "iter  5  loss  0.2687305767995958  grad l2 norm  0.03369890226538204\n",
      "iter  6  loss  0.26726775387381607  grad l2 norm  0.034640993162580805\n",
      "iter  7  loss  0.26566395919452074  grad l2 norm  0.03558756571940598\n",
      "iter  8  loss  0.2639648946433402  grad l2 norm  0.0365528095343027\n",
      "iter  9  loss  0.26220256586988744  grad l2 norm  0.03755961026956408\n",
      "unsuccessful, tol:  0.03755961026956408\n",
      "iter  0  loss  0.26039851391577995  grad l2 norm  0.038631243398056646\n",
      "iter  1  loss  0.25856432655336103  grad l2 norm  0.03978127517212304\n",
      "iter  2  loss  0.25670319782654805  grad l2 norm  0.04098780502668547\n",
      "iter  3  loss  0.2548135211319157  grad l2 norm  0.042188714323631256\n",
      "iter  4  loss  0.2528946796363151  grad l2 norm  0.043317515105872555\n",
      "iter  5  loss  0.25095087914389863  grad l2 norm  0.044343890778653784\n",
      "iter  6  loss  0.24898969504007304  grad l2 norm  0.04528442466894253\n",
      "iter  7  loss  0.2470177848812144  grad l2 norm  0.04618051926564771\n",
      "iter  8  loss  0.24503756208114694  grad l2 norm  0.04706414663655135\n",
      "iter  9  loss  0.24304712168243228  grad l2 norm  0.047937696597248405\n",
      "unsuccessful, tol:  0.047937696597248405\n",
      "iter  0  loss  0.24104334290850882  grad l2 norm  0.048780919457533656\n",
      "iter  1  loss  0.23902515907378674  grad l2 norm  0.049573611379246055\n",
      "iter  2  loss  0.2369941154506767  grad l2 norm  0.05031056273040577\n",
      "iter  3  loss  0.2349526265199914  grad l2 norm  0.050999509839771064\n",
      "iter  4  loss  0.23290226291878774  grad l2 norm  0.05165033777087979\n",
      "iter  5  loss  0.2308432102090574  grad l2 norm  0.052266279204069246\n",
      "iter  6  loss  0.2287749519850282  grad l2 norm  0.05284305417327555\n",
      "iter  7  loss  0.22669755989523024  grad l2 norm  0.05337556374406504\n",
      "iter  8  loss  0.22461220886189945  grad l2 norm  0.05386404488354416\n",
      "iter  9  loss  0.22252055560664488  grad l2 norm  0.05431303482644417\n",
      "unsuccessful, tol:  0.05431303482644417\n",
      "iter  0  loss  0.22042393889909184  grad l2 norm  0.054726187077301015\n",
      "iter  1  loss  0.21832332095415172  grad l2 norm  0.05510361138749729\n",
      "iter  2  loss  0.21621973447024012  grad l2 norm  0.055443660694517005\n",
      "iter  3  loss  0.21411445387409234  grad l2 norm  0.055745903800748606\n",
      "iter  4  loss  0.2120090637017803  grad l2 norm  0.056013222639273726\n",
      "iter  5  loss  0.2099068697499506  grad l2 norm  0.05625964185331671\n",
      "iter  6  loss  0.2078034638348296  grad l2 norm  0.056454708287952085\n",
      "iter  7  loss  0.2057026725260416  grad l2 norm  0.05662390211654346\n",
      "iter  8  loss  0.2036029333094361  grad l2 norm  0.05674762634577698\n",
      "iter  9  loss  0.20150685998993498  grad l2 norm  0.056840225223027716\n",
      "unsuccessful, tol:  0.056840225223027716\n",
      "iter  0  loss  0.19941345245345105  grad l2 norm  0.05688746020275163\n",
      "iter  1  loss  0.1973225214503714  grad l2 norm  0.05689585788084784\n",
      "iter  2  loss  0.19524241860139155  grad l2 norm  0.05689448680459266\n",
      "iter  3  loss  0.1931702161019256  grad l2 norm  0.05686017253884593\n",
      "iter  4  loss  0.19110718879916727  grad l2 norm  0.0568020698240877\n",
      "iter  5  loss  0.18905418351376907  grad l2 norm  0.056710915514936865\n",
      "iter  6  loss  0.18701277928897805  grad l2 norm  0.05659980045418245\n",
      "iter  7  loss  0.1849835512416233  grad l2 norm  0.05645497795807449\n",
      "iter  8  loss  0.1829683046419152  grad l2 norm  0.056298875090196314\n",
      "iter  9  loss  0.18096678260192786  grad l2 norm  0.056103309563308855\n",
      "unsuccessful, tol:  0.056103309563308855\n",
      "iter  0  loss  0.17898091891811285  grad l2 norm  0.05590418524732114\n",
      "iter  1  loss  0.1770106569732159  grad l2 norm  0.05566878402366996\n",
      "iter  2  loss  0.17505744119465716  grad l2 norm  0.05542736301580991\n",
      "iter  3  loss  0.1731208891283889  grad l2 norm  0.05516212698875689\n",
      "iter  4  loss  0.17119870578452348  grad l2 norm  0.05486643968151661\n",
      "iter  5  loss  0.16929414118759883  grad l2 norm  0.054553747442472156\n",
      "iter  6  loss  0.16740885212224044  grad l2 norm  0.054237082722807285\n",
      "iter  7  loss  0.16554083181196622  grad l2 norm  0.053895182462326115\n",
      "iter  8  loss  0.16369035177297747  grad l2 norm  0.05353859783437061\n",
      "iter  9  loss  0.16185975264390606  grad l2 norm  0.05316986560484741\n",
      "unsuccessful, tol:  0.05316986560484741\n",
      "iter  0  loss  0.16004933209781452  grad l2 norm  0.05279719872222866\n",
      "iter  1  loss  0.15825861564627386  grad l2 norm  0.05241127836108064\n",
      "iter  2  loss  0.15648829679583984  grad l2 norm  0.05201396435023463\n",
      "iter  3  loss  0.15473929558151153  grad l2 norm  0.051622542212213816\n",
      "iter  4  loss  0.1530095923057669  grad l2 norm  0.051195566524737134\n",
      "iter  5  loss  0.1513003127642128  grad l2 norm  0.050779666432070575\n",
      "iter  6  loss  0.14961078293820135  grad l2 norm  0.050330718408518396\n",
      "iter  7  loss  0.1479424999494407  grad l2 norm  0.04989012249064478\n",
      "iter  8  loss  0.1462965568911596  grad l2 norm  0.04944144815415655\n",
      "iter  9  loss  0.14467151829802524  grad l2 norm  0.04899566800724671\n",
      "unsuccessful, tol:  0.04899566800724671\n",
      "iter  0  loss  0.14306563711924505  grad l2 norm  0.0485252766157135\n",
      "iter  1  loss  0.14148085541102412  grad l2 norm  0.0480620680102443\n",
      "iter  2  loss  0.13991746709778324  grad l2 norm  0.0475911030135548\n",
      "iter  3  loss  0.1383754724489288  grad l2 norm  0.04713088240007677\n",
      "iter  4  loss  0.13685384765686615  grad l2 norm  0.04665624447805674\n",
      "iter  5  loss  0.13535304095677386  grad l2 norm  0.04619671246961596\n",
      "iter  6  loss  0.1338715631627222  grad l2 norm  0.04571399291795406\n",
      "iter  7  loss  0.13241027441701783  grad l2 norm  0.04524781793219367\n",
      "iter  8  loss  0.13096799052513552  grad l2 norm  0.04476054147910007\n",
      "iter  9  loss  0.12954602669703869  grad l2 norm  0.04429166822825442\n",
      "unsuccessful, tol:  0.04429166822825442\n",
      "iter  0  loss  0.1281435612938282  grad l2 norm  0.04380898106797794\n",
      "iter  1  loss  0.12676059529872477  grad l2 norm  0.04334103065091099\n",
      "iter  2  loss  0.12539595444992854  grad l2 norm  0.0428529493211474\n",
      "iter  3  loss  0.12405118843389576  grad l2 norm  0.04238424232499225\n",
      "iter  4  loss  0.12272542914658624  grad l2 norm  0.0419029681727886\n",
      "iter  5  loss  0.12141933495079707  grad l2 norm  0.041441454038768095\n",
      "iter  6  loss  0.1201316010794424  grad l2 norm  0.040964125526755574\n",
      "iter  7  loss  0.11886301253895641  grad l2 norm  0.04050696371918629\n",
      "iter  8  loss  0.1176122135543514  grad l2 norm  0.0400316343948611\n",
      "iter  9  loss  0.1163801131435318  grad l2 norm  0.03957640829821361\n",
      "unsuccessful, tol:  0.03957640829821361\n",
      "iter  0  loss  0.11516560833722786  grad l2 norm  0.03910543077939507\n",
      "iter  1  loss  0.11396967256875647  grad l2 norm  0.03865697919848874\n",
      "iter  2  loss  0.11279077526899024  grad l2 norm  0.03819190464206945\n",
      "iter  3  loss  0.11162973007000324  grad l2 norm  0.03774836027796106\n",
      "iter  4  loss  0.11048529444759128  grad l2 norm  0.037289706818469624\n",
      "iter  5  loss  0.10935845895397453  grad l2 norm  0.036854952648644836\n",
      "iter  6  loss  0.10824776955571805  grad l2 norm  0.03640683039975463\n",
      "iter  7  loss  0.1071539771671811  grad l2 norm  0.035981784260092034\n",
      "iter  8  loss  0.10607558607901538  grad l2 norm  0.03554176305922442\n",
      "iter  9  loss  0.10501343435157477  grad l2 norm  0.03512396644045169\n",
      "unsuccessful, tol:  0.03512396644045169\n",
      "iter  0  loss  0.10396611981916508  grad l2 norm  0.03469053709541672\n",
      "iter  1  loss  0.10293465627340015  grad l2 norm  0.03427959160951202\n",
      "iter  2  loss  0.10191771771696596  grad l2 norm  0.03385364090498819\n",
      "iter  3  loss  0.10091631269342774  grad l2 norm  0.03345065270229121\n",
      "iter  4  loss  0.09992901908776711  grad l2 norm  0.03303181715571025\n",
      "iter  5  loss  0.09895697770098094  grad l2 norm  0.03263655646922823\n",
      "iter  6  loss  0.09799884164148816  grad l2 norm  0.032227146831759985\n",
      "iter  7  loss  0.09705564740196736  grad l2 norm  0.03184144554906138\n",
      "iter  8  loss  0.09612597704702704  grad l2 norm  0.031441717528955956\n",
      "iter  9  loss  0.09521079549992643  grad l2 norm  0.031064731786848043\n",
      "unsuccessful, tol:  0.031064731786848043\n",
      "iter  0  loss  0.09430869506691192  grad l2 norm  0.030673232542125478\n",
      "iter  1  loss  0.09342066175134554  grad l2 norm  0.0303038304523032\n",
      "iter  2  loss  0.09254533181792098  grad l2 norm  0.029920152177908204\n",
      "iter  3  loss  0.09168373082037751  grad l2 norm  0.02955913804149089\n",
      "iter  4  loss  0.09083443866561837  grad l2 norm  0.02918395179588561\n",
      "iter  5  loss  0.08999846510002144  grad l2 norm  0.02883202482393061\n",
      "iter  6  loss  0.08917435924917967  grad l2 norm  0.02846649617753138\n",
      "iter  7  loss  0.08836309393099029  grad l2 norm  0.02812494721395278\n",
      "iter  8  loss  0.08756317150159867  grad l2 norm  0.027770777869745935\n",
      "iter  9  loss  0.08677544694668614  grad l2 norm  0.027440264872958805\n",
      "unsuccessful, tol:  0.027440264872958805\n",
      "iter  0  loss  0.0859984176781303  grad l2 norm  0.02709729489783003\n",
      "iter  1  loss  0.08523291991182888  grad l2 norm  0.026777096770401174\n",
      "iter  2  loss  0.08447753127663668  grad l2 norm  0.026444384742121123\n",
      "iter  3  loss  0.08373315014137744  grad l2 norm  0.02613391990109668\n",
      "iter  4  loss  0.08299842744763146  grad l2 norm  0.02581088517424887\n",
      "iter  5  loss  0.08227431643589567  grad l2 norm  0.025510233200656637\n",
      "iter  6  loss  0.08155947741243393  grad l2 norm  0.02519671835736741\n",
      "iter  7  loss  0.08085490482286765  grad l2 norm  0.024905738706654273\n",
      "iter  8  loss  0.0801592817322735  grad l2 norm  0.024601903558315964\n",
      "iter  9  loss  0.07947359308578547  grad l2 norm  0.024320279629079213\n",
      "unsuccessful, tol:  0.024320279629079213\n",
      "iter  0  loss  0.0787965310848565  grad l2 norm  0.02402573269519757\n",
      "iter  1  loss  0.07812903096758558  grad l2 norm  0.023752528720983768\n",
      "iter  2  loss  0.07746979368570088  grad l2 norm  0.02346638568626774\n",
      "iter  3  loss  0.07681970524978943  grad l2 norm  0.023200813551474322\n",
      "iter  4  loss  0.07617748856477957  grad l2 norm  0.02292230676299536\n",
      "iter  5  loss  0.07554404405946513  grad l2 norm  0.022664221002545632\n",
      "iter  6  loss  0.07491814910716581  grad l2 norm  0.022393105710100093\n",
      "iter  7  loss  0.07430076739860608  grad l2 norm  0.022142652743575315\n",
      "iter  8  loss  0.07369072192850663  grad l2 norm  0.021879033163157504\n",
      "iter  9  loss  0.07308900772770573  grad l2 norm  0.021636131307227063\n",
      "unsuccessful, tol:  0.021636131307227063\n",
      "iter  0  loss  0.07249446022328612  grad l2 norm  0.02138002930045514\n",
      "iter  1  loss  0.0719080474837808  grad l2 norm  0.021144208440436137\n",
      "iter  2  loss  0.0713286044572295  grad l2 norm  0.020895349496713296\n",
      "iter  3  loss  0.07075703970282238  grad l2 norm  0.020666187550744217\n",
      "iter  4  loss  0.07019217493561057  grad l2 norm  0.020424295730304208\n",
      "iter  5  loss  0.06963484735862449  grad l2 norm  0.02020176586884184\n",
      "iter  6  loss  0.06908385194682413  grad l2 norm  0.01996677738070694\n",
      "iter  7  loss  0.06853997266477645  grad l2 norm  0.019751452986036724\n",
      "iter  8  loss  0.06800196411972456  grad l2 norm  0.01952396819620011\n",
      "iter  9  loss  0.06747056161701243  grad l2 norm  0.01931670903963388\n",
      "unsuccessful, tol:  0.01931670903963388\n",
      "iter  0  loss  0.06694447129948565  grad l2 norm  0.019097598426252558\n",
      "iter  1  loss  0.06642435281884466  grad l2 norm  0.01889897120067118\n",
      "iter  2  loss  0.06590885691960169  grad l2 norm  0.018688954765119614\n",
      "iter  3  loss  0.0653985247854353  grad l2 norm  0.018499437365111025\n",
      "iter  4  loss  0.06489195180836399  grad l2 norm  0.018299289298159296\n",
      "iter  5  loss  0.06438955926609592  grad l2 norm  0.018119502360168283\n",
      "iter  6  loss  0.06388997329071942  grad l2 norm  0.017929508335864014\n",
      "iter  7  loss  0.06339367415012533  grad l2 norm  0.017759478823916375\n",
      "iter  8  loss  0.0628995200894593  grad l2 norm  0.017579041349789794\n",
      "iter  9  loss  0.06240821446792108  grad l2 norm  0.017418101508294852\n",
      "unsuccessful, tol:  0.017418101508294852\n",
      "iter  0  loss  0.06191884396950313  grad l2 norm  0.01724619476126959\n",
      "iter  1  loss  0.061432268756783794  grad l2 norm  0.017093091498494088\n",
      "iter  2  loss  0.06094771425411767  grad l2 norm  0.01692824212838717\n",
      "iter  3  loss  0.060466147056416876  grad l2 norm  0.016781089185515626\n",
      "iter  4  loss  0.05998690483547037  grad l2 norm  0.016621598417040574\n",
      "iter  5  loss  0.05951099064155515  grad l2 norm  0.016478661270727835\n",
      "iter  6  loss  0.059037787737423734  grad l2 norm  0.016323097619757792\n",
      "iter  7  loss  0.05856826065071822  grad l2 norm  0.016183054118334955\n",
      "iter  8  loss  0.05810180078901052  grad l2 norm  0.016030102786602517\n",
      "iter  9  loss  0.057639329131442735  grad l2 norm  0.015892044301596384\n",
      "unsuccessful, tol:  0.015892044301596384\n",
      "iter  0  loss  0.057180228569335224  grad l2 norm  0.015740932983999412\n",
      "iter  1  loss  0.05672537140510122  grad l2 norm  0.015604484044736357\n",
      "iter  2  loss  0.0562741010237239  grad l2 norm  0.015454880151431458\n",
      "iter  3  loss  0.055827219616407374  grad l2 norm  0.015319607296152074\n",
      "iter  4  loss  0.05538402500786973  grad l2 norm  0.015171178723592024\n",
      "iter  5  loss  0.05494524334237502  grad l2 norm  0.015036680783014098\n",
      "iter  6  loss  0.05451014965902634  grad l2 norm  0.014889443689065907\n",
      "iter  7  loss  0.054079395100702914  grad l2 norm  0.014755788126201453\n",
      "iter  8  loss  0.05365224911903508  grad l2 norm  0.014609900678801139\n",
      "iter  9  loss  0.053229305475416454  grad l2 norm  0.01447725952488135\n",
      "unsuccessful, tol:  0.01447725952488135\n",
      "iter  0  loss  0.05280984487413111  grad l2 norm  0.014332817644470803\n",
      "iter  1  loss  0.05239444017224244  grad l2 norm  0.01420155958162297\n",
      "iter  2  loss  0.05198238926583602  grad l2 norm  0.014058904597662616\n",
      "iter  3  loss  0.051574259023207454  grad l2 norm  0.013929403450300587\n",
      "iter  4  loss  0.0511693564825552  grad l2 norm  0.01378871918688031\n",
      "iter  5  loss  0.050768251949817204  grad l2 norm  0.013660877303940753\n",
      "iter  6  loss  0.05037030050918548  grad l2 norm  0.01352208837903602\n",
      "iter  7  loss  0.04997611891744127  grad l2 norm  0.01339571013989686\n",
      "iter  8  loss  0.049585154438067554  grad l2 norm  0.013258759094655664\n",
      "iter  9  loss  0.04919806116332392  grad l2 norm  0.013133689085726782\n",
      "unsuccessful, tol:  0.013133689085726782\n",
      "iter  0  loss  0.048814347369277845  grad l2 norm  0.012998339170011483\n",
      "iter  1  loss  0.04843465980936997  grad l2 norm  0.012874446694300566\n",
      "iter  2  loss  0.04805852430837924  grad l2 norm  0.012740635433392272\n",
      "iter  3  loss  0.04768655105149108  grad l2 norm  0.01261818139540665\n",
      "iter  4  loss  0.047318246118790784  grad l2 norm  0.0124862220237328\n",
      "iter  5  loss  0.04695415847043669  grad l2 norm  0.012365526810133374\n",
      "iter  6  loss  0.04659375535760691  grad l2 norm  0.012235726550968228\n",
      "iter  7  loss  0.04623751519871686  grad l2 norm  0.012117039373969195\n",
      "iter  8  loss  0.04588487507592789  grad l2 norm  0.01198990474205067\n",
      "iter  9  loss  0.045536246500954815  grad l2 norm  0.011873757782828391\n",
      "unsuccessful, tol:  0.011873757782828391\n",
      "iter  0  loss  0.04519105608480881  grad l2 norm  0.011749883909631595\n",
      "iter  1  loss  0.04484967079092884  grad l2 norm  0.011636734019828857\n",
      "iter  2  loss  0.04451153701155736  grad l2 norm  0.011516388559953782\n",
      "iter  3  loss  0.044177015873115184  grad l2 norm  0.011406548848682288\n",
      "iter  4  loss  0.043845591272005394  grad l2 norm  0.011289959750603437\n",
      "iter  5  loss  0.043517632273082404  grad l2 norm  0.011183742298767525\n",
      "iter  6  loss  0.04319264878658337  grad l2 norm  0.01107109220294163\n",
      "iter  7  loss  0.04287100437748182  grad l2 norm  0.010968583547974661\n",
      "iter  8  loss  0.042552222172982775  grad l2 norm  0.010860006416405105\n",
      "iter  9  loss  0.0422366426366014  grad l2 norm  0.010761387010403833\n",
      "unsuccessful, tol:  0.010761387010403833\n",
      "iter  0  loss  0.04192379186365532  grad l2 norm  0.010657302311890967\n",
      "iter  1  loss  0.041613964680349495  grad l2 norm  0.01056301450874727\n",
      "iter  2  loss  0.041306678900744055  grad l2 norm  0.010463821616030987\n",
      "iter  3  loss  0.04100218408345076  grad l2 norm  0.010374168259477129\n",
      "iter  4  loss  0.040700008309318726  grad l2 norm  0.010279981463225967\n",
      "iter  5  loss  0.040400390777810206  grad l2 norm  0.010195037711510428\n",
      "iter  6  loss  0.040102892513948114  grad l2 norm  0.010105742274410379\n",
      "iter  7  loss  0.03980776276364444  grad l2 norm  0.01002525528531353\n",
      "iter  8  loss  0.039514605457020305  grad l2 norm  0.009940403988062552\n",
      "iter  9  loss  0.03922369785494824  grad l2 norm  0.009863762922064383\n",
      "unsuccessful, tol:  0.009863762922064383\n",
      "iter  0  loss  0.03893470107640604  grad l2 norm  0.009782776450813732\n",
      "iter  1  loss  0.038647919863441536  grad l2 norm  0.0097094052228206\n",
      "iter  2  loss  0.03836305403919252  grad l2 norm  0.00963180829907352\n",
      "iter  3  loss  0.03808040455847055  grad l2 norm  0.009561259112044824\n",
      "iter  4  loss  0.03779967732263999  grad l2 norm  0.009486604653224729\n",
      "iter  5  loss  0.03752114868196512  grad l2 norm  0.00941857509794549\n",
      "iter  6  loss  0.037244514273477955  grad l2 norm  0.009346613784909539\n",
      "iter  7  loss  0.0369700217570862  grad l2 norm  0.009281029601575038\n",
      "iter  8  loss  0.03669735352196465  grad l2 norm  0.009211676223392877\n",
      "iter  9  loss  0.03642673314310353  grad l2 norm  0.009148433147085768\n",
      "unsuccessful, tol:  0.009148433147085768\n",
      "iter  0  loss  0.036157842571470894  grad l2 norm  0.009081562531326134\n",
      "iter  1  loss  0.03589089722957015  grad l2 norm  0.009020483732039352\n",
      "iter  2  loss  0.03562560354474968  grad l2 norm  0.008955966039674112\n",
      "iter  3  loss  0.035362191134557304  grad l2 norm  0.008896827853656326\n",
      "iter  4  loss  0.03510041504201266  grad l2 norm  0.008834333499865699\n",
      "iter  5  loss  0.03484053731542157  grad l2 norm  0.008776673409623336\n",
      "iter  6  loss  0.034582364692034344  grad l2 norm  0.008715613792865071\n",
      "iter  7  loss  0.034326183183979245  grad l2 norm  0.008658889041550677\n",
      "iter  8  loss  0.03407182429720778  grad l2 norm  0.008598717642680618\n",
      "iter  9  loss  0.03381957133178793  grad l2 norm  0.008542470135202374\n",
      "unsuccessful, tol:  0.008542470135202374\n",
      "iter  0  loss  0.033569255660662975  grad l2 norm  0.008482740326750487\n",
      "iter  1  loss  0.033321143142129236  grad l2 norm  0.008426560923086374\n",
      "iter  2  loss  0.0330750618859655  grad l2 norm  0.00836694116036807\n",
      "iter  3  loss  0.03283126051098855  grad l2 norm  0.0083105561699869\n",
      "iter  4  loss  0.03258957103906233  grad l2 norm  0.008250841267088799\n",
      "iter  5  loss  0.03235022923976541  grad l2 norm  0.008194048205595916\n",
      "iter  6  loss  0.032113076532719866  grad l2 norm  0.008134012902972313\n",
      "iter  7  loss  0.031878338415413904  grad l2 norm  0.008076610738259556\n",
      "iter  8  loss  0.03164586321330609  grad l2 norm  0.008016068595396976\n",
      "iter  9  loss  0.031415861451004196  grad l2 norm  0.007957963820358445\n",
      "unsuccessful, tol:  0.007957963820358445\n",
      "iter  0  loss  0.031188178616259006  grad l2 norm  0.007896854513646408\n",
      "iter  1  loss  0.030963001811655465  grad l2 norm  0.007838043693437298\n",
      "iter  2  loss  0.030740166670231907  grad l2 norm  0.007776399661307445\n",
      "iter  3  loss  0.030519832347666784  grad l2 norm  0.007716973023499571\n",
      "iter  4  loss  0.030301825107564996  grad l2 norm  0.007654958404776801\n",
      "iter  5  loss  0.030086277714821103  grad l2 norm  0.007595129288428928\n",
      "iter  6  loss  0.029873013381862386  grad l2 norm  0.007532972853134311\n",
      "iter  7  loss  0.029662145560973494  grad l2 norm  0.007472966066616884\n",
      "iter  8  loss  0.02945350235632385  grad l2 norm  0.007410862086462041\n",
      "iter  9  loss  0.029247185430796383  grad l2 norm  0.007350879713094189\n",
      "unsuccessful, tol:  0.007350879713094189\n",
      "iter  0  loss  0.02904303250612021  grad l2 norm  0.007289007126805863\n",
      "iter  1  loss  0.028841138840209602  grad l2 norm  0.0072292170241898615\n",
      "iter  2  loss  0.028641356216708068  grad l2 norm  0.007167697784179196\n",
      "iter  3  loss  0.02844377960529537  grad l2 norm  0.007108176828541336\n",
      "iter  4  loss  0.02824828013300482  grad l2 norm  0.007047065473233368\n",
      "iter  5  loss  0.0280549556912256  grad l2 norm  0.006987855066616928\n",
      "iter  6  loss  0.02786369598539553  grad l2 norm  0.006927201365699309\n",
      "iter  7  loss  0.02767459751454553  grad l2 norm  0.006868355914676202\n",
      "iter  8  loss  0.027487560504811725  grad l2 norm  0.006808210767082648\n",
      "iter  9  loss  0.02730267200010597  grad l2 norm  0.006749805928020343\n",
      "unsuccessful, tol:  0.006749805928020343\n",
      "iter  0  loss  0.02711983288287962  grad l2 norm  0.006690255151962513\n",
      "iter  1  loss  0.026939113393953065  grad l2 norm  0.006632423949854642\n",
      "iter  2  loss  0.026760407522226307  grad l2 norm  0.006573606720521667\n",
      "iter  3  loss  0.02658376501865277  grad l2 norm  0.006516499556873972\n",
      "iter  4  loss  0.02640907266010743  grad l2 norm  0.006458537861988709\n",
      "iter  5  loss  0.026236365450454525  grad l2 norm  0.006402236904381286\n",
      "iter  6  loss  0.026065535591731206  grad l2 norm  0.006345146982905916\n",
      "iter  7  loss  0.025896621862994157  grad l2 norm  0.006289571549937941\n",
      "iter  8  loss  0.025729543009670626  grad l2 norm  0.006233158340987702\n",
      "iter  9  loss  0.025564360134930797  grad l2 norm  0.006177996598270912\n",
      "unsuccessful, tol:  0.006177996598270912\n",
      "iter  0  loss  0.025401027620729138  grad l2 norm  0.006121861171314449\n",
      "iter  1  loss  0.025239628688394997  grad l2 norm  0.0060666838670012475\n",
      "iter  2  loss  0.025080141856144666  grad l2 norm  0.006010405110453485\n",
      "iter  3  loss  0.024922657895429707  grad l2 norm  0.005954849213483375\n",
      "iter  4  loss  0.024767162097676668  grad l2 norm  0.00589812392316189\n",
      "iter  5  loss  0.024613739671405704  grad l2 norm  0.005841967167207234\n",
      "iter  6  loss  0.024462372209050384  grad l2 norm  0.005784643044901707\n",
      "iter  7  loss  0.02431313388580491  grad l2 norm  0.005727808872148934\n",
      "iter  8  loss  0.024165999643788925  grad l2 norm  0.005669866524923548\n",
      "iter  9  loss  0.024021032219393834  grad l2 norm  0.005612386951206884\n",
      "unsuccessful, tol:  0.005612386951206884\n",
      "iter  0  loss  0.02387820002678526  grad l2 norm  0.005553888159505059\n",
      "iter  1  loss  0.02373755531701188  grad l2 norm  0.005495859373666115\n",
      "iter  2  loss  0.023599060159269675  grad l2 norm  0.005436922135336708\n",
      "iter  3  loss  0.023462757057162508  grad l2 norm  0.005378495932680157\n",
      "iter  4  loss  0.02332860163101827  grad l2 norm  0.0053192914255792995\n",
      "iter  5  loss  0.023196627386064594  grad l2 norm  0.005260663382852638\n",
      "iter  6  loss  0.023066783929855533  grad l2 norm  0.005201399116931913\n",
      "iter  7  loss  0.022939096984371122  grad l2 norm  0.005142793589868467\n",
      "iter  8  loss  0.022813511261336114  grad l2 norm  0.005083703188316474\n",
      "iter  9  loss  0.022690046143802798  grad l2 norm  0.00502536534814237\n",
      "unsuccessful, tol:  0.00502536534814237\n",
      "iter  0  loss  0.02256864268411129  grad l2 norm  0.004966692327841432\n",
      "iter  1  loss  0.02244931531085582  grad l2 norm  0.004908866632079743\n",
      "iter  2  loss  0.022332002474519025  grad l2 norm  0.0048508437125302385\n",
      "iter  3  loss  0.022216714856090548  grad l2 norm  0.004793762514403022\n",
      "iter  4  loss  0.022103389093546925  grad l2 norm  0.004736611709709928\n",
      "iter  5  loss  0.021992032944923065  grad l2 norm  0.004680497749550124\n",
      "iter  6  loss  0.02188258182666235  grad l2 norm  0.004624432127823849\n",
      "iter  7  loss  0.021775041051955168  grad l2 norm  0.004569496391962467\n",
      "iter  8  loss  0.021669345434748927  grad l2 norm  0.0045147189739110465\n",
      "iter  9  loss  0.02156549816998667  grad l2 norm  0.004461162007388105\n",
      "unsuccessful, tol:  0.004461162007388105\n",
      "iter  0  loss  0.021463434130034657  grad l2 norm  0.0044078656302471994\n",
      "iter  1  loss  0.021363154698296995  grad l2 norm  0.004355873685502036\n",
      "iter  2  loss  0.021264595433074126  grad l2 norm  0.004304231686480552\n",
      "iter  3  loss  0.021167756316131794  grad l2 norm  0.004253968220835096\n",
      "iter  4  loss  0.021072574160822838  grad l2 norm  0.004204131356111968\n",
      "iter  5  loss  0.0209790480094774  grad l2 norm  0.004155740321750159\n",
      "iter  6  loss  0.020887116379013995  grad l2 norm  0.004107843703584338\n",
      "iter  7  loss  0.02079677773884315  grad l2 norm  0.004061455371435633\n",
      "iter  8  loss  0.020707972637556384  grad l2 norm  0.004015622833150815\n",
      "iter  9  loss  0.02062069919473532  grad l2 norm  0.003971357586720268\n",
      "unsuccessful, tol:  0.003971357586720268\n",
      "iter  0  loss  0.02053490018216617  grad l2 norm  0.003927705384801237\n",
      "iter  1  loss  0.0204505734240216  grad l2 norm  0.003885676266957392\n",
      "iter  2  loss  0.020367663884671264  grad l2 norm  0.0038443117606383782\n",
      "iter  3  loss  0.02028616900225786  grad l2 norm  0.003804620098084024\n",
      "iter  4  loss  0.020206035689739468  grad l2 norm  0.0037656368732143417\n",
      "iter  5  loss  0.020127260845981532  grad l2 norm  0.003728370155009175\n",
      "iter  6  loss  0.020049792968930176  grad l2 norm  0.0036918498112955673\n",
      "iter  7  loss  0.019973628258059116  grad l2 norm  0.003657085173509115\n",
      "iter  8  loss  0.019898716429486398  grad l2 norm  0.003623100634227037\n",
      "iter  9  loss  0.01982505287656563  grad l2 norm  0.0035909067113560193\n",
      "unsuccessful, tol:  0.0035909067113560193\n",
      "iter  0  loss  0.019752588258796076  grad l2 norm  0.0035595230747838256\n",
      "iter  1  loss  0.01968131715551534  grad l2 norm  0.0035299602543850538\n",
      "iter  2  loss  0.01961119098734725  grad l2 norm  0.003501233478001481\n",
      "iter  3  loss  0.01954220361432682  grad l2 norm  0.0034743505908236025\n",
      "iter  4  loss  0.019474307094089625  grad l2 norm  0.0034483223361827394\n",
      "iter  5  loss  0.019407494763795226  grad l2 norm  0.0034241517502073084\n",
      "iter  6  loss  0.01934171923821527  grad l2 norm  0.003400846737637293\n",
      "iter  7  loss  0.019276973597708188  grad l2 norm  0.003379404573339307\n",
      "iter  8  loss  0.019213210960265372  grad l2 norm  0.003358832856778317\n",
      "iter  9  loss  0.01915042443347609  grad l2 norm  0.0033401217651827016\n",
      "unsuccessful, tol:  0.0033401217651827016\n",
      "iter  0  loss  0.019088567601524317  grad l2 norm  0.0033222811243454058\n",
      "iter  1  loss  0.019027633877449285  grad l2 norm  0.0033062922268180047\n",
      "iter  2  loss  0.018967577284706355  grad l2 norm  0.003291169363610611\n",
      "iter  3  loss  0.01890839180540379  grad l2 norm  0.003277882578041313\n",
      "iter  4  loss  0.018850031873261418  grad l2 norm  0.003265452028871108\n",
      "iter  5  loss  0.01879249228929845  grad l2 norm  0.0032548341615293926\n",
      "iter  6  loss  0.0187357278722105  grad l2 norm  0.0032450566338374075\n",
      "iter  7  loss  0.018679734478183777  grad l2 norm  0.0032370611632349727\n",
      "iter  8  loss  0.018624467297737785  grad l2 norm  0.003229885232165334\n",
      "iter  9  loss  0.018569923445876515  grad l2 norm  0.0032244553126778435\n",
      "unsuccessful, tol:  0.0032244553126778435\n",
      "iter  0  loss  0.018516058483421916  grad l2 norm  0.003219820897055581\n",
      "iter  1  loss  0.018462870938631956  grad l2 norm  0.0032168925520857385\n",
      "iter  2  loss  0.018410316752077494  grad l2 norm  0.003214733550454374\n",
      "iter  3  loss  0.018358395967109545  grad l2 norm  0.0032142375672410965\n",
      "iter  4  loss  0.01830706491925896  grad l2 norm  0.0032144828563451487\n",
      "iter  5  loss  0.018256325219496664  grad l2 norm  0.0032163448128544785\n",
      "iter  6  loss  0.018206133617049185  grad l2 norm  0.0032189177112329228\n",
      "iter  7  loss  0.018156493303476955  grad l2 norm  0.00322305752890373\n",
      "iter  8  loss  0.0181073614669681  grad l2 norm  0.0032278761886720448\n",
      "iter  9  loss  0.01805874285809113  grad l2 norm  0.0032342095696271567\n",
      "unsuccessful, tol:  0.0032342095696271567\n",
      "iter  0  loss  0.01801059513293  grad l2 norm  0.0032411891800952506\n",
      "iter  1  loss  0.017962924544334367  grad l2 norm  0.003249630064371302\n",
      "iter  2  loss  0.01791568924373817  grad l2 norm  0.0032586850239111364\n",
      "iter  3  loss  0.017868896898377753  grad l2 norm  0.003269147287459804\n",
      "iter  4  loss  0.017822506177196946  grad l2 norm  0.003280192207348225\n",
      "iter  5  loss  0.017776526049757498  grad l2 norm  0.0032925899826864702\n",
      "iter  6  loss  0.017730915717139153  grad l2 norm  0.0033055393450828496\n",
      "iter  7  loss  0.017685685323618788  grad l2 norm  0.003319786323004454\n",
      "iter  8  loss  0.017640794611885332  grad l2 norm  0.0033345538669139183\n",
      "iter  9  loss  0.017596254765726817  grad l2 norm  0.0033505631630690457\n",
      "unsuccessful, tol:  0.0033505631630690457\n",
      "iter  0  loss  0.017552026075616373  grad l2 norm  0.0033670623861481075\n",
      "iter  1  loss  0.01750812062520409  grad l2 norm  0.003384747532475869\n",
      "iter  2  loss  0.0174644992542797  grad l2 norm  0.003402892847700121\n",
      "iter  3  loss  0.017421174804971913  grad l2 norm  0.003422168831704267\n",
      "iter  4  loss  0.017378108662645417  grad l2 norm  0.0034418762680263554\n",
      "iter  5  loss  0.017335314289873692  grad l2 norm  0.003462659813592498\n",
      "iter  6  loss  0.017292753609555865  grad l2 norm  0.0034838468505530625\n",
      "iter  7  loss  0.017250440575909198  grad l2 norm  0.003506055886241185\n",
      "iter  8  loss  0.01720833764040336  grad l2 norm  0.0035286407606537782\n",
      "iter  9  loss  0.017166459135205446  grad l2 norm  0.0035521938147126766\n",
      "unsuccessful, tol:  0.0035521938147126766\n",
      "iter  0  loss  0.01712476803497306  grad l2 norm  0.003576095244464751\n",
      "iter  1  loss  0.017083278954520047  grad l2 norm  0.0036009115672120456\n",
      "iter  2  loss  0.017041955392246882  grad l2 norm  0.0036260491960735516\n",
      "iter  3  loss  0.017000812169543647  grad l2 norm  0.0036520493374951487\n",
      "iter  4  loss  0.016959813316551785  grad l2 norm  0.0036783442545450388\n",
      "iter  5  loss  0.01691897380593011  grad l2 norm  0.0037054503767534083\n",
      "iter  6  loss  0.016878258217689956  grad l2 norm  0.0037328251581486113\n",
      "iter  7  loss  0.01683768164367735  grad l2 norm  0.0037609607780501714\n",
      "iter  8  loss  0.01679720924557864  grad l2 norm  0.003789339016946289\n",
      "iter  9  loss  0.016756856227418945  grad l2 norm  0.003818428418147949\n",
      "unsuccessful, tol:  0.003818428418147949\n",
      "iter  0  loss  0.016716588380437  grad l2 norm  0.0038477342102792734\n",
      "iter  1  loss  0.016676421037181574  grad l2 norm  0.0038777021046465733\n",
      "iter  2  loss  0.01663632067950945  grad l2 norm  0.003907859968684739\n",
      "iter  3  loss  0.016596302800143695  grad l2 norm  0.003938631618813082\n",
      "iter  4  loss  0.01655633463618931  grad l2 norm  0.003969566743075685\n",
      "iter  5  loss  0.01651643187487632  grad l2 norm  0.004001068212716057\n",
      "iter  6  loss  0.01647656256416229  grad l2 norm  0.004032706637507554\n",
      "iter  7  loss  0.016436742610380944  grad l2 norm  0.004064864849770454\n",
      "iter  8  loss  0.016396940908901418  grad l2 norm  0.004097133411300488\n",
      "iter  9  loss  0.01635717359243919  grad l2 norm  0.004129876003117461\n",
      "unsuccessful, tol:  0.004129876003117461\n",
      "iter  0  loss  0.01631741041793937  grad l2 norm  0.004162702183730217\n",
      "iter  1  loss  0.016277667733850778  grad l2 norm  0.004195957421876698\n",
      "iter  2  loss  0.016237916153644293  grad l2 norm  0.004229269406996784\n",
      "iter  3  loss  0.016198172217320502  grad l2 norm  0.004262966395763928\n",
      "iter  4  loss  0.0161584073767179  grad l2 norm  0.004296693405845886\n",
      "iter  5  loss  0.016118638331631272  grad l2 norm  0.004330762472600768\n",
      "iter  6  loss  0.016078837347003865  grad l2 norm  0.004364835129233911\n",
      "iter  7  loss  0.016039021248404537  grad l2 norm  0.004399208109162692\n",
      "iter  8  loss  0.015999163086123456  grad l2 norm  0.004433558606863615\n",
      "iter  9  loss  0.01595927977886521  grad l2 norm  0.0044681688974830625\n",
      "unsuccessful, tol:  0.0044681688974830625\n",
      "iter  0  loss  0.015919345135002122  grad l2 norm  0.00450273097152026\n",
      "iter  1  loss  0.01587937613701957  grad l2 norm  0.004537513459899718\n",
      "iter  2  loss  0.015839347326136195  grad l2 norm  0.004572222333119869\n",
      "iter  3  loss  0.015799275722918994  grad l2 norm  0.0046071134052049195\n",
      "iter  4  loss  0.015759136578252623  grad l2 norm  0.004641905881227873\n",
      "iter  5  loss  0.015718946927825772  grad l2 norm  0.004676843587438907\n",
      "iter  6  loss  0.015678682709909797  grad l2 norm  0.004711658255754461\n",
      "iter  7  loss  0.01563836095362882  grad l2 norm  0.0047465825153291605\n",
      "iter  8  loss  0.015597958261865962  grad l2 norm  0.004781359919388462\n",
      "iter  9  loss  0.015557491635515472  grad l2 norm  0.004816212632637449\n",
      "unsuccessful, tol:  0.004816212632637449\n",
      "iter  0  loss  0.015516938317873276  grad l2 norm  0.004850895324853716\n",
      "iter  1  loss  0.015476315259220323  grad l2 norm  0.00488562038696349\n",
      "iter  2  loss  0.015435600317332068  grad l2 norm  0.004920152930898661\n",
      "iter  3  loss  0.015394810368468279  grad l2 norm  0.004954696254724234\n",
      "iter  4  loss  0.015353923857209616  grad l2 norm  0.004989025294513587\n",
      "iter  5  loss  0.015312957561196631  grad l2 norm  0.0050233349369420465\n",
      "iter  6  loss  0.015271890482386525  grad l2 norm  0.00505740937275477\n",
      "iter  7  loss  0.015230739274001823  grad l2 norm  0.005091435738596332\n",
      "iter  8  loss  0.01518948346444783  grad l2 norm  0.0051252069311506976\n",
      "iter  9  loss  0.015148139556149758  grad l2 norm  0.0051589029546096074\n",
      "unsuccessful, tol:  0.0051589029546096074\n",
      "iter  0  loss  0.015106687572675456  grad l2 norm  0.005192324866859237\n",
      "iter  1  loss  0.015065143840445588  grad l2 norm  0.005225646109127706\n",
      "iter  2  loss  0.015023488849016447  grad l2 norm  0.00525867536486432\n",
      "iter  3  loss  0.014981738727888867  grad l2 norm  0.005291580047183712\n",
      "iter  4  loss  0.014939874409372923  grad l2 norm  0.005324175952578786\n",
      "iter  5  loss  0.014897911813712753  grad l2 norm  0.0053566249766772355\n",
      "iter  6  loss  0.014855832303358213  grad l2 norm  0.005388749539436265\n",
      "iter  7  loss  0.014813651590254518  grad l2 norm  0.005420706496038417\n",
      "iter  8  loss  0.014771351469488762  grad l2 norm  0.005452324404721858\n",
      "iter  9  loss  0.014728947462743282  grad l2 norm  0.00548375550393111\n",
      "unsuccessful, tol:  0.00548375550393111\n",
      "iter  0  loss  0.014686421817919106  grad l2 norm  0.005514833991698271\n",
      "iter  1  loss  0.014643789902294027  grad l2 norm  0.005545707851879698\n",
      "iter  2  loss  0.014601034452795982  grad l2 norm  0.0055762164118369085\n",
      "iter  3  loss  0.01455817073152175  grad l2 norm  0.005606503715354061\n",
      "iter  4  loss  0.014515182009315945  grad l2 norm  0.005636413718150259\n",
      "iter  5  loss  0.014472083495718779  grad l2 norm  0.005666086818911613\n",
      "iter  6  loss  0.01442885903734785  grad l2 norm  0.005695371136542625\n",
      "iter  7  loss  0.014385523834969475  grad l2 norm  0.005724403722973104\n",
      "iter  8  loss  0.014342062338598477  grad l2 norm  0.005753036444403407\n",
      "iter  9  loss  0.014298489766427067  grad l2 norm  0.005781403313591052\n",
      "unsuccessful, tol:  0.005781403313591052\n",
      "iter  0  loss  0.014254791180170551  grad l2 norm  0.005809359574851525\n",
      "iter  1  loss  0.014210981823037087  grad l2 norm  0.005837036511976818\n",
      "iter  2  loss  0.014167047360293174  grad l2 norm  0.005864292415568693\n",
      "iter  3  loss  0.01412300305493213  grad l2 norm  0.005891256145755522\n",
      "iter  4  loss  0.014078835158614748  grad l2 norm  0.0059177887388418135\n",
      "iter  5  loss  0.014034558943671678  grad l2 norm  0.005944016929944707\n",
      "iter  6  loss  0.013990161228553196  grad l2 norm  0.005969804231022278\n",
      "iter  7  loss  0.013945657283864613  grad l2 norm  0.005995275545671953\n",
      "iter  8  loss  0.013901034475123246  grad l2 norm  0.006020296621674861\n",
      "iter  9  loss  0.013856308059604803  grad l2 norm  0.006044990818051315\n",
      "unsuccessful, tol:  0.006044990818051315\n",
      "iter  0  loss  0.013811465928387586  grad l2 norm  0.006069225905143653\n",
      "iter  1  loss  0.013766523310421407  grad l2 norm  0.006093123973038606\n",
      "iter  2  loss  0.013721468595781683  grad l2 norm  0.006116554623082775\n",
      "iter  3  loss  0.013676316963551687  grad l2 norm  0.006139638936842633\n",
      "iter  4  loss  0.013631057268637808  grad l2 norm  0.006162248174240261\n",
      "iter  5  loss  0.013585704610471832  grad l2 norm  0.006184502653026975\n",
      "iter  6  loss  0.013540248266215348  grad l2 norm  0.006206275138144531\n",
      "iter  7  loss  0.01349470321764591  grad l2 norm  0.006227685413361949\n",
      "iter  8  loss  0.01344905911382701  grad l2 norm  0.006248607612801663\n",
      "iter  9  loss  0.013403330773703996  grad l2 norm  0.006269161199302038\n",
      "unsuccessful, tol:  0.006269161199302038\n",
      "iter  0  loss  0.013357508161326451  grad l2 norm  0.0062892215535656866\n",
      "iter  1  loss  0.013311605881682943  grad l2 norm  0.006308908008469996\n",
      "iter  2  loss  0.013265614151485297  grad l2 norm  0.006328097075941863\n",
      "iter  3  loss  0.013219547305737251  grad l2 norm  0.006346908122789352\n",
      "iter  4  loss  0.01317339574708443  grad l2 norm  0.006365218680280348\n",
      "iter  5  loss  0.01312717348068085  grad l2 norm  0.006383148284634749\n",
      "iter  6  loss  0.01308087102527696  grad l2 norm  0.006400575377449461\n",
      "iter  7  loss  0.013034501994238381  grad l2 norm  0.006417619772603795\n",
      "iter  8  loss  0.012988056951969381  grad l2 norm  0.006434160715288256\n",
      "iter  9  loss  0.012941549059521516  grad l2 norm  0.0064503183769379765\n",
      "unsuccessful, tol:  0.0064503183769379765\n",
      "iter  0  loss  0.012894968860710843  grad l2 norm  0.006465972696719764\n",
      "iter  1  loss  0.01284832901161488  grad l2 norm  0.006481244247872487\n",
      "iter  2  loss  0.01280161998184268  grad l2 norm  0.006496013539755647\n",
      "iter  3  loss  0.012754853888160535  grad l2 norm  0.006510401543610206\n",
      "iter  4  loss  0.012708021097564656  grad l2 norm  0.006524289187024909\n",
      "iter  5  loss  0.01266113318579168  grad l2 norm  0.006537797776014781\n",
      "iter  6  loss  0.012614180429489101  grad l2 norm  0.006550808466884957\n",
      "iter  7  loss  0.01256717390963656  grad l2 norm  0.006563442776011547\n",
      "iter  8  loss  0.012520103879380854  grad l2 norm  0.0065755818655926866\n",
      "iter  9  loss  0.01247298102972404  grad l2 norm  0.006587347292830078\n",
      "unsuccessful, tol:  0.006587347292830078\n",
      "iter  0  loss  0.012425795717172693  grad l2 norm  0.006598619994615932\n",
      "iter  1  loss  0.012378558404691332  grad l2 norm  0.006609521391141525\n",
      "iter  2  loss  0.012331259730090915  grad l2 norm  0.006619931998605591\n",
      "iter  3  loss  0.012283910130642835  grad l2 norm  0.006629972963347615\n",
      "iter  4  loss  0.012236500730609114  grad l2 norm  0.006639524270033045\n",
      "iter  5  loss  0.012189042154456032  grad l2 norm  0.006648706737853197\n",
      "iter  6  loss  0.012141526210697359  grad l2 norm  0.006657399825034324\n",
      "iter  7  loss  0.012093963896633796  grad l2 norm  0.00666572407075408\n",
      "iter  8  loss  0.01204634785563849  grad l2 norm  0.0066735585275784215\n",
      "iter  9  loss  0.011998689576425891  grad l2 norm  0.006681023587545798\n",
      "unsuccessful, tol:  0.006681023587545798\n",
      "iter  0  loss  0.011950982603386869  grad l2 norm  0.006687998103437406\n",
      "iter  1  loss  0.01190323893765275  grad l2 norm  0.00669460249976189\n",
      "iter  2  loss  0.011855452986850387  grad l2 norm  0.006700715671361567\n",
      "iter  3  loss  0.011807637183806547  grad l2 norm  0.006706458251776712\n",
      "iter  4  loss  0.011759786670910469  grad l2 norm  0.006711709404007113\n",
      "iter  5  loss  0.011711914160331105  grad l2 norm  0.006716590099850678\n",
      "iter  6  loss  0.011664015353107515  grad l2 norm  0.006720979943066195\n",
      "iter  7  loss  0.011616103066694927  grad l2 norm  0.00672500030134423\n",
      "iter  8  loss  0.011568173386929542  grad l2 norm  0.0067285313251498335\n",
      "iter  9  loss  0.011520239084589897  grad l2 norm  0.006731694764919641\n",
      "unsuccessful, tol:  0.006731694764919641\n",
      "iter  0  loss  0.011472296492453802  grad l2 norm  0.006734371367009628\n",
      "iter  1  loss  0.011424358226860973  grad l2 norm  0.006736683199524609\n",
      "iter  2  loss  0.011376420776877684  grad l2 norm  0.0067385116193513965\n",
      "iter  3  loss  0.011328496543187124  grad l2 norm  0.00673997892011129\n",
      "iter  4  loss  0.011280582123949109  grad l2 norm  0.006740967072716306\n",
      "iter  5  loss  0.011232689682225006  grad l2 norm  0.006741598499430415\n",
      "iter  6  loss  0.011184815912841508  grad l2 norm  0.006741755796617987\n",
      "iter  7  loss  0.011136972749755626  grad l2 norm  0.00674156143094315\n",
      "iter  8  loss  0.011089156998705944  grad l2 norm  0.006740898657776799\n",
      "iter  9  loss  0.011041380396208497  grad l2 norm  0.006739889929730282\n",
      "unsuccessful, tol:  0.006739889929730282\n",
      "iter  0  loss  0.010993639893018152  grad l2 norm  0.006738419227004123\n",
      "iter  1  loss  0.010945947076187956  grad l2 norm  0.006736608964776984\n",
      "iter  2  loss  0.010898299088018112  grad l2 norm  0.006734343951727027\n",
      "iter  3  loss  0.010850707421777276  grad l2 norm  0.006731746581064461\n",
      "iter  4  loss  0.010803169460095758  grad l2 norm  0.0067287026301544595\n",
      "iter  5  loss  0.010755696653995998  grad l2 norm  0.0067253345201814235\n",
      "iter  6  loss  0.010708286664237048  grad l2 norm  0.006721529163703428\n",
      "iter  7  loss  0.010660950933015471  grad l2 norm  0.006717409080591397\n",
      "iter  8  loss  0.010613687412382226  grad l2 norm  0.006712862492465424\n",
      "iter  9  loss  0.010566507539020612  grad l2 norm  0.0067080120885932215\n",
      "unsuccessful, tol:  0.0067080120885932215\n",
      "iter  0  loss  0.010519409535887285  grad l2 norm  0.0067027475575163285\n",
      "iter  1  loss  0.010472404802779018  grad l2 norm  0.006697191806806289\n",
      "iter  2  loss  0.01042549178018941  grad l2 norm  0.00669123611304494\n",
      "iter  3  loss  0.010378681770887565  grad l2 norm  0.0066850036134423595\n",
      "iter  4  loss  0.010331973357269045  grad l2 norm  0.006678387245149326\n",
      "iter  5  loss  0.010285377670518  grad l2 norm  0.006671510343258033\n",
      "iter  6  loss  0.010238893353356236  grad l2 norm  0.0066642675281686275\n",
      "iter  7  loss  0.010192531292969919  grad l2 norm  0.006656782261625579\n",
      "iter  8  loss  0.010146290120052859  grad l2 norm  0.00664895082619619\n",
      "iter  9  loss  0.010100180420164404  grad l2 norm  0.006640896707524248\n",
      "unsuccessful, tol:  0.006640896707524248\n",
      "iter  0  loss  0.010054200758294513  grad l2 norm  0.0066325177935725324\n",
      "iter  1  loss  0.010008361381796562  grad l2 norm  0.0066239374668381765\n",
      "iter  2  loss  0.009962660758317895  grad l2 norm  0.00661505513436502\n",
      "iter  3  loss  0.009917108782577724  grad l2 norm  0.006605993928084593\n",
      "iter  4  loss  0.009871703814649128  grad l2 norm  0.006596654661673864\n",
      "iter  5  loss  0.009826455402524116  grad l2 norm  0.006587160050009578\n",
      "iter  6  loss  0.009781361807035125  grad l2 norm  0.006577412177710411\n",
      "iter  7  loss  0.009736432251889792  grad l2 norm  0.006567533159085686\n",
      "iter  8  loss  0.009691664920577974  grad l2 norm  0.006557426197408961\n",
      "iter  9  loss  0.009647068745091279  grad l2 norm  0.006547212613227093\n",
      "unsuccessful, tol:  0.006547212613227093\n",
      "iter  0  loss  0.009602641859030765  grad l2 norm  0.00653679657377736\n",
      "iter  1  loss  0.009558392935793142  grad l2 norm  0.00652629841800852\n",
      "iter  2  loss  0.009514320081396202  grad l2 norm  0.006515623141516295\n",
      "iter  3  loss  0.009470431733106084  grad l2 norm  0.0065048899386738685\n",
      "iter  4  loss  0.009426725975983345  grad l2 norm  0.006494004538971793\n",
      "iter  5  loss  0.009383211014072971  grad l2 norm  0.006483084868835157\n",
      "iter  6  loss  0.00933988489650288  grad l2 norm  0.006472037350389367\n",
      "iter  7  loss  0.009296755574976156  grad l2 norm  0.006460978558818254\n",
      "iter  8  loss  0.009253821027701512  grad l2 norm  0.006449815616124325\n",
      "iter  9  loss  0.00921108891794381  grad l2 norm  0.00643866368730363\n",
      "unsuccessful, tol:  0.00643866368730363\n",
      "iter  0  loss  0.009168557106110514  grad l2 norm  0.006427430631162586\n",
      "iter  1  loss  0.009126232923780914  grad l2 norm  0.006416230143614741\n",
      "iter  2  loss  0.00908411406545895  grad l2 norm  0.00640497086385865\n",
      "iter  3  loss  0.009042207490974593  grad l2 norm  0.0063937649383186785\n",
      "iter  4  loss  0.009000510689026997  grad l2 norm  0.006382521819036323\n",
      "iter  5  loss  0.00895903021887884  grad l2 norm  0.006371351989234401\n",
      "iter  6  loss  0.008917763338347443  grad l2 norm  0.006360165726138973\n",
      "iter  7  loss  0.008876716193934816  grad l2 norm  0.006349071701161667\n",
      "iter  8  loss  0.008835885806569419  grad l2 norm  0.006337981007214112\n",
      "iter  9  loss  0.008795277917897961  grad l2 norm  0.006327000325582235\n",
      "unsuccessful, tol:  0.006327000325582235\n",
      "iter  0  loss  0.008754889327647142  grad l2 norm  0.006316041535962482\n",
      "iter  1  loss  0.008714725402523304  grad l2 norm  0.006305209130181334\n",
      "iter  2  loss  0.008674782759178866  grad l2 norm  0.0062944157314221635\n",
      "iter  3  loss  0.008635066440664101  grad l2 norm  0.0062837634329028415\n",
      "iter  4  loss  0.008595572938352928  grad l2 norm  0.006273165551970286\n",
      "iter  5  loss  0.00855630703897756  grad l2 norm  0.006262721579350003\n",
      "iter  6  loss  0.00851726517754732  grad l2 norm  0.00625234548532894\n",
      "iter  7  loss  0.008478451955949116  grad l2 norm  0.006242133980795249\n",
      "iter  8  loss  0.00843986381752138  grad l2 norm  0.006232001677690733\n",
      "iter  9  loss  0.00840150523691016  grad l2 norm  0.006222042384233572\n",
      "unsuccessful, tol:  0.006222042384233572\n",
      "iter  0  loss  0.008363372706686857  grad l2 norm  0.00621217140052876\n",
      "iter  1  loss  0.008325470598756017  grad l2 norm  0.0062024795937570985\n",
      "iter  2  loss  0.008287795455182788  grad l2 norm  0.006192883082119076\n",
      "iter  3  loss  0.008250351524371561  grad l2 norm  0.00618346985817344\n",
      "iter  4  loss  0.008213135351775078  grad l2 norm  0.006174157080647859\n",
      "iter  5  loss  0.008176150998126092  grad l2 norm  0.006165030034304636\n",
      "iter  6  loss  0.00813939492905001  grad l2 norm  0.006156007212304982\n",
      "iter  7  loss  0.008102870925572473  grad l2 norm  0.006147171420699991\n",
      "iter  8  loss  0.008066575272735447  grad l2 norm  0.006138442798814887\n",
      "iter  9  loss  0.008030511372937473  grad l2 norm  0.006129901899347699\n",
      "unsuccessful, tol:  0.006129901899347699\n",
      "iter  0  loss  0.007994675231996614  grad l2 norm  0.006121470765244488\n",
      "iter  1  loss  0.007959069784177781  grad l2 norm  0.006113227845088502\n",
      "iter  2  loss  0.007923690671243717  grad l2 norm  0.006105097223862199\n",
      "iter  3  loss  0.00788854028614464  grad l2 norm  0.006097155267461326\n",
      "iter  4  loss  0.007853613838348067  grad l2 norm  0.006089328089592822\n",
      "iter  5  loss  0.007818913122449759  grad l2 norm  0.006081689855556747\n",
      "iter  6  loss  0.0077844328622448545  grad l2 norm  0.0060741685539796066\n",
      "iter  7  loss  0.007750174210918191  grad l2 norm  0.006066835923938756\n",
      "iter  8  loss  0.00771613136642194  grad l2 norm  0.0060596215839748765\n",
      "iter  9  loss  0.0076823048105103845  grad l2 norm  0.0060525945743418914\n",
      "unsuccessful, tol:  0.0060525945743418914\n",
      "iter  0  loss  0.007648688188162458  grad l2 norm  0.006045685874392396\n",
      "iter  1  loss  0.007615281292787937  grad l2 norm  0.006038961571700708\n",
      "iter  2  loss  0.0075820772023050586  grad l2 norm  0.0060323537730082605\n",
      "iter  3  loss  0.007549075017393133  grad l2 norm  0.006025925425433559\n",
      "iter  4  loss  0.007516267246829956  grad l2 norm  0.006019609598692808\n",
      "iter  5  loss  0.00748365230552075  grad l2 norm  0.006013465994077017\n",
      "iter  6  loss  0.007451222142751378  grad l2 norm  0.006007428550662663\n",
      "iter  7  loss  0.007418974507893329  grad l2 norm  0.006001553689377278\n",
      "iter  8  loss  0.0073869008179367575  grad l2 norm  0.005995776176670995\n",
      "iter  9  loss  0.007354998201400805  grad l2 norm  0.005990149166773629\n",
      "unsuccessful, tol:  0.005990149166773629\n",
      "iter  0  loss  0.007323257605095473  grad l2 norm  0.0059846082433384915\n",
      "iter  1  loss  0.007291675629290668  grad l2 norm  0.005979203342859724\n",
      "iter  2  loss  0.007260242875625615  grad l2 norm  0.00597387088510109\n",
      "iter  3  loss  0.007228955586038119  grad l2 norm  0.005968657681351063\n",
      "iter  4  loss  0.007197804230956543  grad l2 norm  0.005963501052942821\n",
      "iter  5  loss  0.0071667849581945105  grad l2 norm  0.005958444845489158\n",
      "iter  6  loss  0.0071358884114691165  grad l2 norm  0.005953427417994963\n",
      "iter  7  loss  0.007105110983667883  grad l2 norm  0.005948489896686237\n",
      "iter  8  loss  0.0070744438430803795  grad l2 norm  0.005943571895877891\n",
      "iter  9  loss  0.007043883973680674  grad l2 norm  0.005938712154423926\n",
      "unsuccessful, tol:  0.005938712154423926\n",
      "iter  0  loss  0.007013423378480943  grad l2 norm  0.0059338518281258235\n",
      "iter  1  loss  0.00698305988743507  grad l2 norm  0.005929027661690034\n",
      "iter  2  loss  0.006952786513096815  grad l2 norm  0.0059241826753452225\n",
      "iter  3  loss  0.006922602019956254  grad l2 norm  0.005919352036679898\n",
      "iter  4  loss  0.0068925004265497775  grad l2 norm  0.005914480951632275\n",
      "iter  5  loss  0.006862481350697301  grad l2 norm  0.0059096034115004486\n",
      "iter  6  loss  0.006832539670683842  grad l2 norm  0.0059046670885861035\n",
      "iter  7  loss  0.006802675673055279  grad l2 norm  0.005899705142379931\n",
      "iter  8  loss  0.006772884890131667  grad l2 norm  0.00589466791299493\n",
      "iter  9  loss  0.00674316807402289  grad l2 norm  0.0058895879786129675\n",
      "unsuccessful, tol:  0.0058895879786129675\n",
      "iter  0  loss  0.0067135212199858005  grad l2 norm  0.005884418437014508\n",
      "iter  1  loss  0.006683945380381236  grad l2 norm  0.0058791914150538235\n",
      "iter  2  loss  0.006654436874698714  grad l2 norm  0.005873862729462297\n",
      "iter  3  loss  0.00662499695085525  grad l2 norm  0.005868464057280948\n",
      "iter  4  loss  0.006595622179636078  grad l2 norm  0.005862953769963923\n",
      "iter  5  loss  0.006566313969394057  grad l2 norm  0.005857362981163203\n",
      "iter  6  loss  0.0065370691420425  grad l2 norm  0.005851652353600124\n",
      "iter  7  loss  0.006507889305191746  grad l2 norm  0.005845852249382487\n",
      "iter  8  loss  0.0064787716029257375  grad l2 norm  0.005839925318312687\n",
      "iter  9  loss  0.006449717942976707  grad l2 norm  0.005833900985705694\n",
      "unsuccessful, tol:  0.005833900985705694\n",
      "iter  0  loss  0.006420725907898  grad l2 norm  0.0058277436399037695\n",
      "iter  1  loss  0.006391797827175583  grad l2 norm  0.005821481696331281\n",
      "iter  2  loss  0.006362931829034329  grad l2 norm  0.005815081200894818\n",
      "iter  3  loss  0.0063341307455917805  grad l2 norm  0.0058085696963762826\n",
      "iter  4  loss  0.006305393288741977  grad l2 norm  0.005801915039253541\n",
      "iter  5  loss  0.006276722784080679  grad l2 norm  0.005795144264189186\n",
      "iter  6  loss  0.006248118465456896  grad l2 norm  0.0057882273821878065\n",
      "iter  7  loss  0.006219584045506878  grad l2 norm  0.0057811914077068125\n",
      "iter  8  loss  0.006191119134560996  grad l2 norm  0.005774008888807033\n",
      "iter  9  loss  0.006162727663409832  grad l2 norm  0.005766707252459275\n",
      "unsuccessful, tol:  0.005766707252459275\n",
      "iter  0  loss  0.006134409436129748  grad l2 norm  0.00575926182924505\n",
      "iter  1  loss  0.00610616842037634  grad l2 norm  0.005751700690941662\n",
      "iter  2  loss  0.006078004440425122  grad l2 norm  0.005744001965339365\n",
      "iter  3  loss  0.006049921345030061  grad l2 norm  0.0057361943674609\n",
      "iter  4  loss  0.006021918841349609  grad l2 norm  0.0057282586305899405\n",
      "iter  5  loss  0.005994000545048863  grad l2 norm  0.005720223946613223\n",
      "iter  6  loss  0.005966165951589432  grad l2 norm  0.005712073352155607\n",
      "iter  7  loss  0.0059384183701129585  grad l2 norm  0.005703836288435788\n",
      "iter  8  loss  0.005910757026754879  grad l2 norm  0.005695497778819302\n",
      "iter  9  loss  0.005883184882859075  grad l2 norm  0.005687087302672048\n",
      "unsuccessful, tol:  0.005687087302672048\n",
      "iter  0  loss  0.0058557008648869435  grad l2 norm  0.005678591593549274\n",
      "iter  1  loss  0.00582830756779871  grad l2 norm  0.005670040005328906\n",
      "iter  2  loss  0.0058010036068785326  grad l2 norm  0.005661420756757573\n",
      "iter  3  loss  0.005773791207896523  grad l2 norm  0.005652762956584501\n",
      "iter  4  loss  0.005746668677125305  grad l2 norm  0.005644056122185072\n",
      "iter  5  loss  0.005719637880691493  grad l2 norm  0.005635329023128391\n",
      "iter  6  loss  0.005692696829876241  grad l2 norm  0.00562657230746718\n",
      "iter  7  loss  0.005665847052635125  grad l2 norm  0.005617814317813513\n",
      "iter  8  loss  0.005639086291779347  grad l2 norm  0.0056090466665757845\n",
      "iter  9  loss  0.005612415771857811  grad l2 norm  0.005600297173387887\n",
      "unsuccessful, tol:  0.005600297173387887\n",
      "iter  0  loss  0.005585833007884441  grad l2 norm  0.005591558239081736\n",
      "iter  1  loss  0.005559338970526538  grad l2 norm  0.005582857047403982\n",
      "iter  2  loss  0.005532931002574158  grad l2 norm  0.0055741865968100745\n",
      "iter  3  loss  0.005506609884592797  grad l2 norm  0.005565573302680168\n",
      "iter  4  loss  0.00548037285531476  grad l2 norm  0.005557010556426384\n",
      "iter  5  loss  0.005454220578699272  grad l2 norm  0.005548523856761976\n",
      "iter  6  loss  0.005428150263120509  grad l2 norm  0.00554010677718705\n",
      "iter  7  loss  0.005402162529589393  grad l2 norm  0.005531783745890904\n",
      "iter  8  loss  0.0053762546236703354  grad l2 norm  0.005523548314832899\n",
      "iter  9  loss  0.0053504271842444474  grad l2 norm  0.0055154236967062415\n",
      "unsuccessful, tol:  0.0055154236967062415\n",
      "iter  0  loss  0.0053246775423452415  grad l2 norm  0.005507403240417319\n",
      "iter  1  loss  0.005299006390974831  grad l2 norm  0.0054995088194296484\n",
      "iter  2  loss  0.0052734111668943625  grad l2 norm  0.005491733428408306\n",
      "iter  3  loss  0.00524789262451666  grad l2 norm  0.0054840975004593236\n",
      "iter  4  loss  0.0052224482994490925  grad l2 norm  0.005476593548286535\n",
      "iter  5  loss  0.005197078992144015  grad l2 norm  0.005469240470497901\n",
      "iter  6  loss  0.0051717823141451446  grad l2 norm  0.0054620301699059405\n",
      "iter  7  loss  0.005146559088385368  grad l2 norm  0.005454979895001413\n",
      "iter  8  loss  0.005121406979359882  grad l2 norm  0.005448080780939076\n",
      "iter  9  loss  0.0050963268159693875  grad l2 norm  0.005441348264915265\n",
      "unsuccessful, tol:  0.005441348264915265\n",
      "iter  0  loss  0.00507131630518483  grad l2 norm  0.005434772513259894\n",
      "iter  1  loss  0.005046376280722359  grad l2 norm  0.005428366949302444\n",
      "iter  2  loss  0.005021504497268482  grad l2 norm  0.005422120550329917\n",
      "iter  3  loss  0.004996701805415766  grad l2 norm  0.005416044529671505\n",
      "iter  4  loss  0.004971966021606854  grad l2 norm  0.005410126503671489\n",
      "iter  5  loss  0.00494729802796869  grad l2 norm  0.0054043753691872835\n",
      "iter  6  loss  0.004922695712821967  grad l2 norm  0.005398777345764967\n",
      "iter  7  loss  0.004898159993984001  grad l2 norm  0.005393339079655756\n",
      "iter  8  loss  0.004873688826146722  grad l2 norm  0.005388045557149429\n",
      "iter  9  loss  0.004849283147614848  grad l2 norm  0.005382901447174923\n",
      "unsuccessful, tol:  0.005382901447174923\n",
      "iter  0  loss  0.004824940953330187  grad l2 norm  0.0053778908636049755\n",
      "iter  1  loss  0.004800663166852982  grad l2 norm  0.0053730169389008394\n",
      "iter  2  loss  0.00477644778007491  grad l2 norm  0.005368263397424246\n",
      "iter  3  loss  0.004752295653437329  grad l2 norm  0.0053636323430327985\n",
      "iter  4  loss  0.004728204724528782  grad l2 norm  0.005359107600802528\n",
      "iter  5  loss  0.004704175739618849  grad l2 norm  0.005354690706478766\n",
      "iter  6  loss  0.00468020653330924  grad l2 norm  0.00535036598051205\n",
      "iter  7  loss  0.004656297693780502  grad l2 norm  0.005346134722695977\n",
      "iter  8  loss  0.004632446915009131  grad l2 norm  0.0053419819930141535\n",
      "iter  9  loss  0.004608654597316614  grad l2 norm  0.005337909032603823\n",
      "unsuccessful, tol:  0.005337909032603823\n",
      "iter  0  loss  0.004584918272970627  grad l2 norm  0.005333901733188874\n",
      "iter  1  loss  0.004561238143117972  grad l2 norm  0.005329961319849179\n",
      "iter  2  loss  0.004537611577121344  grad l2 norm  0.0053260744915587166\n",
      "iter  3  loss  0.004514038586778234  grad l2 norm  0.00532224241047686\n",
      "iter  4  loss  0.004490516399417638  grad l2 norm  0.0053184524911879\n",
      "iter  5  loss  0.004467044870191343  grad l2 norm  0.005314705746450607\n",
      "iter  6  loss  0.004443621128467134  grad l2 norm  0.005310990194693482\n",
      "iter  7  loss  0.004420244928544098  grad l2 norm  0.005307306614935744\n",
      "iter  8  loss  0.004396913367898365  grad l2 norm  0.005303643533182809\n",
      "iter  9  loss  0.004373626175620523  grad l2 norm  0.005300001440676722\n",
      "unsuccessful, tol:  0.005300001440676722\n",
      "iter  0  loss  0.004350380499454079  grad l2 norm  0.005296369311389284\n",
      "iter  1  loss  0.004327176129615512  grad l2 norm  0.005292747338333003\n",
      "iter  2  loss  0.004304010349639413  grad l2 norm  0.005289124927115737\n",
      "iter  3  loss  0.004280883091655028  grad l2 norm  0.005285502005160851\n",
      "iter  4  loss  0.004257791844773595  grad l2 norm  0.005281868430769925\n",
      "iter  5  loss  0.004234736737554884  grad l2 norm  0.005278223930335163\n",
      "iter  6  loss  0.004211715498296539  grad l2 norm  0.005274558859120818\n",
      "iter  7  loss  0.00418872846237738  grad l2 norm  0.00527087282244436\n",
      "iter  8  loss  0.00416577358186073  grad l2 norm  0.0052671567234829795\n",
      "iter  9  loss  0.004142851359399652  grad l2 norm  0.005263410125903791\n",
      "unsuccessful, tol:  0.005263410125903791\n",
      "iter  0  loss  0.004119959908902392  grad l2 norm  0.005259624524774227\n",
      "iter  1  loss  0.004097099821810876  grad l2 norm  0.005255799509503105\n",
      "iter  2  loss  0.004074269283839385  grad l2 norm  0.005251927195242905\n",
      "iter  3  loss  0.004051468881417849  grad l2 norm  0.005248007245512065\n",
      "iter  4  loss  0.004028696780022817  grad l2 norm  0.005244032402711873\n",
      "iter  5  loss  0.004005953477701839  grad l2 norm  0.005240002429506925\n",
      "iter  6  loss  0.003983237048617734  grad l2 norm  0.005235910678438892\n",
      "iter  7  loss  0.003960547848071137  grad l2 norm  0.0052317570106937845\n",
      "iter  8  loss  0.0039378838222961865  grad l2 norm  0.005227535346775018\n",
      "iter  9  loss  0.003915245167033013  grad l2 norm  0.005223245621080757\n",
      "unsuccessful, tol:  0.005223245621080757\n",
      "iter  0  loss  0.0038926297027415692  grad l2 norm  0.0052188822581009705\n",
      "iter  1  loss  0.003870037487363316  grad l2 norm  0.00521444522098929\n",
      "iter  2  loss  0.003847466255145389  grad l2 norm  0.0052099293601476256\n",
      "iter  3  loss  0.0038249159834231553  grad l2 norm  0.005205334613070795\n",
      "iter  4  loss  0.0038023843924808454  grad l2 norm  0.005200656173812643\n",
      "iter  5  loss  0.0037798714651862395  grad l2 norm  0.0051958938994793305\n",
      "iter  6  loss  0.003757375004061571  grad l2 norm  0.005191043249878451\n",
      "iter  7  loss  0.003734895101009175  grad l2 norm  0.0051861039534769\n",
      "iter  8  loss  0.0037124297460611067  grad l2 norm  0.005181071667173699\n",
      "iter  9  loss  0.003689979242785277  grad l2 norm  0.0051759459522922775\n",
      "unsuccessful, tol:  0.0051759459522922775\n",
      "iter  0  loss  0.0036675418620324028  grad l2 norm  0.005170722605839488\n",
      "iter  1  loss  0.003645118198249453  grad l2 norm  0.005165400995939326\n",
      "iter  2  loss  0.0036227068623671584  grad l2 norm  0.005159977018453271\n",
      "iter  3  loss  0.003600308776306099  grad l2 norm  0.0051544498408453475\n",
      "iter  4  loss  0.003577922902422908  grad l2 norm  0.005148815440594551\n",
      "iter  5  loss  0.003555550477526058  grad l2 norm  0.005143072805303602\n",
      "iter  6  loss  0.0035331907805930037  grad l2 norm  0.005137218010284728\n",
      "iter  7  loss  0.003510845311424548  grad l2 norm  0.0051312499194965475\n",
      "iter  8  loss  0.0034885136011768942  grad l2 norm  0.005125164758573747\n",
      "iter  9  loss  0.0034661973421897634  grad l2 norm  0.005118961356528656\n",
      "unsuccessful, tol:  0.005118961356528656\n",
      "iter  0  loss  0.0034438962458675864  grad l2 norm  0.005112636168910967\n",
      "iter  1  loss  0.0034216121289177785  grad l2 norm  0.0051061880962733714\n",
      "iter  2  loss  0.0033993448210027873  grad l2 norm  0.0050996139121928215\n",
      "iter  3  loss  0.0033770962100890703  grad l2 norm  0.005092912692814305\n",
      "iter  4  loss  0.0033548661998371123  grad l2 norm  0.005086081606806368\n",
      "iter  5  loss  0.0033326567146235147  grad l2 norm  0.0050791199906131766\n",
      "iter  6  loss  0.003310467705055541  grad l2 norm  0.005072025461055108\n",
      "iter  7  loss  0.003288301112175609  grad l2 norm  0.005064797671877894\n",
      "iter  8  loss  0.0032661569189792908  grad l2 norm  0.0050574347132901045\n",
      "iter  9  loss  0.003244037073175895  grad l2 norm  0.005049936585828018\n",
      "unsuccessful, tol:  0.005049936585828018\n",
      "iter  0  loss  0.0032219415827730064  grad l2 norm  0.005042301853841758\n",
      "iter  1  loss  0.003199872397091596  grad l2 norm  0.005034530872243839\n",
      "iter  2  loss  0.003177829544918406  grad l2 norm  0.005026622662737195\n",
      "iter  3  loss  0.003155814973894357  grad l2 norm  0.005018577927802156\n",
      "iter  4  loss  0.0031338287301551247  grad l2 norm  0.005010396119603276\n",
      "iter  5  loss  0.0031118727565426574  grad l2 norm  0.0050020782739310715\n",
      "iter  6  loss  0.0030899471129467115  grad l2 norm  0.004993624242543251\n",
      "iter  7  loss  0.0030680537339827316  grad l2 norm  0.0049850353780266045\n",
      "iter  8  loss  0.003046192689404949  grad l2 norm  0.004976311901063221\n",
      "iter  9  loss  0.0030243659019698545  grad l2 norm  0.004967455465488333\n",
      "unsuccessful, tol:  0.004967455465488333\n",
      "iter  0  loss  0.003002573447350492  grad l2 norm  0.004958466632769214\n",
      "iter  1  loss  0.00298081723290445  grad l2 norm  0.00494934734499464\n",
      "iter  2  loss  0.0029590973365227883  grad l2 norm  0.00494009847974882\n",
      "iter  3  loss  0.002937415646985506  grad l2 norm  0.004930722257382453\n",
      "iter  4  loss  0.002915772241159732  grad l2 norm  0.0049212198504510335\n",
      "iter  5  loss  0.00289416898658182  grad l2 norm  0.004911593750387128\n",
      "iter  6  loss  0.002872605956400275  grad l2 norm  0.004901845406708545\n",
      "iter  7  loss  0.0028510849947851894  grad l2 norm  0.00489197757706907\n",
      "iter  8  loss  0.002829606169000833  grad l2 norm  0.004881991972518424\n",
      "iter  9  loss  0.0028081712982155702  grad l2 norm  0.004871891613841832\n",
      "unsuccessful, tol:  0.004871891613841832\n",
      "iter  0  loss  0.002786780442088614  grad l2 norm  0.004861678460214954\n",
      "iter  1  loss  0.002765435393536473  grad l2 norm  0.004851355793720502\n",
      "iter  2  loss  0.0027441362032250634  grad l2 norm  0.00484092580978876\n",
      "iter  3  loss  0.0027228846368393266  grad l2 norm  0.004830392050804407\n",
      "iter  4  loss  0.0027016807348841073  grad l2 norm  0.004819756937741444\n",
      "iter  5  loss  0.0026805262349975266  grad l2 norm  0.0048090242727994465\n",
      "iter  6  loss  0.0026594211664774645  grad l2 norm  0.00479819669263565\n",
      "iter  7  loss  0.002638367238176023  grad l2 norm  0.004787278259039567\n",
      "iter  8  loss  0.0026173644671827604  grad l2 norm  0.004776271815117293\n",
      "iter  9  loss  0.002596414532832759  grad l2 norm  0.004765181682092096\n",
      "unsuccessful, tol:  0.004765181682092096\n",
      "iter  0  loss  0.0025755174389908353  grad l2 norm  0.004754010900723584\n",
      "iter  1  loss  0.0025546748347024745  grad l2 norm  0.004742764051425907\n",
      "iter  2  loss  0.0025338867095340716  grad l2 norm  0.004731444364108427\n",
      "iter  3  loss  0.0025131546813902603  grad l2 norm  0.004720056677920679\n",
      "iter  4  loss  0.0024924787243761097  grad l2 norm  0.0047086044035753154\n",
      "iter  5  loss  0.0024718604242988293  grad l2 norm  0.004697092638176653\n",
      "iter  6  loss  0.0024512997385257185  grad l2 norm  0.004685524964934583\n",
      "iter  7  loss  0.0024307982196815265  grad l2 norm  0.004673906737689461\n",
      "iter  8  loss  0.002410355806984711  grad l2 norm  0.004662241703769579\n",
      "iter  9  loss  0.0023899740186442554  grad l2 norm  0.004650535472002036\n",
      "unsuccessful, tol:  0.004650535472002036\n",
      "iter  0  loss  0.0023696527741631427  grad l2 norm  0.004638791945274743\n",
      "iter  1  loss  0.0023493935559317416  grad l2 norm  0.004627016985021039\n",
      "iter  2  loss  0.0023291962619936013  grad l2 norm  0.004615214640849805\n",
      "iter  3  loss  0.0023090623373244524  grad l2 norm  0.004603391023698945\n",
      "iter  4  loss  0.0022889916565614135  grad l2 norm  0.004591550320675381\n",
      "iter  5  loss  0.0022689856254490596  grad l2 norm  0.004579698888302766\n",
      "iter  6  loss  0.0022490440930382372  grad l2 norm  0.004567841041492114\n",
      "iter  7  loss  0.0022291684237725753  grad l2 norm  0.00455598337753117\n",
      "iter  8  loss  0.002209358438668215  grad l2 norm  0.004544130328864797\n",
      "iter  9  loss  0.00218961545850565  grad l2 norm  0.00453228872772956\n",
      "unsuccessful, tol:  0.00453228872772956\n",
      "iter  0  loss  0.00216993927350619  grad l2 norm  0.004520463113166909\n",
      "iter  1  loss  0.00215033115808569  grad l2 norm  0.004508660545457966\n",
      "iter  2  loss  0.0021307908685475  grad l2 norm  0.004496885658539268\n",
      "iter  3  loss  0.0021113196298458785  grad l2 norm  0.004485145732652032\n",
      "iter  4  loss  0.0020919171608184415  grad l2 norm  0.004473445484061491\n",
      "iter  5  loss  0.002072584633397511  grad l2 norm  0.004461792403596892\n",
      "iter  6  loss  0.0020533217249086508  grad l2 norm  0.00445019127630592\n",
      "iter  7  loss  0.0020341295501583785  grad l2 norm  0.0044386497928348674\n",
      "iter  8  loss  0.002015007740326983  grad l2 norm  0.0044271727923727204\n",
      "iter  9  loss  0.0019959573483510186  grad l2 norm  0.0044157681530793205\n",
      "unsuccessful, tol:  0.0044157681530793205\n",
      "iter  0  loss  0.00197697795394069  grad l2 norm  0.0044044407523889965\n",
      "iter  1  loss  0.001958070542664297  grad l2 norm  0.0043931986419724015\n",
      "iter  2  loss  0.0019392346366215378  grad l2 norm  0.004382046720186544\n",
      "iter  3  loss  0.0019204711476259286  grad l2 norm  0.004370993196316443\n",
      "iter  4  loss  0.0019017795330667565  grad l2 norm  0.004360042970671928\n",
      "iter  5  loss  0.001883160623573601  grad l2 norm  0.004349204392120722\n",
      "iter  6  loss  0.0018646138036005123  grad l2 norm  0.004338482342044619\n",
      "iter  7  loss  0.0018461398139466361  grad l2 norm  0.004327885288435425\n",
      "iter  8  loss  0.0018277379565984921  grad l2 norm  0.004317418070638839\n",
      "iter  9  loss  0.001809408872471153  grad l2 norm  0.004307089252545045\n",
      "unsuccessful, tol:  0.004307089252545045\n",
      "iter  0  loss  0.0017911517700494725  grad l2 norm  0.004296903605763233\n",
      "iter  1  loss  0.001772967178709625  grad l2 norm  0.004286869763699632\n",
      "iter  2  loss  0.0017548542007103018  grad l2 norm  0.004276992401527166\n",
      "iter  3  loss  0.001736813240456315  grad l2 norm  0.004267280192203103\n",
      "iter  4  loss  0.001718843279411245  grad l2 norm  0.004257737682321214\n",
      "iter  5  loss  0.001700944581680308  grad l2 norm  0.004248373550388617\n",
      "iter  6  loss  0.0016831159914530744  grad l2 norm  0.004239192178386957\n",
      "iter  7  loss  0.001665357615320709  grad l2 norm  0.004230202211888888\n",
      "iter  8  loss  0.0016476681419227703  grad l2 norm  0.004221407827878037\n",
      "iter  9  loss  0.0016300475014869855  grad l2 norm  0.004212817595554538\n",
      "unsuccessful, tol:  0.004212817595554538\n",
      "iter  0  loss  0.0016124942074661407  grad l2 norm  0.004204435441697399\n",
      "iter  1  loss  0.0015950079939175653  grad l2 norm  0.004196269810264053\n",
      "iter  2  loss  0.0015775871790944045  grad l2 norm  0.004188324327277467\n",
      "iter  3  loss  0.0015602312814941574  grad l2 norm  0.004180607256553106\n",
      "iter  4  loss  0.001542938405643006  grad l2 norm  0.00417312186676468\n",
      "iter  5  loss  0.0015257078379629287  grad l2 norm  0.004165876179825766\n",
      "iter  6  loss  0.0015085374553826514  grad l2 norm  0.004158873043370119\n",
      "iter  7  loss  0.0014914263026151995  grad l2 norm  0.004152120167431618\n",
      "iter  8  loss  0.0014743720247374801  grad l2 norm  0.004145619906089908\n",
      "iter  9  loss  0.0014573734280009859  grad l2 norm  0.004139379577140268\n",
      "unsuccessful, tol:  0.004139379577140268\n",
      "iter  0  loss  0.0014404279380600242  grad l2 norm  0.00413340095706864\n",
      "iter  1  loss  0.001423534146903246  grad l2 norm  0.00412769087743095\n",
      "iter  2  loss  0.0014066892987366888  grad l2 norm  0.004122250437802191\n",
      "iter  3  loss  0.0013898918256666222  grad l2 norm  0.004117085871816461\n",
      "iter  4  loss  0.0013731388628769035  grad l2 norm  0.0041121974835614744\n",
      "iter  5  loss  0.0013564287747232272  grad l2 norm  0.004107590776117383\n",
      "iter  6  loss  0.0013397586991253372  grad l2 norm  0.004103265118705666\n",
      "iter  7  loss  0.0013231270631283271  grad l2 norm  0.004099225131841797\n",
      "iter  8  loss  0.0013065311529480783  grad l2 norm  0.00409546909607978\n",
      "iter  9  loss  0.0012899696138443995  grad l2 norm  0.0040920005910960915\n",
      "unsuccessful, tol:  0.0040920005910960915\n",
      "iter  0  loss  0.0012734400377352546  grad l2 norm  0.004088816662186518\n",
      "iter  1  loss  0.0012569414380342696  grad l2 norm  0.004085919715003753\n",
      "iter  2  loss  0.0012404718437511316  grad l2 norm  0.0040833054611924095\n",
      "iter  3  loss  0.0012240307390201503  grad l2 norm  0.004080975073028264\n",
      "iter  4  loss  0.001207616654452737  grad l2 norm  0.004078922930626121\n",
      "iter  5  loss  0.0011912295659273276  grad l2 norm  0.004077149037048456\n",
      "iter  6  loss  0.0011748684813738467  grad l2 norm  0.004075646582806218\n",
      "iter  7  loss  0.001158533802277447  grad l2 norm  0.004074414610689165\n",
      "iter  8  loss  0.0011422249128814153  grad l2 norm  0.004073445400523045\n",
      "iter  9  loss  0.0011259425149831844  grad l2 norm  0.004072737359135507\n",
      "unsuccessful, tol:  0.004072737359135507\n",
      "iter  0  loss  0.0011096862307833607  grad l2 norm  0.004072282217120579\n",
      "iter  1  loss  0.0010934569224002521  grad l2 norm  0.004072078111425622\n",
      "iter  2  loss  0.0010772543173391677  grad l2 norm  0.004072116583479287\n",
      "iter  3  loss  0.0010610793196756734  grad l2 norm  0.004072395824025143\n",
      "iter  4  loss  0.001044931661606139  grad l2 norm  0.004072907468767786\n",
      "iter  5  loss  0.0010288122077932828  grad l2 norm  0.004073649984154326\n",
      "iter  6  loss  0.0010127206318830168  grad l2 norm  0.004074615267321544\n",
      "iter  7  loss  0.000996657712208175  grad l2 norm  0.004075802161312774\n",
      "iter  8  loss  0.0009806230302157322  grad l2 norm  0.004077202874953261\n",
      "iter  9  loss  0.0009646172549330409  grad l2 norm  0.0040788166223199775\n",
      "unsuccessful, tol:  0.0040788166223199775\n",
      "iter  0  loss  0.0009486398603264166  grad l2 norm  0.00408063588287776\n",
      "iter  1  loss  0.0009326913965361967  grad l2 norm  0.004082660162166825\n",
      "iter  2  loss  0.0009167712240622808  grad l2 norm  0.004084882113520145\n",
      "iter  3  loss  0.0009008797707028645  grad l2 norm  0.00408730141608864\n",
      "iter  4  loss  0.0008850162813374308  grad l2 norm  0.004089910778240577\n",
      "iter  5  loss  0.0008691810602465891  grad l2 norm  0.004092709926740265\n",
      "iter  6  loss  0.0008533732358676652  grad l2 norm  0.004095691509876292\n",
      "iter  7  loss  0.0008375929886331448  grad l2 norm  0.004098855189017595\n",
      "iter  8  loss  0.0008218393304230286  grad l2 norm  0.004102193457500708\n",
      "iter  9  loss  0.000806112318311702  grad l2 norm  0.004105705823568907\n",
      "unsuccessful, tol:  0.004105705823568907\n",
      "iter  0  loss  0.0007904108486286019  grad l2 norm  0.004109384559528028\n",
      "iter  1  loss  0.0007747348571413853  grad l2 norm  0.004113228963506018\n",
      "iter  2  loss  0.0007590831277423782  grad l2 norm  0.004117231052486833\n",
      "iter  3  loss  0.000743455479677454  grad l2 norm  0.004121389889445541\n",
      "iter  4  loss  0.0007278505909416605  grad l2 norm  0.004125697233482983\n",
      "iter  5  loss  0.0007122681732095786  grad l2 norm  0.004130151918517595\n",
      "iter  6  loss  0.0006967068100538957  grad l2 norm  0.0041347454736720525\n",
      "iter  7  loss  0.0006811661202376117  grad l2 norm  0.0041394765396417545\n",
      "iter  8  loss  0.0006656446108451355  grad l2 norm  0.004144336472437517\n",
      "iter  9  loss  0.0006501418295299967  grad l2 norm  0.004149323783546651\n",
      "unsuccessful, tol:  0.004149323783546651\n",
      "iter  0  loss  0.0006346562325371812  grad l2 norm  0.004154429739871211\n",
      "iter  1  loss  0.000619187326323913  grad l2 norm  0.004159652813690591\n",
      "iter  2  loss  0.0006037335502190864  grad l2 norm  0.004164984290923795\n",
      "iter  3  loss  0.000588294407517041  grad l2 norm  0.0041704227165410696\n",
      "iter  4  loss  0.0005728683621464251  grad l2 norm  0.004175959522043663\n",
      "iter  5  loss  0.0005574549588800745  grad l2 norm  0.004181593451404594\n",
      "iter  6  loss  0.0005420527329019043  grad l2 norm  0.004187316216905257\n",
      "iter  7  loss  0.0005266613182428887  grad l2 norm  0.00419312689010262\n",
      "iter  8  loss  0.0005112793686513298  grad l2 norm  0.004199017593251932\n",
      "iter  9  loss  0.0004959066529082028  grad l2 norm  0.004204987839656881\n",
      "unsuccessful, tol:  0.004204987839656881\n",
      "iter  0  loss  0.0004805419851152258  grad l2 norm  0.004211030266381374\n",
      "iter  1  loss  0.00046518530537955396  grad l2 norm  0.0042171449092622305\n",
      "iter  2  loss  0.0004498356177575142  grad l2 norm  0.0042233249822213334\n",
      "iter  3  loss  0.0004344930551219829  grad l2 norm  0.004229571074699557\n",
      "iter  4  loss  0.00041915682366037375  grad l2 norm  0.004235876984030946\n",
      "iter  5  loss  0.00040382725157195077  grad l2 norm  0.004242243827023387\n",
      "iter  6  loss  0.0003885037401051623  grad l2 norm  0.004248665934242671\n",
      "iter  7  loss  0.0003731867968304895  grad l2 norm  0.0042551448719517345\n",
      "iter  8  loss  0.0003578759943380495  grad l2 norm  0.004261675409629686\n",
      "iter  9  loss  0.000342571989744604  grad l2 norm  0.004268259451929527\n",
      "unsuccessful, tol:  0.004268259451929527\n",
      "iter  0  loss  0.00032727449288706705  grad l2 norm  0.004274892091855676\n",
      "iter  1  loss  0.00031198427405154693  grad l2 norm  0.004281575453571587\n",
      "iter  2  loss  0.0002967011435158648  grad l2 norm  0.00428830484261014\n",
      "iter  3  loss  0.0002814259491475424  grad l2 norm  0.004295082499191173\n",
      "iter  4  loss  0.00026615856856979444  grad l2 norm  0.004301903854061722\n",
      "iter  5  loss  0.0002508998975350441  grad l2 norm  0.004308771189646652\n",
      "iter  6  loss  0.00023564985519188415  grad l2 norm  0.004315680006864646\n",
      "iter  7  loss  0.0002204093634130452  grad l2 norm  0.004322632589488496\n",
      "iter  8  loss  0.0002051783650022877  grad l2 norm  0.00432962448468266\n",
      "iter  9  loss  0.00018995779371673113  grad l2 norm  0.004336657965297008\n",
      "unsuccessful, tol:  0.004336657965297008\n",
      "iter  0  loss  0.00017474760493233494  grad l2 norm  0.004343728625287542\n",
      "iter  1  loss  0.00015954873597899368  grad l2 norm  0.004350838735283969\n",
      "iter  2  loss  0.00014436114872151072  grad l2 norm  0.004357983953173346\n",
      "iter  3  loss  0.0001291857798895646  grad l2 norm  0.004365166569534016\n",
      "iter  4  loss  0.0001140225950815615  grad l2 norm  0.004372382333036039\n",
      "iter  5  loss  9.887252887654159e-05  grad l2 norm  0.004379633583602499\n",
      "iter  6  loss  8.373554982200398e-05  grad l2 norm  0.0043869161916477144\n",
      "iter  7  loss  6.86125901850387e-05  grad l2 norm  0.004394232577787793\n",
      "iter  8  loss  5.350362150717146e-05  grad l2 norm  0.004401578764323498\n",
      "iter  9  loss  3.8409573883089024e-05  grad l2 norm  0.004408957280976113\n",
      "unsuccessful, tol:  0.004408957280976113\n",
      "iter  0  loss  2.3330421669521357e-05  grad l2 norm  0.004416364326564623\n",
      "iter  1  loss  8.267092198518667e-06  grad l2 norm  0.004423802560702155\n",
      "iter  2  loss  -6.780438754648118e-06  grad l2 norm  0.004431268373457189\n",
      "iter  3  loss  -2.1811248878671105e-05  grad l2 norm  0.00443876456343749\n",
      "iter  4  loss  -3.6825364441240506e-05  grad l2 norm  0.004446287713437682\n",
      "iter  5  loss  -5.182187279503406e-05  grad l2 norm  0.004453840755937792\n",
      "iter  6  loss  -6.680080832324059e-05  grad l2 norm  0.004461420453294165\n",
      "iter  7  loss  -8.176127541317135e-05  grad l2 norm  0.0044690298524373995\n",
      "iter  8  loss  -9.670332535999043e-05  grad l2 norm  0.004476665868905154\n",
      "iter  9  loss  -0.00011162608955833769  grad l2 norm  0.00448433163326107\n",
      "unsuccessful, tol:  0.00448433163326107\n",
      "iter  0  loss  -0.00012652964738114512  grad l2 norm  0.004492024178936473\n",
      "iter  1  loss  -0.0001414131692468333  grad l2 norm  0.004499746683397693\n",
      "iter  2  loss  -0.00015627677541975608  grad l2 norm  0.0045074962601032165\n",
      "iter  3  loss  -0.00017111968856906904  grad l2 norm  0.004515276097477198\n",
      "iter  4  loss  -0.00018594208342138138  grad l2 norm  0.004523083355128823\n",
      "iter  5  loss  -0.00020074324843709324  grad l2 norm  0.004530921203240241\n",
      "iter  6  loss  -0.00021552342627125196  grad l2 norm  0.00453878682259288\n",
      "iter  7  loss  -0.00023028198422858493  grad l2 norm  0.004546683346172461\n",
      "iter  8  loss  -0.0002450192455222579  grad l2 norm  0.004554607961763853\n",
      "iter  9  loss  -0.000259734668195317  grad l2 norm  0.00456256375676437\n",
      "unsuccessful, tol:  0.00456256375676437\n",
      "iter  0  loss  -0.0002744286671759725  grad l2 norm  0.004570547921444746\n",
      "iter  1  loss  -0.0002891008013432203  grad l2 norm  0.004578563497399472\n",
      "iter  2  loss  -0.00030375158634479065  grad l2 norm  0.004586607679270693\n",
      "iter  3  loss  -0.0003183806894315291  grad l2 norm  0.0045946834670916184\n",
      "iter  4  loss  -0.00033298873293328437  grad l2 norm  0.004602788064341733\n",
      "iter  5  loss  -0.00034757549640744323  grad l2 norm  0.004610924434591386\n",
      "iter  6  loss  -0.0003621417105880387  grad l2 norm  0.004619089794065622\n",
      "iter  7  loss  -0.0003766872663276856  grad l2 norm  0.004627287073296761\n",
      "iter  8  loss  -0.000391212998800216  grad l2 norm  0.004635513502728404\n",
      "iter  9  loss  -0.00040571890273259734  grad l2 norm  0.0046437719803523805\n",
      "unsuccessful, tol:  0.0046437719803523805\n",
      "iter  0  loss  -0.0004202059067317849  grad l2 norm  0.0046520597491556915\n",
      "iter  1  loss  -0.0004346740944907464  grad l2 norm  0.004660379671855932\n",
      "iter  2  loss  -0.000449124469412817  grad l2 norm  0.0046687289991384565\n",
      "iter  3  loss  -0.0004635571818997269  grad l2 norm  0.004677110552644543\n",
      "iter  4  loss  -0.00047797328498432886  grad l2 norm  0.0046855215830012694\n",
      "iter  5  loss  -0.0004923729684263164  grad l2 norm  0.004693964862199617\n",
      "iter  6  loss  -0.000506757306705169  grad l2 norm  0.004702437630470543\n",
      "iter  7  loss  -0.0005211265012282912  grad l2 norm  0.004710942599233992\n",
      "iter  8  loss  -0.0005354816224584098  grad l2 norm  0.004719476986006972\n",
      "iter  9  loss  -0.0005498228617078546  grad l2 norm  0.004728043429224438\n",
      "unsuccessful, tol:  0.004728043429224438\n",
      "iter  0  loss  -0.0005641512690213602  grad l2 norm  0.0047366391105845874\n",
      "iter  1  loss  -0.0005784670155289853  grad l2 norm  0.004745266583243114\n",
      "iter  2  loss  -0.0005927711278949705  grad l2 norm  0.004753922981222707\n",
      "iter  3  loss  -0.0006070637611710647  grad l2 norm  0.004762610762638439\n",
      "iter  4  loss  -0.000621345929512801  grad l2 norm  0.0047713270059254525\n",
      "iter  5  loss  -0.0006356177884349245  grad l2 norm  0.00478007406975061\n",
      "iter  6  loss  -0.0006498803606603147  grad l2 norm  0.00478884897574755\n",
      "iter  7  loss  -0.0006641338261292329  grad l2 norm  0.004797653986594596\n",
      "iter  8  loss  -0.0006783792416163597  grad l2 norm  0.004806486074738819\n",
      "iter  9  loss  -0.0006926168369043964  grad l2 norm  0.004815347419851729\n",
      "unsuccessful, tol:  0.004815347419851729\n",
      "iter  0  loss  -0.0007068477270912519  grad l2 norm  0.004824234962784355\n",
      "iter  1  loss  -0.0007210722137639553  grad l2 norm  0.004833150823365413\n",
      "iter  2  loss  -0.0007352914894062547  grad l2 norm  0.0048420919386441726\n",
      "iter  3  loss  -0.0007495059428906584  grad l2 norm  0.00485106040180391\n",
      "iter  4  loss  -0.0007637168558385673  grad l2 norm  0.004860053183606629\n",
      "iter  5  loss  -0.0007779247121271206  grad l2 norm  0.004869072393002169\n",
      "iter  6  loss  -0.0007921308863800616  grad l2 norm  0.004878115080493607\n",
      "iter  7  loss  -0.0008063359574646388  grad l2 norm  0.004887183420710739\n",
      "iter  8  loss  -0.000820541389547821  grad l2 norm  0.004896274596031748\n",
      "iter  9  loss  -0.0008347478497984309  grad l2 norm  0.004905390900864017\n",
      "unsuccessful, tol:  0.004905390900864017\n",
      "iter  0  loss  -0.000848956882678573  grad l2 norm  0.0049145297032449535\n",
      "iter  1  loss  -0.0008631692322777846  grad l2 norm  0.004923693469799898\n",
      "iter  2  loss  -0.0008773865106845136  grad l2 norm  0.004932879802218723\n",
      "iter  3  loss  -0.0008916095254680039  grad l2 norm  0.004942091380950163\n",
      "iter  4  loss  -0.0009058399429401332  grad l2 norm  0.004951326072578588\n",
      "iter  5  loss  -0.000920078621080684  grad l2 norm  0.004960586789475904\n",
      "iter  6  loss  -0.0009343272680673569  grad l2 norm  0.004969871663460708\n",
      "iter  7  loss  -0.000948586780266582  grad l2 norm  0.004979183818178782\n",
      "iter  8  loss  -0.0009628588955348592  grad l2 norm  0.004988521604377489\n",
      "iter  9  loss  -0.0009771445348870265  grad l2 norm  0.004997888282166782\n",
      "unsuccessful, tol:  0.004997888282166782\n",
      "iter  0  loss  -0.0009914454490821249  grad l2 norm  0.005007282314340478\n",
      "iter  1  loss  -0.0010057625617891474  grad l2 norm  0.005016706957591178\n",
      "iter  2  loss  -0.0010200976076226494  grad l2 norm  0.005026160613425889\n",
      "iter  3  loss  -0.0010344514749263442  grad l2 norm  0.0050356463317967025\n",
      "iter  4  loss  -0.0010488258345565543  grad l2 norm  0.005045162225096955\n",
      "iter  5  loss  -0.0010632214823958492  grad l2 norm  0.005054710894806314\n",
      "iter  6  loss  -0.0010776399609226843  grad l2 norm  0.005064289921948675\n",
      "iter  7  loss  -0.0010920819046940113  grad l2 norm  0.005073901234743276\n",
      "iter  8  loss  -0.001106548659747391  grad l2 norm  0.0050835416943624204\n",
      "iter  9  loss  -0.001121040637514113  grad l2 norm  0.005093212423151627\n",
      "unsuccessful, tol:  0.005093212423151627\n",
      "iter  0  loss  -0.001135558937964554  grad l2 norm  0.005102909500891775\n",
      "iter  1  loss  -0.0011501037165093022  grad l2 norm  0.005112633263616701\n",
      "iter  2  loss  -0.0011646758141503012  grad l2 norm  0.005122379111011841\n",
      "iter  3  loss  -0.0011792751379258177  grad l2 norm  0.005132146768372108\n",
      "iter  4  loss  -0.001193902296773881  grad l2 norm  0.005141931188605059\n",
      "iter  5  loss  -0.001208556991832746  grad l2 norm  0.005151731756305934\n",
      "iter  6  loss  -0.0012232396536000878  grad l2 norm  0.00516154326032942\n",
      "iter  7  loss  -0.0012379498368046396  grad l2 norm  0.005171365018427386\n",
      "iter  8  loss  -0.0012526878544476688  grad l2 norm  0.005181191902073619\n",
      "iter  9  loss  -0.0012674531732087331  grad l2 norm  0.0051910233701298785\n",
      "unsuccessful, tol:  0.0051910233701298785\n",
      "iter  0  loss  -0.0012822460419871402  grad l2 norm  0.005200854539417181\n",
      "iter  1  loss  -0.0012970658860542333  grad l2 norm  0.005210685127229429\n",
      "iter  2  loss  -0.0013119129298127356  grad l2 norm  0.005220510570464317\n",
      "iter  3  loss  -0.0013267865895659871  grad l2 norm  0.005230330883311751\n",
      "iter  4  loss  -0.0013416870913910974  grad l2 norm  0.005240141831450632\n",
      "iter  5  loss  -0.001356613863073908  grad l2 norm  0.005249943712512499\n",
      "iter  6  loss  -0.0013715671483134695  grad l2 norm  0.005259732591432718\n",
      "iter  7  loss  -0.0013865463983955366  grad l2 norm  0.005269509009745345\n",
      "iter  8  loss  -0.0014015518838338347  grad l2 norm  0.005279269286170925\n",
      "iter  9  loss  -0.0014165830861285317  grad l2 norm  0.005289014158608909\n",
      "unsuccessful, tol:  0.005289014158608909\n",
      "iter  0  loss  -0.001431640307725059  grad l2 norm  0.005298740152071335\n",
      "iter  1  loss  -0.0014467230639550175  grad l2 norm  0.005308448155455125\n",
      "iter  2  loss  -0.0014618316920019469  grad l2 norm  0.005318134857563349\n",
      "iter  3  loss  -0.0014769657429284204  grad l2 norm  0.0053278012595821735\n",
      "iter  4  loss  -0.001492125590212602  grad l2 norm  0.005337444179162466\n",
      "iter  5  loss  -0.0015073108216322054  grad l2 norm  0.005347064698886738\n",
      "iter  6  loss  -0.0015225218478667603  grad l2 norm  0.005356659738041848\n",
      "iter  7  loss  -0.0015377582939201755  grad l2 norm  0.005366230437038625\n",
      "iter  8  loss  -0.001553020608252267  grad l2 norm  0.005375773796415125\n",
      "iter  9  loss  -0.0015683084533612407  grad l2 norm  0.005385290997037462\n",
      "unsuccessful, tol:  0.005385290997037462\n",
      "iter  0  loss  -0.0015836223159003163  grad l2 norm  0.005394779105906241\n",
      "iter  1  loss  -0.0015989618959961385  grad l2 norm  0.005404239331913866\n",
      "iter  2  loss  -0.0016143277188141187  grad l2 norm  0.005413668798119839\n",
      "iter  3  loss  -0.0016297195221510856  grad l2 norm  0.005423068732820764\n",
      "iter  4  loss  -0.0016451378699298218  grad l2 norm  0.005432436308066819\n",
      "iter  5  loss  -0.0016605825375713818  grad l2 norm  0.00544177276579821\n",
      "iter  6  loss  -0.001676054127922202  grad l2 norm  0.0054510753224780014\n",
      "iter  7  loss  -0.0016915524538804313  grad l2 norm  0.005460345230039832\n",
      "iter  8  loss  -0.0017070781572859149  grad l2 norm  0.005469579746603677\n",
      "iter  9  loss  -0.0017226310882445127  grad l2 norm  0.005478780131981344\n",
      "unsuccessful, tol:  0.005478780131981344\n",
      "iter  0  loss  -0.0017382119275424423  grad l2 norm  0.005487943684511173\n",
      "iter  1  loss  -0.00175382056208151  grad l2 norm  0.005497071670861611\n",
      "iter  2  loss  -0.0017694577114128979  grad l2 norm  0.005506161429081948\n",
      "iter  3  loss  -0.0017851232986647514  grad l2 norm  0.005515214232447795\n",
      "iter  4  loss  -0.0018008180818269263  grad l2 norm  0.0055242274588543664\n",
      "iter  5  loss  -0.0018165420195198332  grad l2 norm  0.005533202388458121\n",
      "iter  6  loss  -0.001832295907699936  grad l2 norm  0.005542136439557478\n",
      "iter  7  loss  -0.0018480797395845666  grad l2 norm  0.005551030899810838\n",
      "iter  8  loss  -0.0018638943484881381  grad l2 norm  0.005559883228730904\n",
      "iter  9  loss  -0.0018797397611825997  grad l2 norm  0.005568694722303553\n",
      "unsuccessful, tol:  0.005568694722303553\n",
      "iter  0  loss  -0.0018956168476149027  grad l2 norm  0.005577462882196672\n",
      "iter  1  loss  -0.0019115256669461846  grad l2 norm  0.005586189013659685\n",
      "iter  2  loss  -0.0019274671249431686  grad l2 norm  0.00559487066148191\n",
      "iter  3  loss  -0.0019434413118999052  grad l2 norm  0.005603509141149034\n",
      "iter  4  loss  -0.001959449168538735  grad l2 norm  0.005612102041496831\n",
      "iter  5  loss  -0.001975490814977655  grad l2 norm  0.00562065068920726\n",
      "iter  6  loss  -0.001991567226019251  grad l2 norm  0.005629152717986189\n",
      "iter  7  loss  -0.0020076785502844237  grad l2 norm  0.005637609466633156\n",
      "iter  8  loss  -0.002023825795810107  grad l2 norm  0.0056460186144102545\n",
      "iter  9  loss  -0.0020400091384263084  grad l2 norm  0.005654381513100611\n",
      "unsuccessful, tol:  0.005654381513100611\n",
      "iter  0  loss  -0.0020562296186253342  grad l2 norm  0.005662695888050271\n",
      "iter  1  loss  -0.002072487438214293  grad l2 norm  0.005670963104848677\n",
      "iter  2  loss  -0.002088783669459902  grad l2 norm  0.0056791809352884064\n",
      "iter  3  loss  -0.0021051185390019967  grad l2 norm  0.005687350759570517\n",
      "iter  4  loss  -0.0021214931503193714  grad l2 norm  0.005695470396147669\n",
      "iter  5  loss  -0.002137907753843412  grad l2 norm  0.005703541240667407\n",
      "iter  6  loss  -0.0021543634838293102  grad l2 norm  0.005711561158338631\n",
      "iter  7  loss  -0.002170860613563452  grad l2 norm  0.005719531561196013\n",
      "iter  8  loss  -0.002187400307762825  grad l2 norm  0.005727450361241922\n",
      "iter  9  loss  -0.0022039828617237195  grad l2 norm  0.005735318988045207\n",
      "unsuccessful, tol:  0.005735318988045207\n",
      "iter  0  loss  -0.0022206094704095864  grad l2 norm  0.005743135400455654\n",
      "iter  1  loss  -0.0022372804503454826  grad l2 norm  0.005750901047059236\n",
      "iter  2  loss  -0.0022539970265884114  grad l2 norm  0.005758613933735651\n",
      "iter  3  loss  -0.0022707595361322573  grad l2 norm  0.005766275530067061\n",
      "iter  4  loss  -0.0022875692339862025  grad l2 norm  0.005773883889400288\n",
      "iter  5  loss  -0.0023044264768180625  grad l2 norm  0.005781440504986224\n",
      "iter  6  loss  -0.002321332549395755  grad l2 norm  0.005788943478486825\n",
      "iter  7  loss  -0.0023382878271652483  grad l2 norm  0.005796394330395564\n",
      "iter  8  loss  -0.0023552936243343486  grad l2 norm  0.005803791212105661\n",
      "iter  9  loss  -0.0023723503340575677  grad l2 norm  0.0058111356760583875\n",
      "unsuccessful, tol:  0.0058111356760583875\n",
      "iter  0  loss  -0.002389459299460894  grad l2 norm  0.005818425925521387\n",
      "iter  1  loss  -0.0024066209300897517  grad l2 norm  0.0058256635509300395\n",
      "iter  2  loss  -0.0024238365971874704  grad l2 norm  0.005832846810414844\n",
      "iter  3  loss  -0.0024411067250571794  grad l2 norm  0.00583997733995924\n",
      "iter  4  loss  -0.00245843271191303  grad l2 norm  0.005847053456438721\n",
      "iter  5  loss  -0.0024758149948141137  grad l2 norm  0.0058540768505286195\n",
      "iter  6  loss  -0.0024932549974050026  grad l2 norm  0.0058610459025549965\n",
      "iter  7  loss  -0.002510753167102492  grad l2 norm  0.005867962368561187\n",
      "iter  8  loss  -0.00252831095102792  grad l2 norm  0.005874824697620854\n",
      "iter  9  loss  -0.0025459288041658115  grad l2 norm  0.005881634723119026\n",
      "unsuccessful, tol:  0.005881634723119026\n",
      "iter  0  loss  -0.002563608194763163  grad l2 norm  0.005888390968330001\n",
      "iter  1  loss  -0.002581349582235268  grad l2 norm  0.00589509535680203\n",
      "iter  2  loss  -0.002599154453261243  grad l2 norm  0.005901746490975008\n",
      "iter  3  loss  -0.0026170232682871113  grad l2 norm  0.005908346397580409\n",
      "iter  4  loss  -0.0026349575294787706  grad l2 norm  0.005914893761872198\n",
      "iter  5  loss  -0.0026529576947660325  grad l2 norm  0.005921390726169516\n",
      "iter  6  loss  -0.0026710252787197518  grad l2 norm  0.005927836059947162\n",
      "iter  7  loss  -0.002689160733193299  grad l2 norm  0.0059342320320402535\n",
      "iter  8  loss  -0.002707365582065626  grad l2 norm  0.005940577494413102\n",
      "iter  9  loss  -0.002725640267670651  grad l2 norm  0.005946874851087314\n",
      "unsuccessful, tol:  0.005946874851087314\n",
      "iter  0  loss  -0.0027439863201910533  grad l2 norm  0.005953123030932257\n",
      "iter  1  loss  -0.0027624041692223904  grad l2 norm  0.005959324579014734\n",
      "iter  2  loss  -0.0027808953484131198  grad l2 norm  0.0059654784912855415\n",
      "iter  3  loss  -0.00279946027169341  grad l2 norm  0.0059715874566907785\n",
      "iter  4  loss  -0.0028181004735360767  grad l2 norm  0.005977650524226797\n",
      "iter  5  loss  -0.002836816349593264  grad l2 norm  0.005983670526699277\n",
      "iter  6  loss  -0.0028556094327035725  grad l2 norm  0.0059896465483547726\n",
      "iter  7  loss  -0.002874480097925065  grad l2 norm  0.005995581563496369\n",
      "iter  8  loss  -0.002893429874138608  grad l2 norm  0.00600147467086653\n",
      "iter  9  loss  -0.002912459113744395  grad l2 norm  0.006007328982302124\n",
      "unsuccessful, tol:  0.006007328982302124\n",
      "iter  0  loss  -0.0029315693394123175  grad l2 norm  0.006013143588350931\n",
      "iter  1  loss  -0.0029507608790191927  grad l2 norm  0.006018921733635145\n",
      "iter  2  loss  -0.0029700352467774502  grad l2 norm  0.0060246624769713245\n",
      "iter  3  loss  -0.002989392744327705  grad l2 norm  0.0060303691909862405\n",
      "iter  4  loss  -0.0030088348751384276  grad l2 norm  0.0060360408794119524\n",
      "iter  5  loss  -0.003028361913028452  grad l2 norm  0.006041681038645792\n",
      "iter  6  loss  -0.0030479753483762763  grad l2 norm  0.0060472885950505284\n",
      "iter  7  loss  -0.0030676754257230334  grad l2 norm  0.006052867165456151\n",
      "iter  8  loss  -0.0030874636199642097  grad l2 norm  0.006058415578343794\n",
      "iter  9  loss  -0.003107340145067791  grad l2 norm  0.006063937568651693\n",
      "unsuccessful, tol:  0.006063937568651693\n",
      "iter  0  loss  -0.0031273064580491627  grad l2 norm  0.006069431848759944\n",
      "iter  1  loss  -0.0031473627412243283  grad l2 norm  0.0060749022703155044\n",
      "iter  2  loss  -0.0031675104313965575  grad l2 norm  0.006080347414039132\n",
      "iter  3  loss  -0.003187749678442678  grad l2 norm  0.006085771247570524\n",
      "iter  4  loss  -0.0032080818967737865  grad l2 norm  0.006091172207336598\n",
      "iter  5  loss  -0.0032285072034179433  grad l2 norm  0.0060965543765944975\n",
      "iter  6  loss  -0.003249026988463351  grad l2 norm  0.006101916037943294\n",
      "iter  7  loss  -0.003269641336145025  grad l2 norm  0.00610726138983213\n",
      "iter  8  loss  -0.003290351610645885  grad l2 norm  0.00611258855473653\n",
      "iter  9  loss  -0.0033111578640119366  grad l2 norm  0.006117901845419054\n",
      "unsuccessful, tol:  0.006117901845419054\n",
      "iter  0  loss  -0.003332061433379171  grad l2 norm  0.006123199221276795\n",
      "iter  1  loss  -0.0033530623398275645  grad l2 norm  0.006128485107662669\n",
      "iter  2  loss  -0.0033741618928250124  grad l2 norm  0.0061337573013695084\n",
      "iter  3  loss  -0.003395360084378883  grad l2 norm  0.006139020337407238\n",
      "iter  4  loss  -0.003416658196247372  grad l2 norm  0.006144271853930735\n",
      "iter  5  loss  -0.0034380561939558  grad l2 norm  0.00614951649112686\n",
      "iter  6  loss  -0.0034595553321221  grad l2 norm  0.0061547517360021784\n",
      "iter  7  loss  -0.003481155553070006  grad l2 norm  0.0061599823275736865\n",
      "iter  8  loss  -0.0035028580854522544  grad l2 norm  0.006165205612702428\n",
      "iter  9  loss  -0.003524662852311153  grad l2 norm  0.006170426420699067\n",
      "unsuccessful, tol:  0.006170426420699067\n",
      "iter  0  loss  -0.003546571058059235  grad l2 norm  0.006175641972711579\n",
      "iter  1  loss  -0.003568582610909848  grad l2 norm  0.0061808571772724785\n",
      "iter  2  loss  -0.0035906986931902164  grad l2 norm  0.006186069147498397\n",
      "iter  3  loss  -0.0036129192030884966  grad l2 norm  0.006191282857130541\n",
      "iter  4  loss  -0.0036352453032275104  grad l2 norm  0.006196495331885996\n",
      "iter  5  loss  -0.003657676886652174  grad l2 norm  0.006201711593267907\n",
      "iter  6  loss  -0.003680215098583165  grad l2 norm  0.0062069286027181345\n",
      "iter  7  loss  -0.0037028598315019365  grad l2 norm  0.006212151408020304\n",
      "iter  8  loss  -0.003725612215032732  grad l2 norm  0.006217376931344501\n",
      "iter  9  loss  -0.0037484721448787332  grad l2 norm  0.006222610220521423\n",
      "unsuccessful, tol:  0.006222610220521423\n",
      "iter  0  loss  -0.0037714407358526133  grad l2 norm  0.006227848184565667\n",
      "iter  1  loss  -0.0037945178892845816  grad l2 norm  0.0062330958395768465\n",
      "iter  2  loss  -0.0038177047043434402  grad l2 norm  0.006238350107793893\n",
      "iter  3  loss  -0.00384100108839042  grad l2 norm  0.006243615935484887\n",
      "iter  4  loss  -0.003864408121913851  grad l2 norm  0.006248890283944613\n",
      "iter  5  loss  -0.0038879257161560836  grad l2 norm  0.006254177984587004\n",
      "iter  6  loss  -0.003911554927234198  grad l2 norm  0.006259476062470879\n",
      "iter  7  loss  -0.003935295665232589  grad l2 norm  0.006264789182013092\n",
      "iter  8  loss  -0.0039591489534032825  grad l2 norm  0.0062701144555095565\n",
      "iter  9  loss  -0.00398311469279335  grad l2 norm  0.006275456321552218\n",
      "unsuccessful, tol:  0.006275456321552218\n",
      "iter  0  loss  -0.0040071938627974585  grad l2 norm  0.006280812002467184\n",
      "iter  1  loss  -0.004031386345261424  grad l2 norm  0.006286185646729428\n",
      "iter  2  loss  -0.004055693063064691  grad l2 norm  0.006291574610106199\n",
      "iter  3  loss  -0.004080113867475767  grad l2 norm  0.006296982683178472\n",
      "iter  4  loss  -0.004104649611828032  grad l2 norm  0.006302407381063982\n",
      "iter  5  loss  -0.004129300105660023  grad l2 norm  0.006307852067652871\n",
      "iter  6  loss  -0.0041540661208722286  grad l2 norm  0.006313314447909733\n",
      "iter  7  loss  -0.004178947415841011  grad l2 norm  0.0063187973916873355\n",
      "iter  8  loss  -0.004203944671689311  grad l2 norm  0.006324298830651749\n",
      "iter  9  loss  -0.004229057589124214  grad l2 norm  0.006329821076674559\n",
      "unsuccessful, tol:  0.006329821076674559\n",
      "iter  0  loss  -0.004254286752635531  grad l2 norm  0.006335362332321599\n",
      "iter  1  loss  -0.004279631802315431  grad l2 norm  0.006340924291993921\n",
      "iter  2  loss  -0.004305093223956711  grad l2 norm  0.00634650548067318\n",
      "iter  3  loss  -0.004330670597622862  grad l2 norm  0.006352106920319152\n",
      "iter  4  loss  -0.004356364311797989  grad l2 norm  0.006357727516178251\n",
      "iter  5  loss  -0.004382173890034133  grad l2 norm  0.006363367566578181\n",
      "iter  6  loss  -0.004408099627531944  grad l2 norm  0.006369026419567165\n",
      "iter  7  loss  -0.0044341409968227975  grad l2 norm  0.0063747036012274496\n",
      "iter  8  loss  -0.004460298205434638  grad l2 norm  0.006380398967697906\n",
      "iter  9  loss  -0.004486570681267662  grad l2 norm  0.006386111225467409\n",
      "unsuccessful, tol:  0.006386111225467409\n",
      "iter  0  loss  -0.004512958550305961  grad l2 norm  0.006391840804884152\n",
      "iter  1  loss  -0.004539461202105315  grad l2 norm  0.006397585545583948\n",
      "iter  2  loss  -0.004566078686824219  grad l2 norm  0.006403346517375505\n",
      "iter  3  loss  -0.00459281036104722  grad l2 norm  0.00640912064496204\n",
      "iter  4  loss  -0.0046196562037089176  grad l2 norm  0.006414909700622944\n",
      "iter  5  loss  -0.004646615542316941  grad l2 norm  0.006420709644631715\n",
      "iter  6  loss  -0.004673688287616605  grad l2 norm  0.006426523011487893\n",
      "iter  7  loss  -0.004700873740150526  grad l2 norm  0.006432344745832935\n",
      "iter  8  loss  -0.004728171743749605  grad l2 norm  0.006438178200237816\n",
      "iter  9  loss  -0.004755581572267613  grad l2 norm  0.006444017250814129\n",
      "unsuccessful, tol:  0.006444017250814129\n",
      "iter  0  loss  -0.004783103002180698  grad l2 norm  0.006449866119881073\n",
      "iter  1  loss  -0.004810735279257812  grad l2 norm  0.0064557175606958315\n",
      "iter  2  loss  -0.004838478110700055  grad l2 norm  0.006461576712888663\n",
      "iter  3  loss  -0.00486633071144219  grad l2 norm  0.006467435151586179\n",
      "iter  4  loss  -0.004894292716361621  grad l2 norm  0.006473298977486008\n",
      "iter  5  loss  -0.004922363305841652  grad l2 norm  0.00647915853195185\n",
      "iter  6  loss  -0.004950542038631254  grad l2 norm  0.006485020917128684\n",
      "iter  7  loss  -0.004978828056230571  grad l2 norm  0.006490875185237587\n",
      "iter  8  loss  -0.005007220836998137  grad l2 norm  0.006496729477287496\n",
      "iter  9  loss  -0.005035719478842978  grad l2 norm  0.006502571501769025\n",
      "unsuccessful, tol:  0.006502571501769025\n",
      "iter  0  loss  -0.00506432337517821  grad l2 norm  0.006508410473304972\n",
      "iter  1  loss  -0.0050930315753299595  grad l2 norm  0.006514232703323776\n",
      "iter  2  loss  -0.005121843382952782  grad l2 norm  0.00652004851232837\n",
      "iter  3  loss  -0.005150757793505925  grad l2 norm  0.0065258427631067545\n",
      "iter  4  loss  -0.005179774015803093  grad l2 norm  0.006531626912054122\n",
      "iter  5  loss  -0.0052088909858722  grad l2 norm  0.006537384324274996\n",
      "iter  6  loss  -0.005238107812428513  grad l2 norm  0.006543127620389862\n",
      "iter  7  loss  -0.005267423366426322  grad l2 norm  0.006548838622716365\n",
      "iter  8  loss  -0.005296836651416825  grad l2 norm  0.0065545311440292665\n",
      "iter  9  loss  -0.005326346468105668  grad l2 norm  0.00656018542504474\n",
      "unsuccessful, tol:  0.00656018542504474\n",
      "iter  0  loss  -0.005355951710764377  grad l2 norm  0.006565816500464399\n",
      "iter  1  loss  -0.005385651106091332  grad l2 norm  0.0065714030002574884\n",
      "iter  2  loss  -0.0054154434370450874  grad l2 norm  0.006576961215809645\n",
      "iter  3  loss  -0.005445327355228815  grad l2 norm  0.006582468150945555\n",
      "iter  4  loss  -0.0054753015336560925  grad l2 norm  0.006587941396952529\n",
      "iter  5  loss  -0.0055053645517051195  grad l2 norm  0.006593356333774052\n",
      "iter  6  loss  -0.0055355149783832885  grad l2 norm  0.006598731907030147\n",
      "iter  7  loss  -0.005565751328619827  grad l2 norm  0.006604041895243276\n",
      "iter  8  loss  -0.005596072078566713  grad l2 norm  0.006609306664846883\n",
      "iter  9  loss  -0.005626475691643438  grad l2 norm  0.006614498435682883\n",
      "unsuccessful, tol:  0.006614498435682883\n",
      "iter  0  loss  -0.005656960567332595  grad l2 norm  0.006619639071677956\n",
      "iter  1  loss  -0.005687525135010243  grad l2 norm  0.006624699294266328\n",
      "iter  2  loss  -0.005718167737610456  grad l2 norm  0.006629702547299749\n",
      "iter  3  loss  -0.005748886790916826  grad l2 norm  0.006634618126579152\n",
      "iter  4  loss  -0.005779680603415332  grad l2 norm  0.006639471137965587\n",
      "iter  5  loss  -0.0058105475984149895  grad l2 norm  0.006644229530776496\n",
      "iter  6  loss  -0.005841486071394304  grad l2 norm  0.006648920148126918\n",
      "iter  7  loss  -0.00587249447206638  grad l2 norm  0.006653509672306244\n",
      "iter  8  loss  -0.00590357110054556  grad l2 norm  0.006658026746087839\n",
      "iter  9  loss  -0.0059347144462748285  grad l2 norm  0.006662436859026548\n",
      "unsuccessful, tol:  0.006662436859026548\n",
      "iter  0  loss  -0.005965922824396939  grad l2 norm  0.006666770497789422\n",
      "iter  1  loss  -0.005997194768329157  grad l2 norm  0.006670992023306781\n",
      "iter  2  loss  -0.006028528607839711  grad l2 norm  0.006675133787153241\n",
      "iter  3  loss  -0.006059922912022234  grad l2 norm  0.006679159070492651\n",
      "iter  4  loss  -0.006091376010797895  grad l2 norm  0.006683102082370722\n",
      "iter  5  loss  -0.006122886485163949  grad l2 norm  0.006686925052415855\n",
      "iter  6  loss  -0.006154452634220488  grad l2 norm  0.006690664005901831\n",
      "iter  7  loss  -0.0061860730098938135  grad l2 norm  0.006694280123290666\n",
      "iter  8  loss  -0.006217745832157333  grad l2 norm  0.006697811166444967\n",
      "iter  9  loss  -0.006249469565971093  grad l2 norm  0.00670121723940463\n",
      "unsuccessful, tol:  0.00670121723940463\n",
      "iter  0  loss  -0.006281242288105401  grad l2 norm  0.006704537720455741\n",
      "iter  1  loss  -0.006313062304723516  grad l2 norm  0.006707731579970908\n",
      "iter  2  loss  -0.006344927474011973  grad l2 norm  0.006710839654299375\n",
      "iter  3  loss  -0.006376835863509656  grad l2 norm  0.00671381969781249\n",
      "iter  4  loss  -0.00640878503340827  grad l2 norm  0.006716713816639811\n",
      "iter  5  loss  -0.006440772733186568  grad l2 norm  0.006719478452745038\n",
      "iter  6  loss  -0.006472796150757661  grad l2 norm  0.006722156778091259\n",
      "iter  7  loss  -0.006504852648011625  grad l2 norm  0.006724703827990989\n",
      "iter  8  loss  -0.006536938980812945  grad l2 norm  0.006727163638990757\n",
      "iter  9  loss  -0.006569052072617427  grad l2 norm  0.006729489766326493\n",
      "unsuccessful, tol:  0.006729489766326493\n",
      "iter  0  loss  -0.006601188209671295  grad l2 norm  0.006731726931513344\n",
      "iter  1  loss  -0.006633343851039155  grad l2 norm  0.006733827174215341\n",
      "iter  2  loss  -0.006665514802396628  grad l2 norm  0.006735835758377347\n",
      "iter  3  loss  -0.006697697059813645  grad l2 norm  0.006737703222284646\n",
      "iter  4  loss  -0.006729885964526485  grad l2 norm  0.006739475275594226\n",
      "iter  5  loss  -0.006762077076930998  grad l2 norm  0.006741101023185154\n",
      "iter  6  loss  -0.006794265313912319  grad l2 norm  0.006742626570290475\n",
      "iter  7  loss  -0.006826445848915151  grad l2 norm  0.006743999706383796\n",
      "iter  8  loss  -0.006858613232940754  grad l2 norm  0.006745266922349315\n",
      "iter  9  loss  -0.00689076231634908  grad l2 norm  0.006746374850415091\n",
      "unsuccessful, tol:  0.006746374850415091\n",
      "iter  0  loss  -0.006922887354513202  grad l2 norm  0.0067473703862562925\n",
      "iter  1  loss  -0.006954982947125586  grad l2 norm  0.0067481991899549084\n",
      "iter  2  loss  -0.0069870431296447265  grad l2 norm  0.006748908597141951\n",
      "iter  3  loss  -0.007019062326212089  grad l2 norm  0.006749443494018591\n",
      "iter  4  loss  -0.007051034428310236  grad l2 norm  0.00674985169495337\n",
      "iter  5  loss  -0.007082953757825549  grad l2 norm  0.006750077511503531\n",
      "iter  6  loss  -0.007114814134650839  grad l2 norm  0.006750169269052925\n",
      "iter  7  loss  -0.007146609846854533  grad l2 norm  0.006750070895306939\n",
      "iter  8  loss  -0.007178334709229054  grad l2 norm  0.006749831245342209\n",
      "iter  9  loss  -0.00720998303789608  grad l2 norm  0.006749394039055542\n",
      "unsuccessful, tol:  0.006749394039055542\n",
      "iter  0  loss  -0.007241548701987831  grad l2 norm  0.006748808662217308\n",
      "iter  1  loss  -0.007273026100309641  grad l2 norm  0.006748018784759083\n",
      "iter  2  loss  -0.007304409208440266  grad l2 norm  0.006747074304972649\n",
      "iter  3  loss  -0.007335692555289285  grad l2 norm  0.006745918981318053\n",
      "iter  4  loss  -0.007366870267925584  grad l2 norm  0.006744603187712564\n",
      "iter  5  loss  -0.0073979370460227345  grad l2 norm  0.006743070890789108\n",
      "iter  6  loss  -0.0074288872066884054  grad l2 norm  0.006741372886251252\n",
      "iter  7  loss  -0.007459715654903339  grad l2 norm  0.0067394534512777195\n",
      "iter  8  loss  -0.00749041693054652  grad l2 norm  0.006737363735138295\n",
      "iter  9  loss  -0.007520986173016994  grad l2 norm  0.006735048412835072\n",
      "unsuccessful, tol:  0.006735048412835072\n",
      "iter  0  loss  -0.007551418172544829  grad l2 norm  0.006732558907578038\n",
      "iter  1  loss  -0.007581708327251742  grad l2 norm  0.006729840366836652\n",
      "iter  2  loss  -0.007611851700715941  grad l2 norm  0.006726944399823168\n",
      "iter  3  loss  -0.007641843969784658  grad l2 norm  0.006723816691019938\n",
      "iter  4  loss  -0.007671680490228447  grad l2 norm  0.006720508942469216\n",
      "iter  5  loss  -0.007701357233670111  grad l2 norm  0.006716967432601102\n",
      "iter  6  loss  -0.007730869863023677  grad l2 norm  0.006713243860853025\n",
      "iter  7  loss  -0.007760214656949574  grad l2 norm  0.0067092851513350414\n",
      "iter  8  loss  -0.007789387596626608  grad l2 norm  0.006705142906025447\n",
      "iter  9  loss  -0.007818385276181929  grad l2 norm  0.006700764743548561\n",
      "unsuccessful, tol:  0.006700764743548561\n",
      "iter  0  loss  -0.007847204002188083  grad l2 norm  0.006696202076862957\n",
      "iter  1  loss  -0.007875840688547473  grad l2 norm  0.00669140326726023\n",
      "iter  2  loss  -0.007904291969942984  grad l2 norm  0.006686419453004402\n",
      "iter  3  loss  -0.007932555079679547  grad l2 norm  0.006681199787652072\n",
      "iter  4  loss  -0.007960626978150072  grad l2 norm  0.006675795057438203\n",
      "iter  5  loss  -0.007988505212161927  grad l2 norm  0.006670155261233536\n",
      "iter  6  loss  -0.008016187059324924  grad l2 norm  0.006664330766512696\n",
      "iter  7  loss  -0.00804367036743825  grad l2 norm  0.006658272475771044\n",
      "iter  8  loss  -0.008070952715596137  grad l2 norm  0.006652030283558085\n",
      "iter  9  loss  -0.008098032232322703  grad l2 norm  0.00664555606100223\n",
      "unsuccessful, tol:  0.00664555606100223\n",
      "iter  0  loss  -0.008124906774173905  grad l2 norm  0.006638899189529407\n",
      "iter  1  loss  -0.008151574721543008  grad l2 norm  0.006632012581348822\n",
      "iter  2  loss  -0.008178034175681512  grad l2 norm  0.006624945078972421\n",
      "iter  3  loss  -0.008204283731519267  grad l2 norm  0.006617650715118762\n",
      "iter  4  loss  -0.008230321694381548  grad l2 norm  0.006610177780894037\n",
      "iter  5  loss  -0.008256146829801081  grad l2 norm  0.006602481513654506\n",
      "iter  6  loss  -0.00828175760175602  grad l2 norm  0.006594609650629833\n",
      "iter  7  loss  -0.00830715290021016  grad l2 norm  0.006586518718012569\n",
      "iter  8  loss  -0.008332331303273691  grad l2 norm  0.006578255900966508\n",
      "iter  9  loss  -0.008357291783847044  grad l2 norm  0.006569779091874518\n",
      "unsuccessful, tol:  0.006569779091874518\n",
      "iter  0  loss  -0.008382032998385339  grad l2 norm  0.00656113492226101\n",
      "iter  1  loss  -0.00840655397415788  grad l2 norm  0.00655228271313095\n",
      "iter  2  loss  -0.008430853426906512  grad l2 norm  0.006543268529562737\n",
      "iter  3  loss  -0.008454930429573992  grad l2 norm  0.006534053161727446\n",
      "iter  4  loss  -0.008478783759942723  grad l2 norm  0.0065246820787875645\n",
      "iter  5  loss  -0.00850241255031931  grad l2 norm  0.0065151175554568185\n",
      "iter  6  loss  -0.00852581566457555  grad l2 norm  0.0065054044179767324\n",
      "iter  7  loss  -0.00854899232678606  grad l2 norm  0.006495506417543247\n",
      "iter  8  loss  -0.008571941525702379  grad l2 norm  0.006485467676864089\n",
      "iter  9  loss  -0.008594662619480737  grad l2 norm  0.006475253397872153\n",
      "unsuccessful, tol:  0.006475253397872153\n",
      "iter  0  loss  -0.008617154765123605  grad l2 norm  0.006464906932164992\n",
      "iter  1  loss  -0.00863941749656796  grad l2 norm  0.0064543948959472465\n",
      "iter  2  loss  -0.00866145017706437  grad l2 norm  0.006443759801530757\n",
      "iter  3  loss  -0.008683252548746656  grad l2 norm  0.0064329696377416765\n",
      "iter  4  loss  -0.008704824206781718  grad l2 norm  0.006422066013052647\n",
      "iter  5  loss  -0.008726165119502239  grad l2 norm  0.006411018245319224\n",
      "iter  6  loss  -0.008747275123878538  grad l2 norm  0.006399866980464139\n",
      "iter  7  loss  -0.008768154415937948  grad l2 norm  0.0063885828217378786\n",
      "iter  8  loss  -0.008788803067455374  grad l2 norm  0.006377205401288507\n",
      "iter  9  loss  -0.008809221486685512  grad l2 norm  0.006365706566166395\n",
      "unsuccessful, tol:  0.006365706566166395\n",
      "iter  0  loss  -0.008829409956298008  grad l2 norm  0.006354124893311971\n",
      "iter  1  loss  -0.00884936906468377  grad l2 norm  0.006342433436997363\n",
      "iter  2  loss  -0.008869099265389827  grad l2 norm  0.0063306696903976814\n",
      "iter  3  loss  -0.008888601279726559  grad l2 norm  0.006318807886842267\n",
      "iter  4  loss  -0.008907875678509068  grad l2 norm  0.006306884422894529\n",
      "iter  5  loss  -0.008926923257490213  grad l2 norm  0.006294874693619126\n",
      "iter  6  loss  -0.008945744643038317  grad l2 norm  0.006282814002929237\n",
      "iter  7  loss  -0.008964340642828389  grad l2 norm  0.006270678901258751\n",
      "iter  8  loss  -0.008982711877782736  grad l2 norm  0.006258503619004709\n",
      "iter  9  loss  -0.009000859110935375  grad l2 norm  0.006246265863991825\n",
      "unsuccessful, tol:  0.006246265863991825\n",
      "iter  0  loss  -0.009018782907929561  grad l2 norm  0.00623399882347206\n",
      "iter  1  loss  -0.009036483946468025  grad l2 norm  0.006221681369098704\n",
      "iter  2  loss  -0.00905396270651472  grad l2 norm  0.006209345682369273\n",
      "iter  3  loss  -0.009071219761187022  grad l2 norm  0.006196971806735943\n",
      "iter  4  loss  -0.009088255495870385  grad l2 norm  0.006184590960315038\n",
      "iter  5  loss  -0.009105070379529949  grad l2 norm  0.006172184367946864\n",
      "iter  6  loss  -0.009121664710248296  grad l2 norm  0.006159782332730365\n",
      "iter  7  loss  -0.009138038864872855  grad l2 norm  0.006147367274655619\n",
      "iter  8  loss  -0.009154193068031476  grad l2 norm  0.006134968638299753\n",
      "iter  9  loss  -0.00917012761843905  grad l2 norm  0.006122570058654094\n",
      "unsuccessful, tol:  0.006122570058654094\n",
      "iter  0  loss  -0.00918584267937894  grad l2 norm  0.006110200186006876\n",
      "iter  1  loss  -0.009201338481505885  grad l2 norm  0.006097843893896163\n",
      "iter  2  loss  -0.009216615134332093  grad l2 norm  0.006085529104380525\n",
      "iter  3  loss  -0.009231672806974891  grad l2 norm  0.006073241948238109\n",
      "iter  4  loss  -0.009246511561322782  grad l2 norm  0.006061009675514936\n",
      "iter  5  loss  -0.00926113151300192  grad l2 norm  0.0060488196747283535\n",
      "iter  6  loss  -0.009275532687013551  grad l2 norm  0.006036698555822295\n",
      "iter  7  loss  -0.00928971516094194  grad l2 norm  0.0060246349335324155\n",
      "iter  8  loss  -0.009303678943007808  grad l2 norm  0.006012654773345621\n",
      "iter  9  loss  -0.009317424098540103  grad l2 norm  0.006000747844119769\n",
      "unsuccessful, tol:  0.006000747844119769\n",
      "iter  0  loss  -0.00933095064914654  grad l2 norm  0.005988939422253004\n",
      "iter  1  loss  -0.009344258682401952  grad l2 norm  0.0059772203192686255\n",
      "iter  2  loss  -0.009357348269913732  grad l2 norm  0.005965615044199487\n",
      "iter  3  loss  -0.009370219559647928  grad l2 norm  0.005954115311973181\n",
      "iter  4  loss  -0.009382872710508248  grad l2 norm  0.0059427447676403755\n",
      "iter  5  loss  -0.0093953079669133  grad l2 norm  0.005931495880227275\n",
      "iter  6  loss  -0.009407525608040136  grad l2 norm  0.005920391333984304\n",
      "iter  7  loss  -0.009419526004890485  grad l2 norm  0.005909424210046136\n",
      "iter  8  loss  -0.009431309583130834  grad l2 norm  0.005898616149363705\n",
      "iter  9  loss  -0.009442876863468775  grad l2 norm  0.005887960721972982\n",
      "unsuccessful, tol:  0.005887960721972982\n",
      "iter  0  loss  -0.009454228437611813  grad l2 norm  0.005877478464506506\n",
      "iter  1  loss  -0.009465364993022151  grad l2 norm  0.005867163335131872\n",
      "iter  2  loss  -0.009476287301750626  grad l2 norm  0.005857034726524073\n",
      "iter  3  loss  -0.009486996230551149  grad l2 norm  0.005847086904136851\n",
      "iter  4  loss  -0.009497492742312947  grad l2 norm  0.005837338093924264\n",
      "iter  5  loss  -0.00950777789226508  grad l2 norm  0.005827782802426739\n",
      "iter  6  loss  -0.009517852841593222  grad l2 norm  0.005818438077597359\n",
      "iter  7  loss  -0.009527718840206579  grad l2 norm  0.005809298610644744\n",
      "iter  8  loss  -0.009537377251997324  grad l2 norm  0.0058003802679464506\n",
      "iter  9  loss  -0.009546829524446326  grad l2 norm  0.005791677876085701\n",
      "unsuccessful, tol:  0.005791677876085701\n",
      "iter  0  loss  -0.009556077224897555  grad l2 norm  0.0057832061227450415\n",
      "iter  1  loss  -0.009565121997156487  grad l2 norm  0.005774959927514652\n",
      "iter  2  loss  -0.00957396560811279  grad l2 norm  0.005766952809583079\n",
      "iter  3  loss  -0.009582609891431854  grad l2 norm  0.005759179747565542\n",
      "iter  4  loss  -0.009591056803876131  grad l2 norm  0.005751653111580359\n",
      "iter  5  loss  -0.009599308356218398  grad l2 norm  0.005744367915610247\n",
      "iter  6  loss  -0.009607366678715269  grad l2 norm  0.00573733541105528\n",
      "iter  7  loss  -0.009615233939511966  grad l2 norm  0.005730550635149506\n",
      "iter  8  loss  -0.009622912418951116  grad l2 norm  0.005724023762220941\n",
      "iter  9  loss  -0.009630404416027364  grad l2 norm  0.005717749851552039\n",
      "unsuccessful, tol:  0.005717749851552039\n",
      "iter  0  loss  -0.009637712331596469  grad l2 norm  0.005711738051373458\n",
      "iter  1  loss  -0.009644838563874861  grad l2 norm  0.0057059834494169945\n",
      "iter  2  loss  -0.0096517856011138  grad l2 norm  0.005700494223174794\n",
      "iter  3  loss  -0.009658555907635194  grad l2 norm  0.0056952654965000855\n",
      "iter  4  loss  -0.00966515202700492  grad l2 norm  0.005690304528174694\n",
      "iter  5  loss  -0.00967157646043944  grad l2 norm  0.00568560647860273\n",
      "iter  6  loss  -0.009677831781685076  grad l2 norm  0.005681177727645092\n",
      "iter  7  loss  -0.009683920509529246  grad l2 norm  0.005677013457154629\n",
      "iter  8  loss  -0.00968984523520346  grad l2 norm  0.005673119188631835\n",
      "iter  9  loss  -0.00969560849001119  grad l2 norm  0.00566949009014272\n",
      "unsuccessful, tol:  0.00566949009014272\n",
      "iter  0  loss  -0.009701212885096852  grad l2 norm  0.005666130824497082\n",
      "iter  1  loss  -0.009706660974037475  grad l2 norm  0.005663036492517683\n",
      "iter  2  loss  -0.009711955403643469  grad l2 norm  0.0056602108823947525\n",
      "iter  3  loss  -0.009717098770302954  grad l2 norm  0.0056576489644064205\n",
      "iter  4  loss  -0.009722093779726649  grad l2 norm  0.005655353630985813\n",
      "iter  5  loss  -0.009726943095448056  grad l2 norm  0.005653319660113821\n",
      "iter  6  loss  -0.00973164950547396  grad l2 norm  0.005651549033145742\n",
      "iter  7  loss  -0.009736215761746514  grad l2 norm  0.005650036285289391\n",
      "iter  8  loss  -0.009740644752347214  grad l2 norm  0.005648782484895106\n",
      "iter  9  loss  -0.009744939331523862  grad l2 norm  0.005647781890417536\n",
      "unsuccessful, tol:  0.005647781890417536\n",
      "iter  0  loss  -0.009749102496903146  grad l2 norm  0.005647034671515597\n",
      "iter  1  loss  -0.009753137210363421  grad l2 norm  0.005646534793341457\n",
      "iter  2  loss  -0.009757046580232156  grad l2 norm  0.005646281556900545\n",
      "iter  3  loss  -0.009760833673598137  grad l2 norm  0.005646268633551089\n",
      "iter  4  loss  -0.009764501703630902  grad l2 norm  0.00564649449958487\n",
      "iter  5  loss  -0.00976805383394063  grad l2 norm  0.005646952546235695\n",
      "iter  6  loss  -0.009771493371112714  grad l2 norm  0.0056476404810586935\n",
      "iter  7  loss  -0.009774823561627696  grad l2 norm  0.005648551441236819\n",
      "iter  8  loss  -0.009778047789633272  grad l2 norm  0.005649682431879937\n",
      "iter  9  loss  -0.009781169366870996  grad l2 norm  0.00565102637287287\n",
      "unsuccessful, tol:  0.00565102637287287\n",
      "iter  0  loss  -0.009784191735695905  grad l2 norm  0.005652579641646176\n",
      "iter  1  loss  -0.009787118252454518  grad l2 norm  0.005654334986238938\n",
      "iter  2  loss  -0.00978995239580264  grad l2 norm  0.005656288237503424\n",
      "iter  3  loss  -0.00979269754399822  grad l2 norm  0.005658432023563248\n",
      "iter  4  loss  -0.009795357188597455  grad l2 norm  0.005660761713888525\n",
      "iter  5  loss  -0.009797934706104081  grad l2 norm  0.005663269872941171\n",
      "iter  6  loss  -0.00980043357708811  grad l2 norm  0.005665951496141485\n",
      "iter  7  loss  -0.009802857152531487  grad l2 norm  0.005668799143210836\n",
      "iter  8  loss  -0.009805208878300444  grad l2 norm  0.005671807523674058\n",
      "iter  9  loss  -0.009807492056363562  grad l2 norm  0.005674969253146274\n",
      "unsuccessful, tol:  0.005674969253146274\n",
      "iter  0  loss  -0.009809710074393485  grad l2 norm  0.005678278843571635\n",
      "iter  1  loss  -0.009811866161854397  grad l2 norm  0.005681729028401538\n",
      "iter  2  loss  -0.009813963624362817  grad l2 norm  0.005685314210214723\n",
      "iter  3  loss  -0.009816005594490675  grad l2 norm  0.0056890273034183045\n",
      "iter  4  loss  -0.009817995270403007  grad l2 norm  0.005692862689236395\n",
      "iter  5  loss  -0.009819935661114977  grad l2 norm  0.005696813527418133\n",
      "iter  6  loss  -0.00982182982885236  grad l2 norm  0.005700874265887281\n",
      "iter  7  loss  -0.009823680628454902  grad l2 norm  0.005705038376081274\n",
      "iter  8  loss  -0.009825490952916457  grad l2 norm  0.00570930046176521\n",
      "iter  9  loss  -0.009827263466725349  grad l2 norm  0.005713654375999843\n",
      "unsuccessful, tol:  0.005713654375999843\n",
      "iter  0  loss  -0.009829000854257652  grad l2 norm  0.005718094970363053\n",
      "iter  1  loss  -0.009830705547254797  grad l2 norm  0.005722616554800837\n",
      "iter  2  loss  -0.009832379976336089  grad l2 norm  0.005727214324267134\n",
      "iter  3  loss  -0.009834026293675711  grad l2 norm  0.005731883125416059\n",
      "iter  4  loss  -0.009835646628914766  grad l2 norm  0.005736618592156547\n",
      "iter  5  loss  -0.009837242809309638  grad l2 norm  0.005741416185274859\n",
      "iter  6  loss  -0.00983881662244224  grad l2 norm  0.005746272062235483\n",
      "iter  7  loss  -0.009840369537122516  grad l2 norm  0.005751182358123227\n",
      "iter  8  loss  -0.009841902975773005  grad l2 norm  0.0057561438098243265\n",
      "iter  9  loss  -0.00984341803933463  grad l2 norm  0.005761153250408736\n",
      "unsuccessful, tol:  0.005761153250408736\n",
      "iter  0  loss  -0.00984491579125948  grad l2 norm  0.005766208005499281\n",
      "iter  1  loss  -0.009846396988065843  grad l2 norm  0.005771305578962365\n",
      "iter  2  loss  -0.009847862374457367  grad l2 norm  0.005776443840359503\n",
      "iter  3  loss  -0.009849312418496325  grad l2 norm  0.005781620886322094\n",
      "iter  4  loss  -0.009850747614552943  grad l2 norm  0.005786835040882454\n",
      "iter  5  loss  -0.009852168219765898  grad l2 norm  0.0057920848814897756\n",
      "iter  6  loss  -0.009853574559897223  grad l2 norm  0.005797369075941204\n",
      "iter  7  loss  -0.00985496676316093  grad l2 norm  0.0058026865627060316\n",
      "iter  8  loss  -0.009856345064453063  grad l2 norm  0.005808036247471166\n",
      "iter  9  loss  -0.009857709533888693  grad l2 norm  0.005813417324775477\n",
      "unsuccessful, tol:  0.005813417324775477\n",
      "iter  0  loss  -0.009859060376764844  grad l2 norm  0.005818828854380675\n",
      "iter  1  loss  -0.009860397656220985  grad l2 norm  0.005824270207745475\n",
      "iter  2  loss  -0.00986172158840314  grad l2 norm  0.005829740541179006\n",
      "iter  3  loss  -0.00986303226033266  grad l2 norm  0.005835239348855481\n",
      "iter  4  loss  -0.00986432992110148  grad l2 norm  0.005840765847161878\n",
      "iter  5  loss  -0.009865614696730038  grad l2 norm  0.00584631961665584\n",
      "iter  6  loss  -0.009866886878633727  grad l2 norm  0.005851899910221239\n",
      "iter  7  loss  -0.009868146637118179  grad l2 norm  0.0058575063689621865\n",
      "iter  8  loss  -0.009869394308148573  grad l2 norm  0.0058631382652038514\n",
      "iter  9  loss  -0.00987063010661693  grad l2 norm  0.0058687952805972984\n",
      "unsuccessful, tol:  0.0058687952805972984\n",
      "iter  0  loss  -0.009871854412044061  grad l2 norm  0.005874476693245886\n",
      "iter  1  loss  -0.009873067482229773  grad l2 norm  0.0058801822093255095\n",
      "iter  2  loss  -0.009874269738182173  grad l2 norm  0.005885911101681902\n",
      "iter  3  loss  -0.009875461478392472  grad l2 norm  0.0058916630886249625\n",
      "iter  4  loss  -0.009876643163089756  grad l2 norm  0.005897437429491051\n",
      "iter  5  loss  -0.009877815129174877  grad l2 norm  0.005903233845927579\n",
      "iter  6  loss  -0.009878977873811078  grad l2 norm  0.005909051578444316\n",
      "iter  7  loss  -0.009880131769952376  grad l2 norm  0.005914890346575546\n",
      "iter  8  loss  -0.00988127734924791  grad l2 norm  0.005920749369466672\n",
      "iter  9  loss  -0.009882415018057743  grad l2 norm  0.00592662836197665\n",
      "unsuccessful, tol:  0.00592662836197665\n",
      "iter  0  loss  -0.009883545339638028  grad l2 norm  0.005932526521817641\n",
      "iter  1  loss  -0.009884668750518348  grad l2 norm  0.005938443559047503\n",
      "iter  2  loss  -0.009885785841931002  grad l2 norm  0.005944378652070829\n",
      "iter  3  loss  -0.009886897076419664  grad l2 norm  0.005950331508162335\n",
      "iter  4  loss  -0.009888003068478065  grad l2 norm  0.0059563012905926496\n",
      "iter  5  loss  -0.009889104301268032  grad l2 norm  0.005962287707904664\n",
      "iter  6  loss  -0.009890201406445285  grad l2 norm  0.0059682899144199095\n",
      "iter  7  loss  -0.009891294880890876  grad l2 norm  0.0059743076260090275\n",
      "iter  8  loss  -0.009892385365730769  grad l2 norm  0.00598033999619985\n",
      "iter  9  loss  -0.009893473363033011  grad l2 norm  0.005986386756169261\n",
      "unsuccessful, tol:  0.005986386756169261\n",
      "iter  0  loss  -0.009894559514077188  grad l2 norm  0.005992447068564617\n",
      "iter  1  loss  -0.00989564431604154  grad l2 norm  0.005998520689447\n",
      "iter  2  loss  -0.009896728399647359  grad l2 norm  0.006004606801736816\n",
      "iter  3  loss  -0.00989781224595093  grad l2 norm  0.006010705196871709\n",
      "iter  4  loss  -0.009898896463566169  grad l2 norm  0.0060168150895375345\n",
      "iter  5  loss  -0.009899981505769335  grad l2 norm  0.006022936316886717\n",
      "iter  6  loss  -0.00990106794756853  grad l2 norm  0.0060290681359422005\n",
      "iter  7  loss  -0.009902156203400068  grad l2 norm  0.006035210438391061\n",
      "iter  8  loss  -0.009903246804337759  grad l2 norm  0.006041362531853339\n",
      "iter  9  loss  -0.009904340116679209  grad l2 norm  0.00604752436850561\n",
      "unsuccessful, tol:  0.00604752436850561\n",
      "iter  0  loss  -0.009905436619546295  grad l2 norm  0.006053695311318022\n",
      "iter  1  loss  -0.009906536624610218  grad l2 norm  0.006059875375060337\n",
      "iter  2  loss  -0.009907640554223036  grad l2 norm  0.00606606397868185\n",
      "iter  3  loss  -0.009908748662417274  grad l2 norm  0.006072261197528228\n",
      "iter  4  loss  -0.00990986131355087  grad l2 norm  0.006078466503222052\n",
      "iter  5  loss  -0.009910978704576476  grad l2 norm  0.006084680026083763\n",
      "iter  6  loss  -0.009912101144023184  grad l2 norm  0.0060909012841014526\n",
      "iter  7  loss  -0.009913228775410026  grad l2 norm  0.006097130454559938\n",
      "iter  8  loss  -0.00991436185627625  grad l2 norm  0.0061033670938850405\n",
      "iter  9  loss  -0.00991550048254877  grad l2 norm  0.006109611417322983\n",
      "unsuccessful, tol:  0.006109611417322983\n",
      "iter  0  loss  -0.009916644867293824  grad l2 norm  0.006115863011559526\n",
      "iter  1  loss  -0.009917795065848207  grad l2 norm  0.006122122121052224\n",
      "iter  2  loss  -0.009918951253993839  grad l2 norm  0.006128388355345287\n",
      "iter  3  loss  -0.009920113453710609  grad l2 norm  0.006134661980431206\n",
      "iter  4  loss  -0.00992128181054302  grad l2 norm  0.0061409426226264234\n",
      "iter  5  loss  -0.009922456319899397  grad l2 norm  0.0061472305632173764\n",
      "iter  6  loss  -0.009923637103459232  grad l2 norm  0.006153525440630437\n",
      "iter  7  loss  -0.009924824136002952  grad l2 norm  0.0061598275466380826\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m ps_traj         \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mregister_curve_network(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrajectory\u001b[39m\u001b[38;5;124m\"\u001b[39m, X[:,:\u001b[38;5;241m3\u001b[39m], edges\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mline\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     sol \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39msolution\n\u001b[1;32m     18\u001b[0m     X \u001b[38;5;241m=\u001b[39m sol[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/PlayGround/ergodic_MMD/notebooks/../ergodic_mmd/aug_lagrange_solver.py:76\u001b[0m, in \u001b[0;36mAugmentedLagrangeSolver.solve\u001b[0;34m(self, args, max_iter, eps, alpha)\u001b[0m\n\u001b[1;32m     72\u001b[0m _prev_val   \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m# self.solution, _val, self.avg_sq_grad = self.step(self.solution, args, self.avg_sq_grad, self.c)\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msolution, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdual_solution, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavg_sq_grad, _val, _dldx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual_solution\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavg_sq_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m     _grad_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     78\u001b[0m     _N\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ps.init()\n",
    "\n",
    "ps_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "ps_points = ps.register_point_cloud(\"sampled points\", args['points'][:,:3])\n",
    "ps_mesh.add_scalar_quantity(\"face vals\",vmap(info_distr, in_axes=(0,))(mesh.vertices))\n",
    "\n",
    "ps_points.add_scalar_quantity(\"results\", args['P_XI'])\n",
    "\n",
    "# ps_bunny_mesh.set_transparency(0.8)\n",
    "# ps_bunny_mesh.add_scalar_quantity('info_distr', mesh_func.func_vals, defined_on='vertices', cmap='blues')\n",
    "\n",
    "ps_traj         = ps.register_curve_network(\"trajectory\", X[:,:3], edges=\"line\")\n",
    "\n",
    "\n",
    "for _ in range(1000):\n",
    "    solver.solve(eps=1e-5, max_iter=10)\n",
    "    sol = solver.solution\n",
    "    X = sol['X']\n",
    "\n",
    "    ps_traj.update_node_positions(X[:,:3])\n",
    "    # ps_traj.add_vector_quantity(\"vec img\", X[:,:3], enabled=True)\n",
    "    ps.frame_tick()\n",
    "\n",
    "    time.sleep(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = solver.solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sol['X']\n",
    "\n",
    "ps.init()\n",
    "\n",
    "ps_mesh = ps.register_surface_mesh(\"bunny\", mesh.vertices, mesh.faces)\n",
    "ps_points = ps.register_point_cloud(\"sampled points\", args['points'])\n",
    "ps_mesh.add_scalar_quantity(\"face vals\",vmap(info_distr, in_axes=(0,))(mesh.vertices))\n",
    "\n",
    "ps_points.add_scalar_quantity(\"results\", args['P_XI'])\n",
    "\n",
    "# ps_bunny_mesh.set_transparency(0.8)\n",
    "# ps_bunny_mesh.add_scalar_quantity('info_distr', mesh_func.func_vals, defined_on='vertices', cmap='blues')\n",
    "\n",
    "ps_traj         = ps.register_curve_network(\"trajectory\", X , edges=\"line\")\n",
    "\n",
    "ps.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
